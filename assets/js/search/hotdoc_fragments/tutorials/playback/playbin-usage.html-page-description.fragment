fragment_downloaded_cb({"url": "tutorials/playback/playbin-usage.html#page-description", "fragment": "<div id=\"page-description\" data-hotdoc-source=\"playbin-usage.md\">\n<h1 id=\"playback-tutorial-1-playbin-usage\">Playback tutorial 1: Playbin usage</h1>\n<h2 id=\"goal\">Goal</h2>\n<p>We have already worked with the <code>playbin</code> element, which is capable of\nbuilding a complete playback pipeline without much work on our side.\nThis tutorial shows how to further customize <code>playbin</code> in case its\ndefault values do not suit our particular needs.</p>\n<p>We will learn:</p>\n<ul>\n<li>\n<p>How to find out how many streams a file contains, and how to switch\namong them.</p>\n</li>\n<li>\n<p>How to gather information regarding each stream.</p>\n</li>\n</ul>\n<h2 id=\"introduction\">Introduction</h2>\n<p>More often than not, multiple audio, video and subtitle streams can be\nfound embedded in a single file. The most common case are regular\nmovies, which contain one video and one audio stream (Stereo or 5.1\naudio tracks are considered a single stream). It is also increasingly\ncommon to find movies with one video and multiple audio streams, to\naccount for different languages. In this case, the user selects one\naudio stream, and the application will only play that one, ignoring the\nothers.</p>\n<p>To be able to select the appropriate stream, the user needs to know\ncertain information about them, for example, their language. This\ninformation is embedded in the streams in the form of \u201cmetadata\u201d\n(annexed data), and this tutorial shows how to retrieve it.</p>\n<p>Subtitles can also be embedded in a file, along with audio and video,\nbut they are dealt with in more detail in <a href=\"subtitle-management.html\">Playback tutorial 2: Subtitle\nmanagement</a>. Finally, multiple video streams can also be found in a\nsingle file, for example, in DVD with multiple angles of the same scene,\nbut they are somewhat rare.</p>\n<blockquote>\n<p><img src=\"images/icons/emoticons/information.png\" alt=\"information\" id=\"information\"> Embedding multiple streams inside a single file is\ncalled \u201cmultiplexing\u201d or \u201cmuxing\u201d, and such file is then known as a\n\u201ccontainer\u201d. Common container formats are Matroska (.mkv), Quicktime\n(.qt, .mov, .mp4), Ogg (.ogg) or Webm (.webm).</p>\n<p>Retrieving the individual streams from within the container is called\n\u201cdemultiplexing\u201d or \u201cdemuxing\u201d.</p>\n</blockquote>\n<p>The following code recovers the amount of streams in the file, their\nassociated metadata, and allows switching the audio stream while the\nmedia is playing.</p>\n<h2 id=\"the-multilingual-player\">The multilingual player</h2>\n<p>Copy this code into a text file named <code>playback-tutorial-1.c</code> (or find\nit in the GStreamer installation).</p>\n<p><strong>playback-tutorial-1.c</strong></p>\n<pre><code class=\"language-c\">#include &lt;gst/gst.h&gt;\n\n/* Structure to contain all our information, so we can pass it around */\ntypedef struct _CustomData {\n  GstElement *playbin;  /* Our one and only element */\n\n  gint n_video;          /* Number of embedded video streams */\n  gint n_audio;          /* Number of embedded audio streams */\n  gint n_text;           /* Number of embedded subtitle streams */\n\n  gint current_video;    /* Currently playing video stream */\n  gint current_audio;    /* Currently playing audio stream */\n  gint current_text;     /* Currently playing subtitle stream */\n\n  GMainLoop *main_loop;  /* GLib's Main Loop */\n} CustomData;\n\n/* playbin flags */\ntypedef enum {\n  GST_PLAY_FLAG_VIDEO         = (1 &lt;&lt; 0), /* We want video output */\n  GST_PLAY_FLAG_AUDIO         = (1 &lt;&lt; 1), /* We want audio output */\n  GST_PLAY_FLAG_TEXT          = (1 &lt;&lt; 2)  /* We want subtitle output */\n} GstPlayFlags;\n\n/* Forward definition for the message and keyboard processing functions */\nstatic gboolean handle_message (GstBus *bus, GstMessage *msg, CustomData *data);\nstatic gboolean handle_keyboard (GIOChannel *source, GIOCondition cond, CustomData *data);\n\nint main(int argc, char *argv[]) {\n  CustomData data;\n  GstBus *bus;\n  GstStateChangeReturn ret;\n  gint flags;\n  GIOChannel *io_stdin;\n\n  /* Initialize GStreamer */\n  gst_init (&amp;argc, &amp;argv);\n\n  /* Create the elements */\n  data.playbin = gst_element_factory_make (\"playbin\", \"playbin\");\n\n  if (!data.playbin) {\n    g_printerr (\"Not all elements could be created.\\n\");\n    return -1;\n  }\n\n  /* Set the URI to play */\n  g_object_set (data.playbin, \"uri\", \"https://www.freedesktop.org/software/gstreamer-sdk/data/media/sintel_cropped_multilingual.webm\", NULL);\n\n  /* Set flags to show Audio and Video but ignore Subtitles */\n  g_object_get (data.playbin, \"flags\", &amp;flags, NULL);\n  flags |= GST_PLAY_FLAG_VIDEO | GST_PLAY_FLAG_AUDIO;\n  flags &amp;= ~GST_PLAY_FLAG_TEXT;\n  g_object_set (data.playbin, \"flags\", flags, NULL);\n\n  /* Set connection speed. This will affect some internal decisions of playbin */\n  g_object_set (data.playbin, \"connection-speed\", 56, NULL);\n\n  /* Add a bus watch, so we get notified when a message arrives */\n  bus = gst_element_get_bus (data.playbin);\n  gst_bus_add_watch (bus, (GstBusFunc)handle_message, &amp;data);\n\n  /* Add a keyboard watch so we get notified of keystrokes */\n#ifdef G_OS_WIN32\n  io_stdin = g_io_channel_win32_new_fd (fileno (stdin));\n#else\n  io_stdin = g_io_channel_unix_new (fileno (stdin));\n#endif\n  g_io_add_watch (io_stdin, G_IO_IN, (GIOFunc)handle_keyboard, &amp;data);\n\n  /* Start playing */\n  ret = gst_element_set_state (data.playbin, GST_STATE_PLAYING);\n  if (ret == GST_STATE_CHANGE_FAILURE) {\n    g_printerr (\"Unable to set the pipeline to the playing state.\\n\");\n    gst_object_unref (data.playbin);\n    return -1;\n  }\n\n  /* Create a GLib Main Loop and set it to run */\n  data.main_loop = g_main_loop_new (NULL, FALSE);\n  g_main_loop_run (data.main_loop);\n\n  /* Free resources */\n  g_main_loop_unref (data.main_loop);\n  g_io_channel_unref (io_stdin);\n  gst_object_unref (bus);\n  gst_element_set_state (data.playbin, GST_STATE_NULL);\n  gst_object_unref (data.playbin);\n  return 0;\n}\n\n/* Extract some metadata from the streams and print it on the screen */\nstatic void analyze_streams (CustomData *data) {\n  gint i;\n  GstTagList *tags;\n  gchar *str;\n  guint rate;\n\n  /* Read some properties */\n  g_object_get (data-&gt;playbin, \"n-video\", &amp;data-&gt;n_video, NULL);\n  g_object_get (data-&gt;playbin, \"n-audio\", &amp;data-&gt;n_audio, NULL);\n  g_object_get (data-&gt;playbin, \"n-text\", &amp;data-&gt;n_text, NULL);\n\n  g_print (\"%d video stream(s), %d audio stream(s), %d text stream(s)\\n\",\n    data-&gt;n_video, data-&gt;n_audio, data-&gt;n_text);\n\n  g_print (\"\\n\");\n  for (i = 0; i &lt; data-&gt;n_video; i++) {\n    tags = NULL;\n    /* Retrieve the stream's video tags */\n    g_signal_emit_by_name (data-&gt;playbin, \"get-video-tags\", i, &amp;tags);\n    if (tags) {\n      g_print (\"video stream %d:\\n\", i);\n      gst_tag_list_get_string (tags, GST_TAG_VIDEO_CODEC, &amp;str);\n      g_print (\"  codec: %s\\n\", str ? str : \"unknown\");\n      g_free (str);\n      gst_tag_list_free (tags);\n    }\n  }\n\n  g_print (\"\\n\");\n  for (i = 0; i &lt; data-&gt;n_audio; i++) {\n    tags = NULL;\n    /* Retrieve the stream's audio tags */\n    g_signal_emit_by_name (data-&gt;playbin, \"get-audio-tags\", i, &amp;tags);\n    if (tags) {\n      g_print (\"audio stream %d:\\n\", i);\n      if (gst_tag_list_get_string (tags, GST_TAG_AUDIO_CODEC, &amp;str)) {\n        g_print (\"  codec: %s\\n\", str);\n        g_free (str);\n      }\n      if (gst_tag_list_get_string (tags, GST_TAG_LANGUAGE_CODE, &amp;str)) {\n        g_print (\"  language: %s\\n\", str);\n        g_free (str);\n      }\n      if (gst_tag_list_get_uint (tags, GST_TAG_BITRATE, &amp;rate)) {\n        g_print (\"  bitrate: %d\\n\", rate);\n      }\n      gst_tag_list_free (tags);\n    }\n  }\n\n  g_print (\"\\n\");\n  for (i = 0; i &lt; data-&gt;n_text; i++) {\n    tags = NULL;\n    /* Retrieve the stream's subtitle tags */\n    g_signal_emit_by_name (data-&gt;playbin, \"get-text-tags\", i, &amp;tags);\n    if (tags) {\n      g_print (\"subtitle stream %d:\\n\", i);\n      if (gst_tag_list_get_string (tags, GST_TAG_LANGUAGE_CODE, &amp;str)) {\n        g_print (\"  language: %s\\n\", str);\n        g_free (str);\n      }\n      gst_tag_list_free (tags);\n    }\n  }\n\n  g_object_get (data-&gt;playbin, \"current-video\", &amp;data-&gt;current_video, NULL);\n  g_object_get (data-&gt;playbin, \"current-audio\", &amp;data-&gt;current_audio, NULL);\n  g_object_get (data-&gt;playbin, \"current-text\", &amp;data-&gt;current_text, NULL);\n\n  g_print (\"\\n\");\n  g_print (\"Currently playing video stream %d, audio stream %d and text stream %d\\n\",\n    data-&gt;current_video, data-&gt;current_audio, data-&gt;current_text);\n  g_print (\"Type any number and hit ENTER to select a different audio stream\\n\");\n}\n\n/* Process messages from GStreamer */\nstatic gboolean handle_message (GstBus *bus, GstMessage *msg, CustomData *data) {\n  GError *err;\n  gchar *debug_info;\n\n  switch (GST_MESSAGE_TYPE (msg)) {\n    case GST_MESSAGE_ERROR:\n      gst_message_parse_error (msg, &amp;err, &amp;debug_info);\n      g_printerr (\"Error received from element %s: %s\\n\", GST_OBJECT_NAME (msg-&gt;src), err-&gt;message);\n      g_printerr (\"Debugging information: %s\\n\", debug_info ? debug_info : \"none\");\n      g_clear_error (&amp;err);\n      g_free (debug_info);\n      g_main_loop_quit (data-&gt;main_loop);\n      break;\n    case GST_MESSAGE_EOS:\n      g_print (\"End-Of-Stream reached.\\n\");\n      g_main_loop_quit (data-&gt;main_loop);\n      break;\n    case GST_MESSAGE_STATE_CHANGED: {\n      GstState old_state, new_state, pending_state;\n      gst_message_parse_state_changed (msg, &amp;old_state, &amp;new_state, &amp;pending_state);\n      if (GST_MESSAGE_SRC (msg) == GST_OBJECT (data-&gt;playbin)) {\n        if (new_state == GST_STATE_PLAYING) {\n          /* Once we are in the playing state, analyze the streams */\n          analyze_streams (data);\n        }\n      }\n    } break;\n  }\n\n  /* We want to keep receiving messages */\n  return TRUE;\n}\n\n/* Process keyboard input */\nstatic gboolean handle_keyboard (GIOChannel *source, GIOCondition cond, CustomData *data) {\n  gchar *str = NULL;\n\n  if (g_io_channel_read_line (source, &amp;str, NULL, NULL, NULL) == G_IO_STATUS_NORMAL) {\n    int index = g_ascii_strtoull (str, NULL, 0);\n    if (index &lt; 0 || index &gt;= data-&gt;n_audio) {\n      g_printerr (\"Index out of bounds\\n\");\n    } else {\n      /* If the input was a valid audio stream index, set the current audio stream */\n      g_print (\"Setting current audio stream to %d\\n\", index);\n      g_object_set (data-&gt;playbin, \"current-audio\", index, NULL);\n    }\n  }\n  g_free (str);\n  return TRUE;\n}\n</code></pre>\n<blockquote>\n<p><img src=\"images/icons/emoticons/information.png\" alt=\"information\" id=\"information1\"> If you need help to compile this code, refer to the\n<strong>Building the tutorials</strong> section for your platform: <a href=\"../../installing/on-mac-osx.html\">Mac</a> or\n<a href=\"../../installing/on-windows.html\">Windows</a> or use this specific command on Linux:</p>\n<p><code>gcc playback-tutorial-1.c -o playback-tutorial-1 `pkg-config --cflags --libs gstreamer-1.0`</code></p>\n<p>If you need help to run this code, refer to the <strong>Running the\ntutorials</strong> section for your platform: <a href=\"../../installing/on-mac-osx.html#building-the-tutorials\">Mac OS X</a>, <a href=\"../../installing/on-windows.html#running-the-tutorials\">Windows</a>, for\n<a href=\"../../installing/for-ios-development.html#building-the-tutorials\">iOS</a> or for <a href=\"../../installing/for-android-development.html#building-the-tutorials\">android</a>.</p>\n<p>This tutorial opens a window and displays a movie, with accompanying\naudio. The media is fetched from the Internet, so the window might take\na few seconds to appear, depending on your connection speed. The number\nof audio streams is shown in the terminal, and the user can switch from\none to another by entering a number and pressing enter. A small delay is\nto be expected.</p>\n<p>Bear in mind that there is no latency management (buffering), so on slow\nconnections, the movie might stop after a few seconds. See how [Tutorial\n12: Live streaming] solves this issue.</p>\n<p>Required libraries: <code>gstreamer-1.0</code></p>\n</blockquote>\n<h2 id=\"walkthrough\">Walkthrough</h2>\n<pre><code class=\"language-c\">/* Structure to contain all our information, so we can pass it around */\ntypedef struct _CustomData {\n  GstElement *playbin;  /* Our one and only element */\n\n  gint n_video;          /* Number of embedded video streams */\n  gint n_audio;          /* Number of embedded audio streams */\n  gint n_text;           /* Number of embedded subtitle streams */\n\n  gint current_video;    /* Currently playing video stream */\n  gint current_audio;    /* Currently playing audio stream */\n  gint current_text;     /* Currently playing subtitle stream */\n\n  GMainLoop *main_loop;  /* GLib's Main Loop */\n} CustomData;\n</code></pre>\n<p>We start, as usual, putting all our variables in a structure, so we can\npass it around to functions. For this tutorial, we need the amount of\nstreams of each type, and the currently playing one. Also, we are going\nto use a different mechanism to wait for messages that allows\ninteractivity, so we need a GLib's main loop object.</p>\n<pre><code class=\"language-c\">/* playbin flags */\ntypedef enum {\n  GST_PLAY_FLAG_VIDEO         = (1 &lt;&lt; 0), /* We want video output */\n  GST_PLAY_FLAG_AUDIO         = (1 &lt;&lt; 1), /* We want audio output */\n  GST_PLAY_FLAG_TEXT          = (1 &lt;&lt; 2)  /* We want subtitle output */\n} GstPlayFlags;\n</code></pre>\n<p>Later we are going to set some of <code>playbin</code>'s flags. We would like to\nhave a handy enum that allows manipulating these flags easily, but since\n<code>playbin</code> is a plug-in and not a part of the GStreamer core, this enum\nis not available to us. The \u201ctrick\u201d is simply to declare this enum in\nour code, as it appears in the <code>playbin</code> documentation: <code>GstPlayFlags</code>.\nGObject allows introspection, so the possible values for these flags can\nbe retrieved at runtime without using this trick, but in a far more\ncumbersome way.</p>\n<pre><code class=\"language-c\">/* Forward definition for the message and keyboard processing functions */\nstatic gboolean handle_message (GstBus *bus, GstMessage *msg, CustomData *data);\nstatic gboolean handle_keyboard (GIOChannel *source, GIOCondition cond, CustomData *data);\n</code></pre>\n<p>Forward declarations for the two callbacks we will be using.\n<code>handle_message</code> for the GStreamer messages, as we have already seen,\nand <code>handle_keyboard</code> for key strokes, since this tutorial is\nintroducing a limited amount of interactivity.</p>\n<p>We skip over the creation of the pipeline, the instantiation of\n<code>playbin</code> and pointing it to our test media through the <code>uri</code>\nproperty. <code>playbin</code> is in itself a pipeline, and in this case it is the\nonly element in the pipeline, so we skip completely the creation of the\npipeline, and use directly the  <code>playbin</code> element.</p>\n<p>We focus on some of the other properties of <code>playbin</code>, though:</p>\n<pre><code class=\"language-c\">/* Set flags to show Audio and Video but ignore Subtitles */\ng_object_get (data.playbin, \"flags\", &amp;flags, NULL);\nflags |= GST_PLAY_FLAG_VIDEO | GST_PLAY_FLAG_AUDIO;\nflags &amp;= ~GST_PLAY_FLAG_TEXT;\ng_object_set (data.playbin, \"flags\", flags, NULL);\n</code></pre>\n<p><code>playbin</code>'s behavior can be changed through its <code>flags</code> property, which\ncan have any combination of <code>GstPlayFlags</code>. The most interesting values\nare:</p>\n<table>\n<thead>\n<tr>\n<th> Flag</th>\n<th> Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td> GST_PLAY_FLAG_VIDEO</td>\n<td> Enable video rendering. If this flag is not set, there will be no video output.</td>\n</tr>\n<tr>\n<td> GST_PLAY_FLAG_AUDIO</td>\n<td> Enable audio rendering. If this flag is not set, there will be no audio output.</td>\n</tr>\n<tr>\n<td> GST_PLAY_FLAG_TEXT</td>\n<td> Enable subtitle rendering. If this flag is not set, subtitles will not be shown in the video output.</td>\n</tr>\n<tr>\n<td> GST_PLAY_FLAG_VIS</td>\n<td> Enable rendering of visualisations when there is no video stream. Playback tutorial 6: Audio visualization goes into more details.</td>\n</tr>\n<tr>\n<td> GST_PLAY_FLAG_DOWNLOAD</td>\n<td> See Basic tutorial 12: Streaming  and Playback tutorial 4: Progressive streaming.</td>\n</tr>\n<tr>\n<td> GST_PLAY_FLAG_BUFFERING</td>\n<td> See Basic tutorial 12: Streaming  and Playback tutorial 4: Progressive streaming.</td>\n</tr>\n<tr>\n<td> GST_PLAY_FLAG_DEINTERLACE</td>\n<td> If the video content was interlaced, this flag instructs playbin to deinterlace it before displaying it.</td>\n</tr></tbody></table>\n<p>In our case, for demonstration purposes, we are enabling audio and video\nand disabling subtitles, leaving the rest of flags to their default\nvalues (this is why we read the current value of the flags with\n<code>g_object_get()</code> before overwriting it with <code>g_object_set()</code>).</p>\n<pre><code class=\"language-c\">/* Set connection speed. This will affect some internal decisions of playbin */\ng_object_set (data.playbin, \"connection-speed\", 56, NULL);\n</code></pre>\n<p>This property is not really useful in this example.\n<code>connection-speed</code> informs <code>playbin</code> of the maximum speed of our network\nconnection, so, in case multiple versions of the requested media are\navailable in the server, <code>playbin</code> chooses the most appropriate. This is\nmostly used in combination with streaming protocols like <code>mms</code> or\n<code>rtsp</code>.</p>\n<p>We have set all these properties one by one, but we could have all of\nthem with a single call to <code>g_object_set()</code>:</p>\n<pre><code class=\"language-c\">g_object_set (data.playbin, \"uri\", \"https://www.freedesktop.org/software/gstreamer-sdk/data/media/sintel_cropped_multilingual.webm\", \"flags\", flags, \"connection-speed\", 56, NULL);\n</code></pre>\n<p>This is why <code>g_object_set()</code> requires a NULL as the last parameter.</p>\n<pre><code class=\"language-c\">  /* Add a keyboard watch so we get notified of keystrokes */\n#ifdef _WIN32\n  io_stdin = g_io_channel_win32_new_fd (fileno (stdin));\n#else\n  io_stdin = g_io_channel_unix_new (fileno (stdin));\n#endif\n  g_io_add_watch (io_stdin, G_IO_IN, (GIOFunc)handle_keyboard, &amp;data);\n</code></pre>\n<p>These lines connect a callback function to the standard input (the\nkeyboard). The mechanism shown here is specific to GLib, and not really\nrelated to GStreamer, so there is no point in going into much depth.\nApplications normally have their own way of handling user input, and\nGStreamer has little to do with it besides the Navigation interface\ndiscussed briefly in [Tutorial 17: DVD playback].</p>\n<pre><code class=\"language-c\">/* Create a GLib Main Loop and set it to run */\ndata.main_loop = g_main_loop_new (NULL, FALSE);\ng_main_loop_run (data.main_loop);\n</code></pre>\n<p>To allow interactivity, we will no longer poll the GStreamer bus\nmanually. Instead, we create a <code>GMainLoop</code>(GLib main loop) and set it\nrunning with <code>g_main_loop_run()</code>. This function blocks and will not\nreturn until <code>g_main_loop_quit()</code> is issued. In the meantime, it will\ncall the callbacks we have registered at the appropriate\ntimes: <code>handle_message</code> when a message appears on the bus, and\n<code>handle_keyboard</code> when the user presses any key.</p>\n<p>There is nothing new in handle_message, except that when the pipeline\nmoves to the PLAYING state, it will call the <code>analyze_streams</code> function:</p>\n<pre><code class=\"language-c\">/* Extract some metadata from the streams and print it on the screen */\nstatic void analyze_streams (CustomData *data) {\n  gint i;\n  GstTagList *tags;\n  gchar *str;\n  guint rate;\n\n  /* Read some properties */\n  g_object_get (data-&gt;playbin, \"n-video\", &amp;data-&gt;n_video, NULL);\n  g_object_get (data-&gt;playbin, \"n-audio\", &amp;data-&gt;n_audio, NULL);\n  g_object_get (data-&gt;playbin, \"n-text\", &amp;data-&gt;n_text, NULL);\n</code></pre>\n<p>As the comment says, this function just gathers information from the\nmedia and prints it on the screen. The number of video, audio and\nsubtitle streams is directly available through the <code>n-video</code>,\n<code>n-audio</code> and <code>n-text</code> properties.</p>\n<pre><code class=\"language-c\">for (i = 0; i &lt; data-&gt;n_video; i++) {\n  tags = NULL;\n  /* Retrieve the stream's video tags */\n  g_signal_emit_by_name (data-&gt;playbin, \"get-video-tags\", i, &amp;tags);\n  if (tags) {\n    g_print (\"video stream %d:\\n\", i);\n    gst_tag_list_get_string (tags, GST_TAG_VIDEO_CODEC, &amp;str);\n    g_print (\"  codec: %s\\n\", str ? str : \"unknown\");\n    g_free (str);\n    gst_tag_list_free (tags);\n  }\n}\n</code></pre>\n<p>Now, for each stream, we want to retrieve its metadata. Metadata is\nstored as tags in a <code>GstTagList</code> structure, which is a list of data\npieces identified by a name. The <code>GstTagList</code> associated with a stream\ncan be recovered with <code>g_signal_emit_by_name()</code>, and then individual\ntags are extracted with the <code>gst_tag_list_get_*</code> functions\nlike <code>gst_tag_list_get_string()</code> for example.</p>\n<blockquote>\n<p><img src=\"images/icons/emoticons/information.png\" alt=\"information\" id=\"information2\">\nThis rather unintuitive way of retrieving the tag list\nis called an Action Signal. Action signals are emitted by the\napplication to a specific element, which then performs an action and\nreturns a result. They behave like a dynamic function call, in which\nmethods of a class are identified by their name (the signal's name)\ninstead of their memory address. These signals are listed In the\ndocumentation along with the regular signals, and are tagged \u201cAction\u201d.\nSee <code>playbin</code>, for example.</p>\n</blockquote>\n<p><code>playbin</code> defines 3 action signals to retrieve metadata:\n<code>get-video-tags</code>, <code>get-audio-tags</code> and <code>get-text-tags</code>. The name if the\ntags is standardized, and the list can be found in the <code>GstTagList</code>\ndocumentation. In this example we are interested in the\n<code>GST_TAG_LANGUAGE_CODE</code> of the streams and their <code>GST_TAG_*_CODEC</code>\n(audio, video or text).</p>\n<pre><code class=\"language-c\">g_object_get (data-&gt;playbin, \"current-video\", &amp;data-&gt;current_video, NULL);\ng_object_get (data-&gt;playbin, \"current-audio\", &amp;data-&gt;current_audio, NULL);\ng_object_get (data-&gt;playbin, \"current-text\", &amp;data-&gt;current_text, NULL);\n</code></pre>\n<p>Once we have extracted all the metadata we want, we get the streams that\nare currently selected through 3 more properties of <code>playbin</code>:\n<code>current-video</code>, <code>current-audio</code> and <code>current-text</code>.</p>\n<p>It is interesting to always check the currently selected streams and\nnever make any assumption. Multiple internal conditions can make\n<code>playbin</code> behave differently in different executions. Also, the order in\nwhich the streams are listed can change from one run to another, so\nchecking the metadata to identify one particular stream becomes crucial.</p>\n<pre><code class=\"language-c\">/* Process keyboard input */\nstatic gboolean handle_keyboard (GIOChannel *source, GIOCondition cond, CustomData *data) {\n  gchar *str = NULL;\n\n  if (g_io_channel_read_line (source, &amp;str, NULL, NULL, NULL) == G_IO_STATUS_NORMAL) {\n    int index = g_ascii_strtoull (str, NULL, 0);\n    if (index &lt; 0 || index &gt;= data-&gt;n_audio) {\n      g_printerr (\"Index out of bounds\\n\");\n    } else {\n      /* If the input was a valid audio stream index, set the current audio stream */\n      g_print (\"Setting current audio stream to %d\\n\", index);\n      g_object_set (data-&gt;playbin, \"current-audio\", index, NULL);\n    }\n  }\n  g_free (str);\n  return TRUE;\n}\n</code></pre>\n<p>Finally, we allow the user to switch the running audio stream. This very\nbasic function just reads a string from the standard input (the\nkeyboard), interprets it as a number, and tries to set the\n<code>current-audio</code> property of <code>playbin</code> (which previously we have only\nread).</p>\n<p>Bear in mind that the switch is not immediate. Some of the previously\ndecoded audio will still be flowing through the pipeline, while the new\nstream becomes active and is decoded. The delay depends on the\nparticular multiplexing of the streams in the container, and the length\n<code>playbin</code> has selected for its internal queues (which depends on the\nnetwork conditions).</p>\n<p>If you execute the tutorial, you will be able to switch from one\nlanguage to another while the movie is running by pressing 0, 1 or 2\n(and ENTER). This concludes this tutorial.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>This tutorial has shown:</p>\n<ul>\n<li>\n<p>A few more of <code>playbin</code>'s properties: <code>flags</code>, <code>connection-speed</code>,\n<code>n-video</code>, <code>n-audio</code>, <code>n-text</code>, <code>current-video</code>, <code>current-audio</code> and\n<code>current-text</code>.</p>\n</li>\n<li>\n<p>How to retrieve the list of tags associated with a stream\nwith <code>g_signal_emit_by_name()</code>.</p>\n</li>\n<li>\n<p>How to retrieve a particular tag from the list with\n<code>gst_tag_list_get_string()</code>or <code>gst_tag_list_get_uint()</code></p>\n</li>\n<li>\n<p>How to switch the current audio simply by writing to the\n<code>current-audio</code> property.</p>\n</li>\n</ul>\n<p>The next playback tutorial shows how to handle subtitles, either\nembedded in the container or in an external file.</p>\n<p>Remember that attached to this page you should find the complete source\ncode of the tutorial and any accessory files needed to build it.</p>\n<p>It has been a pleasure having you here, and see you soon!</p>\n\n</div>\n\n\n\t"});