fragment_downloaded_cb({"url": "tutorials/basic/dynamic-pipelines.html#page-description", "fragment": "<div id=\"page-description\" data-hotdoc-source=\"dynamic-pipelines.md\">\n        <h1 id=\"basic-tutorial-3-dynamic-pipelines\">Basic tutorial 3: Dynamic pipelines</h1>\n<h2 id=\"goal\">Goal</h2>\n<p>This tutorial shows the rest of the basic concepts required to use\nGStreamer, which allow building the pipeline \"on the fly\", as\ninformation becomes available, instead of having a monolithic pipeline\ndefined at the beginning of your application.</p>\n<p>After this tutorial, you will have the necessary knowledge to start the\n<a href=\"../playback/index.html\">Playback tutorials</a>. The points reviewed\nhere will be:</p>\n<ul>\n<li>\n<p>How to attain finer control when linking elements.</p>\n</li>\n<li>\n<p>How to be notified of interesting events so you can react in time.</p>\n</li>\n<li>\n<p>The various states in which an element can be.</p>\n</li>\n</ul>\n<h2 id=\"introduction\">Introduction</h2>\n<p>As you are about to see, the pipeline in this tutorial is not\ncompletely built before it is set to the playing state. This is OK. If\nwe did not take further action, data would reach the end of the\npipeline and the pipeline would produce an error message and stop. But\nwe are going to take further action...</p>\n<p>In this example we are opening a file which is multiplexed (or <em>muxed)</em>,\nthis is, audio and video are stored together inside a <em>container</em> file.\nThe elements responsible for opening such containers are called\n<em>demuxers</em>, and some examples of container formats are Matroska (MKV),\nQuick Time (QT, MOV), Ogg, or Advanced Systems Format (ASF, WMV, WMA).</p>\n<p>If a container embeds multiple streams (one video and two audio tracks,\nfor example), the demuxer will separate them and expose them through\ndifferent output ports. In this way, different branches can be created\nin the pipeline, dealing with different types of data.</p>\n<p>The ports through which GStreamer elements communicate with each other\nare called pads (<code>GstPad</code>). There exists sink pads, through which data\nenters an element, and source pads, through which data exits an element.\nIt follows naturally that source elements only contain source pads, sink\nelements only contain sink pads, and filter elements contain\nboth.</p>\n<p><img src=\"images/src-element.png\" alt=\"\">\u00a0<img src=\"images/filter-element.png\" alt=\"\">\u00a0<img src=\"images/sink-element.png\" alt=\"\"></p>\n<p><strong>Figure 1</strong>. GStreamer elements with their pads.</p>\n<p>A demuxer contains one sink pad, through which the muxed data arrives,\nand multiple source pads, one for each stream found in the container:</p>\n<p><img src=\"images/filter-element-multi.png\" alt=\"\"></p>\n<p><strong>Figure 2</strong>. A demuxer with two source pads.</p>\n<p>For completeness, here you have a simplified pipeline containing a\ndemuxer and two branches, one for audio and one for video. This is\n<strong>NOT</strong> the pipeline that will be built in this example:</p>\n<p><img src=\"images/simple-player.png\" alt=\"\"></p>\n<p><strong>Figure 3</strong>. Example pipeline with two branches.</p>\n<p>The main complexity when dealing with demuxers is that they cannot\nproduce any information until they have received some data and have had\na chance to look at the container to see what is inside. This is,\ndemuxers start with no source pads to which other elements can link, and\nthus the pipeline must necessarily terminate at them.</p>\n<p>The solution is to build the pipeline from the source down to the\ndemuxer, and set it to run (play). When the demuxer has received enough\ninformation to know about the number and kind of streams in the\ncontainer, it will start creating source pads. This is the right time\nfor us to finish building the pipeline and attach it to the newly added\ndemuxer pads.</p>\n<p>For simplicity, in this example, we will only link to the audio pad and\nignore the video.</p>\n<h2 id=\"dynamic-hello-world\">Dynamic Hello World</h2>\n<p>Copy this code into a text file named <code>basic-tutorial-3.c</code> (or find it\nin the SDK installation).</p>\n<p><strong>basic-tutorial-3.c</strong></p>\n<pre><code class=\"language-c\">#include &lt;gst/gst.h&gt;\n\n/* Structure to contain all our information, so we can pass it to callbacks */\ntypedef struct _CustomData {\n  GstElement *pipeline;\n  GstElement *source;\n  GstElement *convert;\n  GstElement *sink;\n} CustomData;\n\n/* Handler for the pad-added signal */\nstatic void pad_added_handler (GstElement *src, GstPad *pad, CustomData *data);\n\nint main(int argc, char *argv[]) {\n  CustomData data;\n  GstBus *bus;\n  GstMessage *msg;\n  GstStateChangeReturn ret;\n  gboolean terminate = FALSE;\n\n  /* Initialize GStreamer */\n  gst_init (&amp;argc, &amp;argv);\n\n  /* Create the elements */\n  data.source = gst_element_factory_make (\"uridecodebin\", \"source\");\n  data.convert = gst_element_factory_make (\"audioconvert\", \"convert\");\n  data.sink = gst_element_factory_make (\"autoaudiosink\", \"sink\");\n\n  /* Create the empty pipeline */\n  data.pipeline = gst_pipeline_new (\"test-pipeline\");\n\n  if (!data.pipeline || !data.source || !data.convert || !data.sink) {\n    g_printerr (\"Not all elements could be created.\\n\");\n    return -1;\n  }\n\n  /* Build the pipeline. Note that we are NOT linking the source at this\n   * point. We will do it later. */\n  gst_bin_add_many (GST_BIN (data.pipeline), data.source, data.convert , data.sink, NULL);\n  if (!gst_element_link (data.convert, data.sink)) {\n    g_printerr (\"Elements could not be linked.\\n\");\n    gst_object_unref (data.pipeline);\n    return -1;\n  }\n\n  /* Set the URI to play */\n  g_object_set (data.source, \"uri\", \"https://www.freedesktop.org/software/gstreamer-sdk/data/media/sintel_trailer-480p.webm\", NULL);\n\n  /* Connect to the pad-added signal */\n  g_signal_connect (data.source, \"pad-added\", G_CALLBACK (pad_added_handler), &amp;data);\n\n  /* Start playing */\n  ret = gst_element_set_state (data.pipeline, GST_STATE_PLAYING);\n  if (ret == GST_STATE_CHANGE_FAILURE) {\n    g_printerr (\"Unable to set the pipeline to the playing state.\\n\");\n    gst_object_unref (data.pipeline);\n    return -1;\n  }\n\n  /* Listen to the bus */\n  bus = gst_element_get_bus (data.pipeline);\n  do {\n    msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE,\n        GST_MESSAGE_STATE_CHANGED | GST_MESSAGE_ERROR | GST_MESSAGE_EOS);\n\n    /* Parse message */\n    if (msg != NULL) {\n      GError *err;\n      gchar *debug_info;\n\n      switch (GST_MESSAGE_TYPE (msg)) {\n        case GST_MESSAGE_ERROR:\n          gst_message_parse_error (msg, &amp;err, &amp;debug_info);\n          g_printerr (\"Error received from element %s: %s\\n\", GST_OBJECT_NAME (msg-&gt;src), err-&gt;message);\n          g_printerr (\"Debugging information: %s\\n\", debug_info ? debug_info : \"none\");\n          g_clear_error (&amp;err);\n          g_free (debug_info);\n          terminate = TRUE;\n          break;\n        case GST_MESSAGE_EOS:\n          g_print (\"End-Of-Stream reached.\\n\");\n          terminate = TRUE;\n          break;\n        case GST_MESSAGE_STATE_CHANGED:\n          /* We are only interested in state-changed messages from the pipeline */\n          if (GST_MESSAGE_SRC (msg) == GST_OBJECT (data.pipeline)) {\n            GstState old_state, new_state, pending_state;\n            gst_message_parse_state_changed (msg, &amp;old_state, &amp;new_state, &amp;pending_state);\n            g_print (\"Pipeline state changed from %s to %s:\\n\",\n                gst_element_state_get_name (old_state), gst_element_state_get_name (new_state));\n          }\n          break;\n        default:\n          /* We should not reach here */\n          g_printerr (\"Unexpected message received.\\n\");\n          break;\n      }\n      gst_message_unref (msg);\n    }\n  } while (!terminate);\n\n  /* Free resources */\n  gst_object_unref (bus);\n  gst_element_set_state (data.pipeline, GST_STATE_NULL);\n  gst_object_unref (data.pipeline);\n  return 0;\n}\n\n/* This function will be called by the pad-added signal */\nstatic void pad_added_handler (GstElement *src, GstPad *new_pad, CustomData *data) {\n  GstPad *sink_pad = gst_element_get_static_pad (data-&gt;convert, \"sink\");\n  GstPadLinkReturn ret;\n  GstCaps *new_pad_caps = NULL;\n  GstStructure *new_pad_struct = NULL;\n  const gchar *new_pad_type = NULL;\n\n  g_print (\"Received new pad '%s' from '%s':\\n\", GST_PAD_NAME (new_pad), GST_ELEMENT_NAME (src));\n\n  /* If our converter is already linked, we have nothing to do here */\n  if (gst_pad_is_linked (sink_pad)) {\n    g_print (\"  We are already linked. Ignoring.\\n\");\n    goto exit;\n  }\n\n  /* Check the new pad's type */\n  new_pad_caps = gst_pad_query_caps (new_pad, NULL);\n  new_pad_struct = gst_caps_get_structure (new_pad_caps, 0);\n  new_pad_type = gst_structure_get_name (new_pad_struct);\n  if (!g_str_has_prefix (new_pad_type, \"audio/x-raw\")) {\n    g_print (\"  It has type '%s' which is not raw audio. Ignoring.\\n\", new_pad_type);\n    goto exit;\n  }\n\n  /* Attempt the link */\n  ret = gst_pad_link (new_pad, sink_pad);\n  if (GST_PAD_LINK_FAILED (ret)) {\n    g_print (\"  Type is '%s' but link failed.\\n\", new_pad_type);\n  } else {\n    g_print (\"  Link succeeded (type '%s').\\n\", new_pad_type);\n  }\n\nexit:\n  /* Unreference the new pad's caps, if we got them */\n  if (new_pad_caps != NULL)\n    gst_caps_unref (new_pad_caps);\n\n  /* Unreference the sink pad */\n  gst_object_unref (sink_pad);\n}\n</code></pre>\n<blockquote>\n<p><img src=\"images/icons/emoticons/information.png\" alt=\"Information\" id=\"information\">\nNeed help?</p>\n<p>If you need help to compile this code, refer to the <strong>Building the tutorials</strong>  section for your platform: <a href=\"../../installing/on-linux.html#InstallingonLinux-Build\">Linux</a>, <a href=\"../../installing/on-mac-osx.html#InstallingonMacOSX-Build\">Mac OS X</a> or <a href=\"../../installing/on-windows.html#InstallingonWindows-Build\">Windows</a>, or use this specific command on Linux:\n<code>gcc basic-tutorial-3.c -o basic-tutorial-3 `pkg-config --cflags --libs gstreamer-1.0`</code></p>\n<p>If you need help to run this code, refer to the <strong>Running the tutorials</strong> section for your platform: <a href=\"../../installing/on-linux.html#InstallingonLinux-Run\">Linux</a>, <a href=\"../../installing/on-mac-osx.html#InstallingonMacOSX-Run\">Mac OS X</a> or <a href=\"../../installing/on-windows.html#InstallingonWindows-Run\">Windows</a>.</p>\n<p>This tutorial only plays audio. The media is fetched from the Internet, so it might take a few seconds to start, depending on your connection speed.</p>\n<p>Required libraries: <code>gstreamer-1.0</code></p>\n</blockquote>\n<h2 id=\"walkthrough\">Walkthrough</h2>\n<pre><code class=\"language-c\">/* Structure to contain all our information, so we can pass it to callbacks */\ntypedef struct _CustomData {\n  GstElement *pipeline;\n  GstElement *source;\n  GstElement *convert;\n  GstElement *sink;\n} CustomData;\n</code></pre>\n<p>So far we have kept all the information we needed (pointers\nto <code>GstElement</code>s, basically) as local variables. Since this tutorial\n(and most real applications) involves callbacks, we will group all our\ndata in a structure for easier handling.</p>\n<pre><code class=\"language-c\">/* Handler for the pad-added signal */\nstatic void pad_added_handler (GstElement *src, GstPad *pad, CustomData *data);\n</code></pre>\n<p>This is a forward reference, to be used later.</p>\n<pre><code class=\"language-c\">/* Create the elements */\ndata.source = gst_element_factory_make (\"uridecodebin\", \"source\");\ndata.convert = gst_element_factory_make (\"audioconvert\", \"convert\");\ndata.sink = gst_element_factory_make (\"autoaudiosink\", \"sink\");\n</code></pre>\n<p>We create the elements as usual. <code>uridecodebin</code> will internally\ninstantiate all the necessary elements (sources, demuxers and decoders)\nto turn a URI into raw audio and/or video streams. It does half the work\nthat <code>playbin</code> does. Since it contains demuxers, its source pads are\nnot initially available and we will need to link to them on the fly.</p>\n<p><code>audioconvert</code> is useful for converting between different audio formats,\nmaking sure that this example will work on any platform, since the\nformat produced by the audio decoder might not be the same that the\naudio sink expects.</p>\n<p>The <code>autoaudiosink</code> is the equivalent of <code>autovideosink</code> seen in the\nprevious tutorial, for audio. It will render the audio stream to the\naudio card.</p>\n<pre><code class=\"language-c\">if (!gst_element_link (data.convert, data.sink)) {\n  g_printerr (\"Elements could not be linked.\\n\");\n  gst_object_unref (data.pipeline);\n  return -1;\n}\n</code></pre>\n<p>Here we link the converter element to the sink, but we <strong>DO NOT</strong> link\nthem with the source, since at this point it contains no source pads. We\njust leave this branch (converter + sink) unlinked, until later on.</p>\n<pre><code class=\"language-c\">/* Set the URI to play */\ng_object_set (data.source, \"uri\", \"https://www.freedesktop.org/software/gstreamer-sdk/data/media/sintel_trailer-480p.webm\", NULL);\n</code></pre>\n<p>We set the URI of the file to play via a property, just like we did in\nthe previous tutorial.</p>\n<h3 id=\"signals\">Signals</h3>\n<pre><code class=\"language-c\">/* Connect to the pad-added signal */\ng_signal_connect (data.source, \"pad-added\", G_CALLBACK (pad_added_handler), &amp;data);\n</code></pre>\n<p><code>GSignals</code> are a crucial point in GStreamer. They allow you to be\nnotified (by means of a callback) when something interesting has\nhappened. Signals are identified by a name, and each <code>GObject</code> has its\nown signals.</p>\n<p>In this line, we are <em>attaching</em> to the \u201cpad-added\u201d signal of our source\n(an <code>uridecodebin</code> element). To do so, we use <code>g_signal_connect()</code> and\nprovide the callback function to be used (<code>pad_added_handler</code>) and a\ndata pointer. GStreamer does nothing with this data pointer, it just\nforwards it to the callback so we can share information with it. In this\ncase, we pass a pointer to the <code>CustomData</code> structure we built specially\nfor this purpose.</p>\n<p>The signals that a <code>GstElement</code> generates can be found in its\ndocumentation or using the <code>gst-inspect-1.0</code> tool as described in <a href=\"gstreamer-tools.html\">Basic\ntutorial 10: GStreamer\ntools</a>.</p>\n<p>We are now ready to go! Just set the pipeline to the PLAYING state and\nstart listening to the bus for interesting messages (like ERROR or EOS),\njust like in the previous tutorials.</p>\n<h3 id=\"the-callback\">The callback</h3>\n<p>When our source element finally has enough information to start\nproducing data, it will create source pads, and trigger the \u201cpad-added\u201d\nsignal. At this point our callback will be\ncalled:</p>\n<pre><code class=\"language-c\">static void pad_added_handler (GstElement *src, GstPad *new_pad, CustomData *data) {\n</code></pre>\n<p><code>src</code> is the <code>GstElement</code> which triggered the signal. In this example,\nit can only be the <code>uridecodebin</code>, since it is the only signal to which\nwe have attached. The first parameter of a signal handler is always the object\nthat has triggered it.</p>\n<p><code>new_pad</code> is the <code>GstPad</code> that has just been added to the <code>src</code> element.\nThis is usually the pad to which we want to link.</p>\n<p><code>data</code> is the pointer we provided when attaching to the signal. In this\nexample, we use it to pass the <code>CustomData</code> pointer.</p>\n<pre><code class=\"language-c\">GstPad *sink_pad = gst_element_get_static_pad (data-&gt;convert, \"sink\");\n</code></pre>\n<p>From <code>CustomData</code> we extract the converter element, and then retrieve\nits sink pad using <code>gst_element_get_static_pad ()</code>. This is the pad to\nwhich we want to link <code>new_pad</code>. In the previous tutorial we linked\nelement against element, and let GStreamer choose the appropriate pads.\nNow we are going to link the pads directly.</p>\n<pre><code class=\"language-c\">/* If our converter is already linked, we have nothing to do here */\nif (gst_pad_is_linked (sink_pad)) {\n  g_print (\"  We are already linked. Ignoring.\\n\");\n  goto exit;\n}\n</code></pre>\n<p><code>uridecodebin</code> can create as many pads as it sees fit, and for each one,\nthis callback will be called. These lines of code will prevent us from\ntrying to link to a new pad once we are already linked.</p>\n<pre><code class=\"language-c\">/* Check the new pad's type */\nnew_pad_caps = gst_pad_query_caps (new_pad, NULL);\nnew_pad_struct = gst_caps_get_structure (new_pad_caps, 0);\nnew_pad_type = gst_structure_get_name (new_pad_struct);\nif (!g_str_has_prefix (new_pad_type, \"audio/x-raw\")) {\n  g_print (\"  It has type '%s' which is not raw audio. Ignoring.\\n\", new_pad_type);\n  goto exit;\n}\n</code></pre>\n<p>Now we will check the type of data this new pad is going to output,\nbecause we are only interested in pads producing audio. We have\npreviously created a piece of pipeline which deals with audio (an\n<code>audioconvert</code> linked with an <code>autoaudiosink</code>), and we will not be able\nto link it to a pad producing video, for example.</p>\n<p><code>gst_pad_query_caps()</code> retrieves the <em>capabilities</em> of the pad (this is,\nthe kind of data it supports), wrapped in a <code>GstCaps</code> structure. A pad\ncan offer many capabilities, and hence <code>GstCaps</code> can contain many\n<code>GstStructure</code>, each representing a different capability.</p>\n<p>Since, in this case, we know that the pad we want only had one\ncapability (audio), we retrieve the first <code>GstStructure</code> with\n<code>gst_caps_get_structure()</code>.</p>\n<p>Finally, with <code>gst_structure_get_name()</code> we recover the name of the\nstructure, which contains the main description of the format (its <em>media\ntype</em>, actually).</p>\n<p>If the name is not <code>audio/x-raw</code>, this is not a decoded\naudio pad, and we are not interested in it.</p>\n<p>Otherwise, attempt the link:</p>\n<pre><code class=\"language-c\">/* Attempt the link */\nret = gst_pad_link (new_pad, sink_pad);\nif (GST_PAD_LINK_FAILED (ret)) {\n  g_print (\"  Type is '%s' but link failed.\\n\", new_pad_type);\n} else {\n  g_print (\"  Link succeeded (type '%s').\\n\", new_pad_type);\n}\n</code></pre>\n<p><code>gst_pad_link()</code> tries to link two pads. As it was the case\nwith <code>gst_element_link()</code>, the link must be specified from source to\nsink, and both pads must be owned by elements residing in the same bin\n(or pipeline).</p>\n<p>And we are done! When a pad of the right kind appears, it will be\nlinked to the rest of the audio-processing pipeline and execution will\ncontinue until ERROR or EOS. However, we will squeeze a bit more content\nfrom this tutorial by also introducing the concept of State.</p>\n<h3 id=\"gstreamer-states\">GStreamer States</h3>\n<p>We already talked a bit about states when we said that playback does not\nstart until you bring the pipeline to the PLAYING state. We will\nintroduce here the rest of states and their meaning. There are 4 states\nin GStreamer:</p>\n<table>\n<thead>\n<tr>\n<th> State</th>\n<th> Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td> <code>NULL</code></td>\n<td> the NULL state or initial state of an element.</td>\n</tr>\n<tr>\n<td> <code>READY</code></td>\n<td> the element is ready to go to PAUSED.</td>\n</tr>\n<tr>\n<td> <code>PAUSED</code></td>\n<td> the element is PAUSED, it is ready to accept and process data. Sink elements however only accept one buffer and then block.</td>\n</tr>\n<tr>\n<td> <code>PLAYING</code></td>\n<td> the element is PLAYING, the clock is running and the data is flowing.</td>\n</tr></tbody></table>\n<p>You can only move between adjacent ones, this is, you can't go from NULL\nto PLAYING, you have to go through the intermediate READY and PAUSED\nstates. If you set the pipeline to PLAYING, though, GStreamer will make\nthe intermediate transitions for you.</p>\n<pre><code class=\"language-c\">case GST_MESSAGE_STATE_CHANGED:\n  /* We are only interested in state-changed messages from the pipeline */\n  if (GST_MESSAGE_SRC (msg) == GST_OBJECT (data.pipeline)) {\n    GstState old_state, new_state, pending_state;\n    gst_message_parse_state_changed (msg, &amp;old_state, &amp;new_state, &amp;pending_state);\n    g_print (\"Pipeline state changed from %s to %s:\\n\",\n        gst_element_state_get_name (old_state), gst_element_state_get_name (new_state));\n  }\n  break;\n</code></pre>\n<p>We added this piece of code that listens to bus messages regarding state\nchanges and prints them on screen to help you understand the\ntransitions. Every element puts messages on the bus regarding its\ncurrent state, so we filter them out and only listen to messages coming\nfrom the pipeline.</p>\n<p>Most applications only need to worry about going to PLAYING to start\nplayback, then to PAUSE to perform a pause, and then back to NULL at\nprogram exit to free all resources.</p>\n<h2 id=\"exercise\">Exercise</h2>\n<p>Dynamic pad linking has traditionally been a difficult topic for a lot\nof programmers. Prove that you have achieved its mastery by\ninstantiating an <code>autovideosink</code> (probably with an <code>videoconvert</code> in\nfront) and link it to the demuxer when the right pad appears. Hint: You\nare already printing on screen the type of the video pads.</p>\n<p>You should now see (and hear) the same movie as in <a href=\"hello-world.html\">Basic tutorial 1:\nHello world!</a>. In\nthat tutorial you used <code>playbin</code>, which is a handy element that\nautomatically takes care of all the demuxing and pad linking for you.\nMost of the <a href=\"../playback/index.html\">Playback tutorials</a> are devoted\nto <code>playbin</code>.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>In this tutorial, you learned:</p>\n<ul>\n<li>How to be notified of events using <code>GSignals</code></li>\n<li>How to connect <code>GstPad</code>s directly instead of their parent elements.</li>\n<li>The various states of a GStreamer element.</li>\n</ul>\n<p>You also combined these items to build a dynamic pipeline, which was not\ndefined at program start, but was created as information regarding the\nmedia was available.</p>\n<p>You can now continue with the basic tutorials and learn about performing\nseeks and time-related queries in <a href=\"time-management.html\">Basic tutorial 4: Time\nmanagement</a> or move\nto the <a href=\"../playback/index.html\">Playback tutorials</a>, and gain more\ninsight about the <code>playbin</code> element.</p>\n<p>Remember that attached to this page you should find the complete source\ncode of the tutorial and any accessory files needed to build it.\nIt has been a pleasure having you here, and see you soon!</p>\n\n        \n\n    </div>\n\n\n        "});