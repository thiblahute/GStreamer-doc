fragment_downloaded_cb({"url": "design/subtitle-overlays.html#possible-solutions", "fragment": "Possible Solutions \nThe goal is to keep knowledge of the subtitle format within the format specific GStreamer plugins and knowledge of any specific video acceleration API to the GStreamer plugins implementing that API. We do not want to make the pango dvbsuboverlay dvdspu kate plugins link to libva libvdpau etc. and we do not want to make the vaapi vdpau plugins link to all of libpango libkate libass etc. \nMultiple possible solutions come to mind \nbackend specific overlay elements \ne.g. vaapitextoverlay vdpautextoverlay vaapidvdspu vdpaudvdspu vaapidvbsuboverlay vdpaudvbsuboverlay etc. \nThis assumes the overlay can be done directly on the backend specific object passed around. \nThe main drawback with this solution is that it leads to a lot of code duplication and may also lead to uncertainty about distributing certain duplicated pieces of code. The code duplication is pretty much unavoidable since making textoverlay dvbsuboverlay dvdspu kate assrender etc. available in form of base classes to derive from is not really an option. Similarly one would not really want the vaapi vdpau plugin to depend on a bunch of other libraries such as libpango libkate libtiger libass etc. \nOne could add some new kind of overlay plugin feature though in combination with a generic base class of some sort but in order to accommodate all the different cases and formats one would end up with quite convoluted tricky API. \nOf course there could also be a GstFancyVideoBuffer that provides an abstraction for such video accelerated objects and that could provide an API to add overlays to it in a generic way but in the end this is just a less generic variant of c and it is not clear that there are real benefits to a specialised solution vs. a more generic one \nconvert backend specific object to raw pixels and then overlay \nEven where possible technically this is most likely very inefficient. \nattach the overlay data to the backend specific video frame buffers in a generic way and do the actual overlaying blitting later in backend specific code such as the video sink or an accelerated encoder transcoder \nIn this case the actual overlay rendering i.e. the actual text rendering or decoding DVD DVB data into pixels is done in the subtitle format specific GStreamer plugin. All knowledge about the subtitle format is contained in the overlay plugin then and all knowledge about the video backend in the video backend specific plugin. \nThe main question then is how to get the overlay pixels and we will only deal with pixels here from the overlay element to the video sink. \nThis could be done in multiple ways One could send custom events downstream with the overlay data or one could attach the overlay data directly to the video buffers in some way. \nSending inline events has the advantage that is is fairly transparent to any elements between the overlay element and the video sink if an effects plugin creates a new video buffer for the output nothing special needs to be done to maintain the subtitle overlay information since the overlay data is not attached to the buffer. However it slightly complicates things at the sink since it would also need to look for the new event in question instead of just processing everything in its buffer render function. \nIf one attaches the overlay data to the buffer directly any element between overlay and video sink that creates a new video buffer would need to be aware of the overlay data attached to it and copy it over to the newly created buffer. \nOne would have to do implement a special kind of new query e.g. FEATURE query that is not passed on automatically by gst_pad_query_default in order to make sure that all elements downstream will handle the attached overlay data. This is only a problem if we want to also attach overlay data to raw video pixel buffers for new non raw types we can just make it mandatory and assume support and be done with it for existing non raw types nothing changes anyway if subtitles don t work we need to maintain backwards compatibility for existing raw video pipelines like e.g. decoder suboverlay encoder.. \nEven though slightly more work attaching the overlay information to buffers seems more intuitive than sending it interleaved as events. And buffers stored or passed around e.g. via the last buffer property in the sink when doing screenshots via playbin always contain all the information needed. \ncreate a video x raw delta format and use a backend specific videomixer \nThis possibility was hinted at already in the digression in section \nOverall c appears to be the most promising solution. It is the least intrusive and should be fairly straight forward to implement with reasonable effort requiring only small changes to existing elements and requiring no new elements. \nDoing the final overlaying in the sink as opposed to a videomixer or overlay in the middle of the pipeline has other advantages \nif video frames need to be dropped e.g. for QoS reasons we could also skip the actual subtitle overlaying and possibly the decoding rendering as well if the implementation and API allows for that to be delayed. \nthe sink often knows the actual size of the window surface screen the output video is rendered to. This may make it possible to render the overlay image in a higher resolution than the input video solving a long standing issue with pixelated subtitles on top of low resolution videos that are then scaled up in the sink. This would require for the rendering to be delayed of course instead of just attaching an AYUV ARGB RGBA blog of pixels to the video buffer in the overlay but that could all be supported. \nif the video backend sink has support for high quality text rendering clutter we could just pass the text or pango markup to the sink and let it do the rest this is unlikely to be supported in the general case text and glyph rendering is hard also we don t really want to make up our own text markup system and pango markup is probably too limited for complex karaoke stuff \n"});