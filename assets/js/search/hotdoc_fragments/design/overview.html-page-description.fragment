fragment_downloaded_cb({"url": "design/overview.html#page-description", "fragment": "<div id=\"page-description\" data-hotdoc-source=\"overview.md\">\n<h1 id=\"overview\">Overview</h1>\n<p>This part gives an overview of the design of GStreamer with references\nto the more detailed explanations of the different topics.</p>\n<p>This document is intented for people that want to have a global overview\nof the inner workings of GStreamer.</p>\n<h2 id=\"introduction\">Introduction</h2>\n<p>GStreamer is a set of libraries and plugins that can be used to\nimplement various multimedia applications ranging from desktop players,\naudio/video recorders, multimedia servers, transcoders, etc.</p>\n<p>Applications are built by constructing a pipeline composed of elements.\nAn element is an object that performs some action on a multimedia stream\nsuch as:</p>\n<ul>\n<li>read a file</li>\n<li>decode or encode between formats</li>\n<li>capture from a hardware device</li>\n<li>render to a hardware device</li>\n<li>mix or multiplex multiple streams</li>\n</ul>\n<p>Elements have input and output pads called sink and source pads in\nGStreamer. An application links elements together on pads to construct a\npipeline. Below is an example of an ogg/vorbis playback pipeline.</p>\n<pre><code>+-----------------------------------------------------------+\n|    ----------&gt; downstream -------------------&gt;            |\n|                                                           |\n| pipeline                                                  |\n| +---------+   +----------+   +-----------+   +----------+ |\n| | filesrc |   | oggdemux |   | vorbisdec |   | alsasink | |\n| |        src-sink       src-sink        src-sink        | |\n| +---------+   +----------+   +-----------+   +----------+ |\n|                                                           |\n|    &lt;---------&lt; upstream &lt;-------------------&lt;             |\n+-----------------------------------------------------------+\n</code></pre>\n<p>The filesrc element reads data from a file on disk. The oggdemux element\nparses the data and sends the compressed audio data to the vorbisdec\nelement. The vorbisdec element decodes the compressed data and sends it\nto the alsasink element. The alsasink element sends the samples to the\naudio card for playback.</p>\n<p>Downstream and upstream are the terms used to describe the direction in\nthe Pipeline. From source to sink is called \"downstream\" and \"upstream\"\nis from sink to source. Dataflow always happens downstream.</p>\n<p>The task of the application is to construct a pipeline as above using\nexisting elements. This is further explained in the pipeline building\ntopic.</p>\n<p>The application does not have to manage any of the complexities of the\nactual dataflow/decoding/conversions/synchronisation etc. but only calls\nhigh level functions on the pipeline object such as PLAY/PAUSE/STOP.</p>\n<p>The application also receives messages and notifications from the\npipeline such as metadata, warning, error and EOS messages.</p>\n<p>If the application needs more control over the graph it is possible to\ndirectly access the elements and pads in the pipeline.</p>\n<h2 id=\"design-overview\">Design overview</h2>\n<p>GStreamer design goals include:</p>\n<ul>\n<li>Process large amounts of data quickly</li>\n<li>Allow fully multithreaded processing</li>\n<li>Ability to deal with multiple formats</li>\n<li>Synchronize different dataflows</li>\n<li>Ability to deal with multiple devices</li>\n</ul>\n<p>The capabilities presented to the application depends on the number of\nelements installed on the system and their functionality.</p>\n<p>The GStreamer core is designed to be media agnostic but provides many\nfeatures to elements to describe media formats.</p>\n<h2 id=\"elements\">Elements</h2>\n<p>The smallest building blocks in a pipeline are elements. An element\nprovides a number of pads which can be source or sinkpads. Sourcepads\nprovide data and sinkpads consume data. Below is an example of an ogg\ndemuxer element that has one pad that takes (sinks) data and two source\npads that produce data.</p>\n<pre><code> +-----------+\n | oggdemux  |\n |          src0\nsink        src1\n +-----------+\n</code></pre>\n<p>An element can be in four different states: NULL, READY, PAUSED,\nPLAYING. In the NULL and READY state, the element is not processing any\ndata. In the PLAYING state it is processing data. The intermediate\nPAUSED state is used to preroll data in the pipeline. A state change can\nbe performed with <code>gst_element_set_state()</code>.</p>\n<p>An element always goes through all the intermediate state changes. This\nmeans that when en element is in the READY state and is put to PLAYING,\nit will first go through the intermediate PAUSED state.</p>\n<p>An element state change to PAUSED will activate the pads of the element.\nFirst the source pads are activated, then the sinkpads. When the pads\nare activated, the pad activate function is called. Some pads will start\na thread (<code>GstTask</code>) or some other mechanism to start producing or\nconsuming data.</p>\n<p>The PAUSED state is special as it is used to preroll data in the\npipeline. The purpose is to fill all connected elements in the pipeline\nwith data so that the subsequent PLAYING state change happens very\nquickly. Some elements will therefore not complete the state change to\nPAUSED before they have received enough data. Sink elements are required\nto only complete the state change to PAUSED after receiving the first\ndata.</p>\n<p>Normally the state changes of elements are coordinated by the pipeline\nas explained in <a href=\"states.html\">states</a>.</p>\n<p>Different categories of elements exist:</p>\n<ul>\n<li>\n<p><em>source elements</em>: these are elements that do not consume data but\nonly provide data for the pipeline.</p>\n</li>\n<li>\n<p><em>sink elements</em>: these are elements that do not produce data but\nrenders data to an output device.</p>\n</li>\n<li>\n<p><em>transform elements</em>: these elements transform an input stream in a\ncertain format into a stream of another format.\nEncoder/decoder/converters are examples.</p>\n</li>\n<li>\n<p><em>demuxer elements</em>: these elements parse a stream and produce several\noutput streams.</p>\n</li>\n<li>\n<p><em>mixer/muxer elements</em>: combine several input streams into one output\nstream.</p>\n</li>\n</ul>\n<p>Other categories of elements can be constructed (see <a href=\"draft-klass.html\">klass</a>).</p>\n<h2 id=\"bins\">Bins</h2>\n<p>A bin is an element subclass and acts as a container for other elements\nso that multiple elements can be combined into one element.</p>\n<p>A bin coordinates its children\u2019s state changes as explained later. It\nalso distributes events and various other functionality to elements.</p>\n<p>A bin can have its own source and sinkpads by ghostpadding one or more\nof its children\u2019s pads to itself.</p>\n<p>Below is a picture of a bin with two elements. The sinkpad of one\nelement is ghostpadded to the bin.</p>\n<pre><code> +---------------------------+\n | bin                       |\n |    +--------+   +-------+ |\n |    |        |   |       | |\n |  /sink     src-sink     | |\nsink  +--------+   +-------+ |\n +---------------------------+\n</code></pre>\n<h2 id=\"pipeline\">Pipeline</h2>\n<p>A pipeline is a special bin subclass that provides the following\nfeatures to its children:</p>\n<ul>\n<li>Select and manage a global clock for all its children.</li>\n<li>Manage <code>running_time</code> based on the selected clock. Running_time is\nthe elapsed time the pipeline spent in the PLAYING state and is used\nfor synchronisation.</li>\n<li>Manage latency in the pipeline.</li>\n<li>Provide means for elements to comunicate with the application by the\n<code>GstBus</code>.</li>\n<li>Manage the global state of the elements such as Errors and\nend-of-stream.</li>\n</ul>\n<p>Normally the application creates one pipeline that will manage all the\nelements in the application.</p>\n<h2 id=\"dataflow-and-buffers\">Dataflow and buffers</h2>\n<p>GStreamer supports two possible types of dataflow, the push and pull\nmodel. In the push model, an upstream element sends data to a downstream\nelement by calling a method on a sinkpad. In the pull model, a\ndownstream element requests data from an upstream element by calling a\nmethod on a source pad.</p>\n<p>The most common dataflow is the push model. The pull model can be used\nin specific circumstances by demuxer elements. The pull model can also\nbe used by low latency audio applications.</p>\n<p>The data passed between pads is encapsulated in Buffers. The buffer\ncontains pointers to the actual memory and also metadata describing the\nmemory. This metadata includes:</p>\n<ul>\n<li>\n<p>timestamp of the data, this is the time instance at which the data\nwas captured or the time at which the data should be played back.</p>\n</li>\n<li>\n<p>offset of the data: a media specific offset, this could be samples\nfor audio or frames for video.</p>\n</li>\n<li>\n<p>the duration of the data in time.</p>\n</li>\n<li>\n<p>additional flags describing special properties of the data such as\ndiscontinuities or delta units.</p>\n</li>\n<li>\n<p>additional arbitrary metadata</p>\n</li>\n</ul>\n<p>When an element whishes to send a buffer to another element is does this\nusing one of the pads that is linked to a pad of the other element. In\nthe push model, a buffer is pushed to the peer pad with\n<code>gst_pad_push()</code>. In the pull model, a buffer is pulled from the peer\nwith the <code>gst_pad_pull_range()</code> function.</p>\n<p>Before an element pushes out a buffer, it should make sure that the peer\nelement can understand the buffer contents. It does this by querying the\npeer element for the supported formats and by selecting a suitable\ncommon format. The selected format is then first sent to the peer\nelement with a CAPS event before pushing the buffer (see\n<a href=\"negotiation.html\">negotiation</a>).</p>\n<p>When an element pad receives a CAPS event, it has to check if it\nunderstand the media type. The element must refuse following buffers if\nthe media type preceding it was not accepted.</p>\n<p>Both <code>gst_pad_push()</code> and <code>gst_pad_pull_range()</code> have a return value\nindicating whether the operation succeeded. An error code means that no\nmore data should be sent to that pad. A source element that initiates\nthe data flow in a thread typically pauses the producing thread when\nthis happens.</p>\n<p>A buffer can be created with <code>gst_buffer_new()</code> or by requesting a\nusable buffer from a buffer pool using\n<code>gst_buffer_pool_acquire_buffer()</code>. Using the second method, it is\npossible for the peer element to implement a custom buffer allocation\nalgorithm.</p>\n<p>The process of selecting a media type is called caps negotiation.</p>\n<h2 id=\"caps\">Caps</h2>\n<p>A media type (Caps) is described using a generic list of key/value\npairs. The key is a string and the value can be a single/list/range of\nint/float/string.</p>\n<p>Caps that have no ranges/list or other variable parts are said to be\nfixed and can be used to put on a buffer.</p>\n<p>Caps with variables in them are used to describe possible media types\nthat can be handled by a pad.</p>\n<h2 id=\"dataflow-and-events\">Dataflow and events</h2>\n<p>Parallel to the dataflow is a flow of events. Unlike the buffers, events\ncan pass both upstream and downstream. Some events only travel upstream\nothers only downstream.</p>\n<p>The events are used to denote special conditions in the dataflow such as\nEOS or to inform plugins of special events such as flushing or seeking.</p>\n<p>Some events must be serialized with the buffer flow, others don\u2019t.\nSerialized events are inserted between the buffers. Non serialized\nevents jump in front of any buffers current being processed.</p>\n<p>An example of a serialized event is a TAG event that is inserted between\nbuffers to mark metadata for those buffers.</p>\n<p>An example of a non serialized event is the FLUSH event.</p>\n<h2 id=\"pipeline-construction\">Pipeline construction</h2>\n<p>The application starts by creating a Pipeline element using\n<code>gst_pipeline_new()</code>. Elements are added to and removed from the\npipeline with <code>gst_bin_add()</code> and <code>gst_bin_remove()</code>.</p>\n<p>After adding the elements, the pads of an element can be retrieved with\n<code>gst_element_get_pad()</code>. Pads can then be linked together with\n<code>gst_pad_link()</code>.</p>\n<p>Some elements create new pads when actual dataflow is happening in the\npipeline. With <code>g_signal_connect()</code> one can receive a notification when\nan element has created a pad. These new pads can then be linked to other\nunlinked pads.</p>\n<p>Some elements cannot be linked together because they operate on\ndifferent incompatible data types. The possible datatypes a pad can\nprovide or consume can be retrieved with <code>gst_pad_get_caps()</code>.</p>\n<p>Below is a simple mp3 playback pipeline that we constructed. We will use\nthis pipeline in further examples.</p>\n<pre><code>+-------------------------------------------+\n| pipeline                                  |\n| +---------+   +----------+   +----------+ |\n| | filesrc |   | mp3dec   |   | alsasink | |\n| |        src-sink       src-sink        | |\n| +---------+   +----------+   +----------+ |\n+-------------------------------------------+\n</code></pre>\n<h2 id=\"pipeline-clock\">Pipeline clock</h2>\n<p>One of the important functions of the pipeline is to select a global\nclock for all the elements in the pipeline.</p>\n<p>The purpose of the clock is to provide a stricly increasing value at the\nrate of one <code>GST_SECOND</code> per second. Clock values are expressed in\nnanoseconds. Elements use the clock time to synchronize the playback of\ndata.</p>\n<p>Before the pipeline is set to PLAYING, the pipeline asks each element if\nthey can provide a clock. The clock is selected in the following order:</p>\n<ul>\n<li>\n<p>If the application selected a clock, use that one.</p>\n</li>\n<li>\n<p>If a source element provides a clock, use that clock.</p>\n</li>\n<li>\n<p>Select a clock from any other element that provides a clock, start\nwith the sinks.</p>\n</li>\n<li>\n<p>If no element provides a clock a default system clock is used for\nthe pipeline.</p>\n</li>\n</ul>\n<p>In a typical playback pipeline this algorithm will select the clock\nprovided by a sink element such as an audio sink.</p>\n<p>In capture pipelines, this will typically select the clock of the data\nproducer, which in most cases can not control the rate at which it\nproduces data.</p>\n<h2 id=\"pipeline-states\">Pipeline states</h2>\n<p>When all the pads are linked and signals have been connected, the\npipeline can be put in the PAUSED state to start dataflow.</p>\n<p>When a bin (and hence a pipeline) performs a state change, it will\nchange the state of all its children. The pipeline will change the state\nof its children from the sink elements to the source elements, this to\nmake sure that no upstream element produces data to an element that is\nnot yet ready to accept it.</p>\n<p>In the mp3 playback pipeline, the state of the elements is changed in\nthe order alsasink, mp3dec, filesrc.</p>\n<p>All intermediate states are traversed for each element resulting in the\nfollowing chain of state changes:</p>\n<ul>\n<li>\n<p>alsasink to READY:  the audio device is probed</p>\n</li>\n<li>\n<p>mp3dec to READY:    nothing happens.</p>\n</li>\n<li>\n<p>filesrc to READY:   the file is probed</p>\n</li>\n<li>\n<p>alsasink to PAUSED: the audio device is opened. alsasink is a sink and returns ASYNC because it did not receive data yet. mp3dec to PAUSED:   the decoding library is initialized</p>\n</li>\n<li>\n<p>filesrc to PAUSED:  the file is opened and a thread is started to push data to mp3dec</p>\n</li>\n</ul>\n<p>At this point data flows from filesrc to mp3dec and alsasink. Since\nmp3dec is PAUSED, it accepts the data from filesrc on the sinkpad and\nstarts decoding the compressed data to raw audio samples.</p>\n<p>The mp3 decoder figures out the samplerate, the number of channels and\nother audio properties of the raw audio samples and sends out a caps\nevent with the media type.</p>\n<p>Alsasink then receives the caps event, inspects the caps and\nreconfigures itself to process the media type.</p>\n<p>mp3dec then puts the decoded samples into a Buffer and pushes this\nbuffer to the next element.</p>\n<p>Alsasink receives the buffer with samples. Since it received the first\nbuffer of samples, it completes the state change to the PAUSED state. At\nthis point the pipeline is prerolled and all elements have samples.\nAlsasink is now also capable of providing a clock to the pipeline.</p>\n<p>Since alsasink is now in the PAUSED state it blocks while receiving the\nfirst buffer. This effectively blocks both mp3dec and filesrc in their\n<code>gst_pad_push()</code>.</p>\n<p>Since all elements now return SUCCESS from the\n<code>gst_element_get_state()</code> function, the pipeline can be put in the\nPLAYING state.</p>\n<p>Before going to PLAYING, the pipeline select a clock and samples the\ncurrent time of the clock. This is the <code>base_time</code>. It then distributes\nthis time to all elements. Elements can then synchronize against the\nclock using the buffer <code>running_time</code>\n<code>base_time</code> (See also <a href=\"synchronisation.html\">synchronisation</a>).</p>\n<p>The following chain of state changes then takes place:</p>\n<ul>\n<li>\n<p>alsasink to PLAYING:  the samples are played to the audio device</p>\n</li>\n<li>\n<p>mp3dec to PLAYING:    nothing happens</p>\n</li>\n<li>\n<p>filesrc to PLAYING:   nothing happens</p>\n</li>\n</ul>\n<h2 id=\"pipeline-status\">Pipeline status</h2>\n<p>The pipeline informs the application of any special events that occur in\nthe pipeline with the bus. The bus is an object that the pipeline\nprovides and that can be retrieved with <code>gst_pipeline_get_bus()</code>.</p>\n<p>The bus can be polled or added to the glib mainloop.</p>\n<p>The bus is distributed to all elements added to the pipeline. The\nelements use the bus to post messages on. Various message types exist\nsuch as ERRORS, WARNINGS, EOS, <code>STATE_CHANGED</code>, etc..</p>\n<p>The pipeline handles EOS messages received from elements in a special\nway. It will only forward the message to the application when all sink\nelements have posted an EOS message.</p>\n<p>Other methods for obtaining the pipeline status include the Query\nfunctionality that can be performed with <code>gst_element_query()</code> on the\npipeline. This type of query is useful for obtaining information about\nthe current position and total time of the pipeline. It can also be used\nto query for the supported seeking formats and ranges.</p>\n<h2 id=\"pipeline-eos\">Pipeline EOS</h2>\n<p>When the source filter encounters the end of the stream, it sends an EOS\nevent to the peer element. This event will then travel downstream to all\nof the connected elements to inform them of the EOS. The element is not\nsupposed to accept any more data after receiving an EOS event on a\nsinkpad.</p>\n<p>The element providing the streaming thread stops sending data after\nsending the EOS event.</p>\n<p>The EOS event will eventually arrive in the sink element. The sink will\nthen post an EOS message on the bus to inform the pipeline that a\nparticular stream has finished. When all sinks have reported EOS, the\npipeline forwards the EOS message to the application. The EOS message is\nonly forwarded to the application in the PLAYING state.</p>\n<p>When in EOS, the pipeline remains in the PLAYING state, it is the\napplications responsability to PAUSE or READY the pipeline. The\napplication can also issue a seek, for example.</p>\n<h2 id=\"pipeline-ready\">Pipeline READY</h2>\n<p>When a running pipeline is set from the PLAYING to READY state, the\nfollowing actions occur in the pipeline:</p>\n<ul>\n<li>alsasink to PAUSED:  alsasink blocks and completes the state change on the\nnext sample. If the element was EOS, it does not wait for a sample to complete\nthe state change.</li>\n<li>mp3dec to PAUSED:    nothing</li>\n<li>filesrc to PAUSED:   nothing</li>\n</ul>\n<p>Going to the intermediate PAUSED state will block all elements in the\n<code>_push()</code> functions. This happens because the sink element blocks on the\nfirst buffer it receives.</p>\n<p>Some elements might be performing blocking operations in the PLAYING\nstate that must be unblocked when they go into the PAUSED state. This\nmakes sure that the state change happens very fast.</p>\n<p>In the next PAUSED to READY state change the pipeline has to shut down\nand all streaming threads must stop sending data. This happens in the\nfollowing sequence:</p>\n<ul>\n<li>alsasink to READY:   alsasink unblocks from the <code>_chain()</code> function and returns\na FLUSHING return value to the peer element. The sinkpad is deactivated and\nbecomes unusable for sending more data.</li>\n<li>mp3dec to READY:     the pads are deactivated and the state change completes\nwhen mp3dec leaves its <code>_chain()</code> function.</li>\n<li>filesrc to READY:    the pads are deactivated and the thread is paused.</li>\n</ul>\n<p>The upstream elements finish their <code>_chain()</code> function because the\ndownstream element returned an error code (FLUSHING) from the <code>_push()</code>\nfunctions. These error codes are eventually returned to the element that\nstarted the streaming thread (filesrc), which pauses the thread and\ncompletes the state change.</p>\n<p>This sequence of events ensure that all elements are unblocked and all\nstreaming threads stopped.</p>\n<h2 id=\"pipeline-seeking\">Pipeline seeking</h2>\n<p>Seeking in the pipeline requires a very specific order of operations to\nmake sure that the elements remain synchronized and that the seek is\nperformed with a minimal amount of latency.</p>\n<p>An application issues a seek event on the pipeline using\n<code>gst_element_send_event()</code> on the pipeline element. The event can be a\nseek event in any of the formats supported by the elements.</p>\n<p>The pipeline first pauses the pipeline to speed up the seek operations.</p>\n<p>The pipeline then issues the seek event to all sink elements. The sink\nthen forwards the seek event upstream until some element can perform the\nseek operation, which is typically the source or demuxer element. All\nintermediate elements can transform the requested seek offset to another\nformat, this way a decoder element can transform a seek to a frame\nnumber to a timestamp, for example.</p>\n<p>When the seek event reaches an element that will perform the seek\noperation, that element performs the following steps.</p>\n<ol>\n<li>send a <code>FLUSH_START</code> event to all downstream and upstream peer elements.</li>\n<li>make sure the streaming thread is not running. The streaming thread will\nalways stop because of step 1).</li>\n<li>perform the seek operation</li>\n<li>send a FLUSH done event to all downstream and upstream peer elements.</li>\n<li>send SEGMENT event to inform all elements of the new position and to complete\nthe seek.</li>\n</ol>\n<p>In step 1) all downstream elements have to return from any blocking\noperations and have to refuse any further buffers or events different\nfrom a FLUSH done.</p>\n<p>The first step ensures that the streaming thread eventually unblocks and\nthat step 2) can be performed. At this point, dataflow is completely\nstopped in the pipeline.</p>\n<p>In step 3) the element performs the seek to the requested position.</p>\n<p>In step 4) all peer elements are allowed to accept data again and\nstreaming can continue from the new position. A FLUSH done event is sent\nto all the peer elements so that they accept new data again and restart\ntheir streaming threads.</p>\n<p>Step 5) informs all elements of the new position in the stream. After\nthat the event function returns back to the application. and the\nstreaming threads start to produce new data.</p>\n<p>Since the pipeline is still PAUSED, this will preroll the next media\nsample in the sinks. The application can wait for this preroll to\ncomplete by performing a <code>_get_state()</code> on the pipeline.</p>\n<p>The last step in the seek operation is then to adjust the stream\n<code>running_time</code> of the pipeline to 0 and to set the pipeline back to\nPLAYING.</p>\n<p>The sequence of events in our mp3 playback example.</p>\n<pre><code>                                   | a) seek on pipeline\n                                   | b) PAUSE pipeline\n+----------------------------------V--------+\n| pipeline                         | c) seek on sink\n| +---------+   +----------+   +---V------+ |\n| | filesrc |   | mp3dec   |   | alsasink | |\n| |        src-sink       src-sink        | |\n| +---------+   +----------+   +----|-----+ |\n+-----------------------------------|-------+\n           &lt;------------------------+\n                 d) seek travels upstream\n\n    --------------------------&gt; 1) FLUSH event\n    | 2) stop streaming\n    | 3) perform seek\n    --------------------------&gt; 4) FLUSH done event\n    --------------------------&gt; 5) SEGMENT event\n\n    | e) update running_time to 0\n    | f) PLAY pipeline\n</code></pre>\n\n</div>\n\n\n\t"});