fragment_downloaded_cb({"url": "design/subtitle-overlays.html#page-description", "fragment": "<div id=\"page-description\" data-hotdoc-source=\"subtitle-overlays.md\">\n        <h1 id=\"subtitle-overlays-hardwareaccelerated-decoding-and-playbin\">Subtitle overlays, hardware-accelerated decoding and playbin</h1>\n<p>This document describes some of the considerations and requirements that\nled to the current <code>GstVideoOverlayCompositionMeta</code> API which allows\nattaching of subtitle bitmaps or logos to video buffers.</p>\n<h2 id=\"background\">Background</h2>\n<p>Subtitles can be muxed in containers or come from an external source.</p>\n<p>Subtitles come in many shapes and colours. Usually they are either\ntext-based (incl. 'pango markup'), or bitmap-based (e.g. DVD subtitles\nand the most common form of DVB subs). Bitmap based subtitles are\nusually compressed in some way, like some form of run-length encoding.</p>\n<p>Subtitles are currently decoded and rendered in subtitle-format-specific\noverlay elements. These elements have two sink pads (one for raw video\nand one for the subtitle format in question) and one raw video source\npad.</p>\n<p>They will take care of synchronising the two input streams, and of\ndecoding and rendering the subtitles on top of the raw video stream.</p>\n<p>Digression: one could theoretically have dedicated decoder/render\nelements that output an AYUV or ARGB image, and then let a videomixer\nelement do the actual overlaying, but this is not very efficient,\nbecause it requires us to allocate and blend whole pictures (1920x1080\nAYUV = 8MB, 1280x720 AYUV = 3.6MB, 720x576 AYUV = 1.6MB) even if the\noverlay region is only a small rectangle at the bottom. This wastes\nmemory and CPU. We could do something better by introducing a new format\nthat only encodes the region(s) of interest, but we don't have such a\nformat yet, and are not necessarily keen to rewrite this part of the\nlogic in playbin at this point - and we can't change existing elements'\nbehaviour, so would need to introduce new elements for this.</p>\n<p>Playbin supports outputting compressed formats, i.e. it does not force\ndecoding to a raw format, but is happy to output to a non-raw format as\nlong as the sink supports that as well.</p>\n<p>In case of certain hardware-accelerated decoding APIs, we will make use\nof that functionality. However, the decoder will not output a raw video\nformat then, but some kind of hardware/API-specific format (in the caps)\nand the buffers will reference hardware/API-specific objects that the\nhardware/API-specific sink will know how to handle.</p>\n<h2 id=\"the-problem\">The Problem</h2>\n<p>In the case of such hardware-accelerated decoding, the decoder will not\noutput raw pixels that can easily be manipulated. Instead, it will\noutput hardware/API-specific objects that can later be used to render a\nframe using the same API.</p>\n<p>Even if we could transform such a buffer into raw pixels, we most likely\nwould want to avoid that, in order to avoid the need to map the data\nback into system memory (and then later back to the GPU). It's much\nbetter to upload the much smaller encoded data to the GPU/DSP and then\nleave it there until rendered.</p>\n<p>Before <code>GstVideoOverlayComposition</code> playbin only supported subtitles on\ntop of raw decoded video. It would try to find a suitable overlay element\nfrom the plugin registry based on the input subtitle caps and the rank.\n(It is assumed that we will be able to convert any raw video format into\nany format required by the overlay using a converter such as videoconvert.)</p>\n<p>It would not render subtitles if the video sent to the sink is not raw\nYUV or RGB or if conversions had been disabled by setting the\nnative-video flag on playbin.</p>\n<p>Subtitle rendering is considered an important feature. Enabling\nhardware-accelerated decoding by default should not lead to a major\nfeature regression in this area.</p>\n<p>This means that we need to support subtitle rendering on top of non-raw\nvideo.</p>\n<h2 id=\"possible-solutions\">Possible Solutions</h2>\n<p>The goal is to keep knowledge of the subtitle format within the\nformat-specific GStreamer plugins, and knowledge of any specific video\nacceleration API to the GStreamer plugins implementing that API. We do\nnot want to make the pango/dvbsuboverlay/dvdspu/kate plugins link to\nlibva/libvdpau/etc. and we do not want to make the vaapi/vdpau plugins\nlink to all of libpango/libkate/libass etc.</p>\n<p>Multiple possible solutions come to mind:</p>\n<ol>\n<li>\n<p>backend-specific overlay elements</p>\n<p>e.g. vaapitextoverlay, vdpautextoverlay, vaapidvdspu, vdpaudvdspu,\nvaapidvbsuboverlay, vdpaudvbsuboverlay, etc.</p>\n<p>This assumes the overlay can be done directly on the\nbackend-specific object passed around.</p>\n<p>The main drawback with this solution is that it leads to a lot of\ncode duplication and may also lead to uncertainty about distributing\ncertain duplicated pieces of code. The code duplication is pretty\nmuch unavoidable, since making textoverlay, dvbsuboverlay, dvdspu,\nkate, assrender, etc. available in form of base classes to derive\nfrom is not really an option. Similarly, one would not really want\nthe vaapi/vdpau plugin to depend on a bunch of other libraries such\nas libpango, libkate, libtiger, libass, etc.</p>\n<p>One could add some new kind of overlay plugin feature though in\ncombination with a generic base class of some sort, but in order to\naccommodate all the different cases and formats one would end up\nwith quite convoluted/tricky API.</p>\n<p>(Of course there could also be a <code>GstFancyVideoBuffer</code> that provides\nan abstraction for such video accelerated objects and that could\nprovide an API to add overlays to it in a generic way, but in the\nend this is just a less generic variant of (c), and it is not clear\nthat there are real benefits to a specialised solution vs. a more\ngeneric one).</p>\n</li>\n<li>\n<p>convert backend-specific object to raw pixels and then overlay</p>\n<p>Even where possible technically, this is most likely very\ninefficient.</p>\n</li>\n<li>\n<p>attach the overlay data to the backend-specific video frame buffers\nin a generic way and do the actual overlaying/blitting later in\nbackend-specific code such as the video sink (or an accelerated\nencoder/transcoder)</p>\n<p>In this case, the actual overlay rendering (i.e. the actual text\nrendering or decoding DVD/DVB data into pixels) is done in the\nsubtitle-format-specific GStreamer plugin. All knowledge about the\nsubtitle format is contained in the overlay plugin then, and all\nknowledge about the video backend in the video backend specific\nplugin.</p>\n<p>The main question then is how to get the overlay pixels (and we will\nonly deal with pixels here) from the overlay element to the video\nsink.</p>\n<p>This could be done in multiple ways: One could send custom events\ndownstream with the overlay data, or one could attach the overlay\ndata directly to the video buffers in some way.</p>\n<p>Sending inline events has the advantage that is is fairly\ntransparent to any elements between the overlay element and the\nvideo sink: if an effects plugin creates a new video buffer for the\noutput, nothing special needs to be done to maintain the subtitle\noverlay information, since the overlay data is not attached to the\nbuffer. However, it slightly complicates things at the sink, since\nit would also need to look for the new event in question instead of\njust processing everything in its buffer render function.</p>\n<p>If one attaches the overlay data to the buffer directly, any element\nbetween overlay and video sink that creates a new video buffer would\nneed to be aware of the overlay data attached to it and copy it over\nto the newly-created buffer.</p>\n<p>One would have to do implement a special kind of new query (e.g.\nFEATURE query) that is not passed on automatically by\n<code>gst_pad_query_default()</code> in order to make sure that all elements\ndownstream will handle the attached overlay data. (This is only a\nproblem if we want to also attach overlay data to raw video pixel\nbuffers; for new non-raw types we can just make it mandatory and\nassume support and be done with it; for existing non-raw types\nnothing changes anyway if subtitles don't work) (we need to maintain\nbackwards compatibility for existing raw video pipelines like e.g.:\n<code>..decoder ! suboverlay ! encoder..</code>)</p>\n<p>Even though slightly more work, attaching the overlay information to\nbuffers seems more intuitive than sending it interleaved as events.\nAnd buffers stored or passed around (e.g. via the \"last-buffer\"\nproperty in the sink when doing screenshots via playbin) always\ncontain all the information needed.</p>\n</li>\n<li>\n<p>create a video/x-raw-*-delta format and use a backend-specific\nvideomixer</p>\n<p>This possibility was hinted at already in the digression in section</p>\n<ol>\n<li>It would satisfy the goal of keeping subtitle format knowledge in\nthe subtitle plugins and video backend knowledge in the video\nbackend plugin. It would also add a concept that might be generally\nuseful (think ximagesrc capture with xdamage). However, it would\nrequire adding foorender variants of all the existing overlay\nelements, and changing playbin to that new design, which is somewhat\nintrusive. And given the general nature of such a new format/API, we\nwould need to take a lot of care to be able to accommodate all\npossible use cases when designing the API, which makes it\nconsiderably more ambitious. Lastly, we would need to write\nvideomixer variants for the various accelerated video backends as\nwell.</li>\n</ol>\n</li>\n</ol>\n<p>Overall (c) appears to be the most promising solution. It is the least\nintrusive and should be fairly straight-forward to implement with\nreasonable effort, requiring only small changes to existing elements and\nrequiring no new elements.</p>\n<p>Doing the final overlaying in the sink as opposed to a videomixer or\noverlay in the middle of the pipeline has other advantages:</p>\n<ul>\n<li>\n<p>if video frames need to be dropped, e.g. for QoS reasons, we could\nalso skip the actual subtitle overlaying and possibly the\ndecoding/rendering as well, if the implementation and API allows for\nthat to be delayed.</p>\n</li>\n<li>\n<p>the sink often knows the actual size of the window/surface/screen\nthe output video is rendered to. This <em>may</em> make it possible to\nrender the overlay image in a higher resolution than the input\nvideo, solving a long standing issue with pixelated subtitles on top\nof low-resolution videos that are then scaled up in the sink. This\nwould require for the rendering to be delayed of course instead of\njust attaching an AYUV/ARGB/RGBA blog of pixels to the video buffer\nin the overlay, but that could all be supported.</p>\n</li>\n<li>\n<p>if the video backend / sink has support for high-quality text\nrendering (clutter?) we could just pass the text or pango markup to\nthe sink and let it do the rest (this is unlikely to be supported in\nthe general case - text and glyph rendering is hard; also, we don't\nreally want to make up our own text markup system, and pango markup\nis probably too limited for complex karaoke stuff).</p>\n</li>\n</ul>\n<h2 id=\"api-needed\">API needed</h2>\n<ol>\n<li>\n<p>Representation of subtitle overlays to be rendered</p>\n<p>We need to pass the overlay pixels from the overlay element to the\nsink somehow. Whatever the exact mechanism, let's assume we pass a\nrefcounted <code>GstVideoOverlayComposition</code> struct or object.</p>\n<p>A composition is made up of one or more overlays/rectangles.</p>\n<p>In the simplest case an overlay rectangle is just a blob of\nRGBA/ABGR [FIXME?] or AYUV pixels with positioning info and other\nmetadata, and there is only one rectangle to render.</p>\n<p>We're keeping the naming generic (\"OverlayFoo\" rather than\n\"SubtitleFoo\") here, since this might also be handy for other use\ncases such as e.g. logo overlays or so. It is not designed for\nfull-fledged video stream mixing\nthough.</p>\n</li>\n</ol>\n<pre><code>        // Note: don't mind the exact implementation details, they'll be hidden\n        \n        // FIXME: might be confusing in 0.11 though since GstXOverlay was\n        //        renamed to GstVideoOverlay in 0.11, but not much we can do,\n        //        maybe we can rename GstVideoOverlay to something better\n        \n        struct GstVideoOverlayComposition\n        {\n            guint                          num_rectangles;\n            GstVideoOverlayRectangle    ** rectangles;\n        \n            /* lowest rectangle sequence number still used by the upstream\n             * overlay element. This way a renderer maintaining some kind of\n             * rectangles &lt;-&gt; surface cache can know when to free cached\n             * surfaces/rectangles. */\n            guint                          min_seq_num_used;\n        \n            /* sequence number for the composition (same series as rectangles) */\n            guint                          seq_num;\n        }\n        \n        struct GstVideoOverlayRectangle\n        {\n            /* Position on video frame and dimension of output rectangle in\n             * output frame terms (already adjusted for the PAR of the output\n             * frame). x/y can be negative (overlay will be clipped then) */\n            gint  x, y;\n            guint render_width, render_height;\n        \n            /* Dimensions of overlay pixels */\n            guint width, height, stride;\n        \n            /* This is the PAR of the overlay pixels */\n            guint par_n, par_d;\n        \n            /* Format of pixels, GST_VIDEO_FORMAT_ARGB on big-endian systems,\n             * and BGRA on little-endian systems (i.e. pixels are treated as\n             * 32-bit values and alpha is always in the most-significant byte,\n             * and blue is in the least-significant byte).\n             *\n             * FIXME: does anyone actually use AYUV in practice? (we do\n             * in our utility function to blend on top of raw video)\n             * What about AYUV and endianness? Do we always have [A][Y][U][V]\n             * in memory? */\n            /* FIXME: maybe use our own enum? */\n            GstVideoFormat format;\n        \n            /* Refcounted blob of memory, no caps or timestamps */\n            GstBuffer *pixels;\n        \n            // FIXME: how to express source like text or pango markup?\n            //        (just add source type enum + source buffer with data)\n            //\n            // FOR 0.10: always send pixel blobs, but attach source data in\n            // addition (reason: if downstream changes, we can't renegotiate\n            // that properly, if we just do a query of supported formats from\n            // the start). Sink will just ignore pixels and use pango markup\n            // from source data if it supports that.\n            //\n            // FOR 0.11: overlay should query formats (pango markup, pixels)\n            // supported by downstream and then only send that. We can\n            // renegotiate via the reconfigure event.\n            //\n        \n            /* sequence number: useful for backends/renderers/sinks that want\n             * to maintain a cache of rectangles &lt;-&gt; surfaces. The value of\n             * the min_seq_num_used in the composition tells the renderer which\n             * rectangles have expired. */\n            guint      seq_num;\n        \n            /* FIXME: we also need a (private) way to cache converted/scaled\n             * pixel blobs */\n        }\n    \n    (a1) Overlay consumer\n        API:\n    \n        How would this work in a video sink that supports scaling of textures:\n        \n        gst_foo_sink_render () {\n          /* assume only one for now */\n          if video_buffer has composition:\n            composition = video_buffer.get_composition()\n        \n            for each rectangle in composition:\n              if rectangle.source_data_type == PANGO_MARKUP\n                actor = text_from_pango_markup (rectangle.get_source_data())\n              else\n                pixels = rectangle.get_pixels_unscaled (FORMAT_RGBA, ...)\n                actor = texture_from_rgba (pixels, ...)\n        \n              .. position + scale on top of video surface ...\n        }\n    \n    (a2) Overlay producer\n        API:\n    \n        e.g. logo or subpicture overlay: got pixels, stuff into rectangle:\n        \n         if (logoverlay-&gt;cached_composition == NULL) {\n           comp = composition_new ();\n        \n           rect = rectangle_new (format, pixels_buf,\n                                 width, height, stride, par_n, par_d,\n                                 x, y, render_width, render_height);\n        \n           /* composition adds its own ref for the rectangle */\n           composition_add_rectangle (comp, rect);\n           rectangle_unref (rect);\n        \n           /* buffer adds its own ref for the composition */\n           video_buffer_attach_composition (comp);\n        \n           /* we take ownership of the composition and save it for later */\n           logoverlay-&gt;cached_composition = comp;\n         } else {\n           video_buffer_attach_composition (logoverlay-&gt;cached_composition);\n         }\n</code></pre>\n<pre><code>FIXME: also add some API to modify render position/dimensions of a\nrectangle (probably requires creation of new rectangle, unless we\nhandle writability like with other mini objects).\n</code></pre>\n<ol start=\"2\">\n<li>\n<p>Fallback overlay rendering/blitting on top of raw video</p>\n<p>Eventually we want to use this overlay mechanism not only for\nhardware-accelerated video, but also for plain old raw video, either\nat the sink or in the overlay element directly.</p>\n<p>Apart from the advantages listed earlier in section 3, this allows\nus to consolidate a lot of overlaying/blitting code that is\ncurrently repeated in every single overlay element in one location.\nThis makes it considerably easier to support a whole range of raw\nvideo formats out of the box, add SIMD-optimised rendering using\nORC, or handle corner cases correctly.</p>\n<p>(Note: side-effect of overlaying raw video at the video sink is that\nif e.g. a screnshotter gets the last buffer via the last-buffer\nproperty of basesink, it would get an image without the subtitles on\ntop. This could probably be fixed by re-implementing the property in\n<code>GstVideoSink</code> though. Playbin2 could handle this internally as well).</p>\n</li>\n</ol>\n<pre><code>        void\n        gst_video_overlay_composition_blend (GstVideoOverlayComposition * comp\n                                             GstBuffer                  * video_buf)\n        {\n          guint n;\n        \n          g_return_if_fail (gst_buffer_is_writable (video_buf));\n          g_return_if_fail (GST_BUFFER_CAPS (video_buf) != NULL);\n        \n          ... parse video_buffer caps into BlendVideoFormatInfo ...\n        \n          for each rectangle in the composition: {\n        \n                 if (gst_video_format_is_yuv (video_buf_format)) {\n                   overlay_format = FORMAT_AYUV;\n                 } else if (gst_video_format_is_rgb (video_buf_format)) {\n                   overlay_format = FORMAT_ARGB;\n                 } else {\n                   /* FIXME: grayscale? */\n                   return;\n                 }\n        \n                 /* this will scale and convert AYUV&lt;-&gt;ARGB if needed */\n                 pixels = rectangle_get_pixels_scaled (rectangle, overlay_format);\n        \n                 ... clip output rectangle ...\n        \n                 __do_blend (video_buf_format, video_buf-&gt;data,\n                             overlay_format, pixels-&gt;data,\n                             x, y, width, height, stride);\n        \n                 gst_buffer_unref (pixels);\n          }\n        }\n</code></pre>\n<ol start=\"3\">\n<li>\n<p>Flatten all rectangles in a composition</p>\n<p>We cannot assume that the video backend API can handle any number of\nrectangle overlays, it's possible that it only supports one single\noverlay, in which case we need to squash all rectangles into one.</p>\n<p>However, we'll just declare this a corner case for now, and\nimplement it only if someone actually needs it. It's easy to add\nlater API-wise. Might be a bit tricky if we have rectangles with\ndifferent PARs/formats (e.g. subs and a logo), though we could\nprobably always just use the code from (b) with a fully transparent\nvideo buffer to create a flattened overlay buffer.</p>\n</li>\n<li>\n<p>query support for the new video composition mechanism</p>\n<p>This is handled via <code>GstMeta</code> and an ALLOCATION query - we can simply\nquery whether downstream supports the <code>GstVideoOverlayComposition</code> meta.</p>\n<p>There appears to be no issue with downstream possibly not being\nlinked yet at the time when an overlay would want to do such a\nquery, but we would just have to default to something and update\nourselves later on a reconfigure event then.</p>\n</li>\n</ol>\n<p>Other considerations:</p>\n<ul>\n<li>\n<p>renderers (overlays or sinks) may be able to handle only ARGB or\nonly AYUV (for most graphics/hw-API it's likely ARGB of some sort,\nwhile our blending utility functions will likely want the same\ncolour space as the underlying raw video format, which is usually\nYUV of some sort). We need to convert where required, and should\ncache the conversion.</p>\n</li>\n<li>\n<p>renderers may or may not be able to scale the overlay. We need to do\nthe scaling internally if not (simple case: just horizontal scaling\nto adjust for PAR differences; complex case: both horizontal and\nvertical scaling, e.g. if subs come from a different source than the\nvideo or the video has been rescaled or cropped between overlay\nelement and sink).</p>\n</li>\n<li>\n<p>renderers may be able to generate (possibly scaled) pixels on demand\nfrom the original data (e.g. a string or RLE-encoded data). We will\nignore this for now, since this functionality can still be added\nlater via API additions. The most interesting case would be to pass\na pango markup string, since e.g. clutter can handle that natively.</p>\n</li>\n<li>\n<p>renderers may be able to write data directly on top of the video\npixels (instead of creating an intermediary buffer with the overlay\nwhich is then blended on top of the actual video frame), e.g.\ndvdspu, dvbsuboverlay</p>\n</li>\n</ul>\n<p>However, in the interest of simplicity, we should probably ignore the\nfact that some elements can blend their overlays directly on top of the\nvideo (decoding/uncompressing them on the fly), even more so as it's not\nobvious that it's actually faster to decode the same overlay 70-90 times\n(say) (ie. ca. 3 seconds of video frames) and then blend it 70-90 times\ninstead of decoding it once into a temporary buffer and then blending it\ndirectly from there, possibly SIMD-accelerated. Also, this is only\nrelevant if the video is raw video and not some hardware-acceleration\nbackend object.</p>\n<p>And ultimately it is the overlay element that decides whether to do the\noverlay right there and then or have the sink do it (if supported). It\ncould decide to keep doing the overlay itself for raw video and only use\nour new API for non-raw video.</p>\n<ul>\n<li>\n<p>renderers may want to make sure they only upload the overlay pixels\nonce per rectangle if that rectangle recurs in subsequent frames (as\npart of the same composition or a different composition), as is\nlikely. This caching of e.g. surfaces needs to be done renderer-side\nand can be accomplished based on the sequence numbers. The\ncomposition contains the lowest sequence number still in use\nupstream (an overlay element may want to cache created\ncompositions+rectangles as well after all to re-use them for\nmultiple frames), based on that the renderer can expire cached\nobjects. The caching needs to be done renderer-side because\nattaching renderer-specific objects to the rectangles won't work\nwell given the refcounted nature of rectangles and compositions,\nmaking it unpredictable when a rectangle or composition will be\nfreed or from which thread context it will be freed. The\nrenderer-specific objects are likely bound to other types of\nrenderer-specific contexts, and need to be managed in connection\nwith those.</p>\n</li>\n<li>\n<p>composition/rectangles should internally provide a certain degree of\nthread-safety. Multiple elements (sinks, overlay element) might\naccess or use the same objects from multiple threads at the same\ntime, and it is expected that elements will keep a ref to\ncompositions and rectangles they push downstream for a while, e.g.\nuntil the current subtitle composition expires.</p>\n</li>\n</ul>\n<h2 id=\"future-considerations\">Future considerations</h2>\n<ul>\n<li>alternatives: there may be multiple versions/variants of the same\nsubtitle stream. On DVDs, there may be a 4:3 version and a 16:9\nversion of the same subtitles. We could attach both variants and let\nthe renderer pick the best one for the situation (currently we just\nuse the 16:9 version). With totem, it's ultimately totem that adds\nthe 'black bars' at the top/bottom, so totem also knows if it's got\na 4:3 display and can/wants to fit 4:3 subs (which may render on top\nof the bars) or not, for example.</li>\n</ul>\n<h2 id=\"misc-fixmes\">Misc. FIXMEs</h2>\n<p>TEST: should these look (roughly) alike (note text distortion) - needs\nfixing in textoverlay</p>\n<pre><code>gst-launch-1.0 \\\n   videotestsrc ! video/x-raw,width=640,height=480,pixel-aspect-ratio=1/1 \\\n     ! textoverlay text=Hello font-desc=72 ! xvimagesink \\\n   videotestsrc ! video/x-raw,width=320,height=480,pixel-aspect-ratio=2/1 \\\n     ! textoverlay text=Hello font-desc=72 ! xvimagesink \\\n   videotestsrc ! video/x-raw,width=640,height=240,pixel-aspect-ratio=1/2 \\\n     ! textoverlay text=Hello font-desc=72 ! xvimagesink\n</code></pre>\n\n        \n\n    </div>\n\n\n        "});