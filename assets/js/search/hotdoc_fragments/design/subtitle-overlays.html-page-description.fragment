fragment_downloaded_cb({"url": "design/subtitle-overlays.html#page-description", "fragment": "This document describes some of the considerations and requirements that led to the current GstVideoOverlayCompositionMeta API which allows attaching of subtitle bitmaps or logos to video buffers. \nSubtitles can be muxed in containers or come from an external source. \nSubtitles come in many shapes and colours. Usually they are either text based incl. pango markup or bitmap based e.g. DVD subtitles and the most common form of DVB subs Bitmap based subtitles are usually compressed in some way like some form of run length encoding. \nSubtitles are currently decoded and rendered in subtitle format specific overlay elements. These elements have two sink pads one for raw video and one for the subtitle format in question and one raw video source pad. \nThey will take care of synchronising the two input streams and of decoding and rendering the subtitles on top of the raw video stream. \nDigression one could theoretically have dedicated decoder render elements that output an AYUV or ARGB image and then let a videomixer element do the actual overlaying but this is not very efficient because it requires us to allocate and blend whole pictures x1080 AYUV MB x720 AYUV MB x576 AYUV MB even if the overlay region is only a small rectangle at the bottom. This wastes memory and CPU. We could do something better by introducing a new format that only encodes the region s of interest but we don t have such a format yet and are not necessarily keen to rewrite this part of the logic in playbin at this point and we can t change existing elements behaviour so would need to introduce new elements for this. \nPlaybin supports outputting compressed formats i.e. it does not force decoding to a raw format but is happy to output to a non raw format as long as the sink supports that as well. \nIn case of certain hardware accelerated decoding APIs we will make use of that functionality. However the decoder will not output a raw video format then but some kind of hardware API specific format in the caps and the buffers will reference hardware API specific objects that the hardware API specific sink will know how to handle. \nIn the case of such hardware accelerated decoding the decoder will not output raw pixels that can easily be manipulated. Instead it will output hardware API specific objects that can later be used to render a frame using the same API. \nEven if we could transform such a buffer into raw pixels we most likely would want to avoid that in order to avoid the need to map the data back into system memory and then later back to the GPU It s much better to upload the much smaller encoded data to the GPU DSP and then leave it there until rendered. \nBefore GstVideoOverlayComposition playbin only supported subtitles on top of raw decoded video. It would try to find a suitable overlay element from the plugin registry based on the input subtitle caps and the rank. It is assumed that we will be able to convert any raw video format into any format required by the overlay using a converter such as videoconvert. \nIt would not render subtitles if the video sent to the sink is not raw YUV or RGB or if conversions had been disabled by setting the native video flag on playbin. \nSubtitle rendering is considered an important feature. Enabling hardware accelerated decoding by default should not lead to a major feature regression in this area. \nThis means that we need to support subtitle rendering on top of non raw video. \nThe goal is to keep knowledge of the subtitle format within the format specific GStreamer plugins and knowledge of any specific video acceleration API to the GStreamer plugins implementing that API. We do not want to make the pango dvbsuboverlay dvdspu kate plugins link to libva libvdpau etc. and we do not want to make the vaapi vdpau plugins link to all of libpango libkate libass etc. \nMultiple possible solutions come to mind \nbackend specific overlay elements \ne.g. vaapitextoverlay vdpautextoverlay vaapidvdspu vdpaudvdspu vaapidvbsuboverlay vdpaudvbsuboverlay etc. \nThis assumes the overlay can be done directly on the backend specific object passed around. \nThe main drawback with this solution is that it leads to a lot of code duplication and may also lead to uncertainty about distributing certain duplicated pieces of code. The code duplication is pretty much unavoidable since making textoverlay dvbsuboverlay dvdspu kate assrender etc. available in form of base classes to derive from is not really an option. Similarly one would not really want the vaapi vdpau plugin to depend on a bunch of other libraries such as libpango libkate libtiger libass etc. \nOne could add some new kind of overlay plugin feature though in combination with a generic base class of some sort but in order to accommodate all the different cases and formats one would end up with quite convoluted tricky API. \nOf course there could also be a GstFancyVideoBuffer that provides an abstraction for such video accelerated objects and that could provide an API to add overlays to it in a generic way but in the end this is just a less generic variant of c and it is not clear that there are real benefits to a specialised solution vs. a more generic one \nconvert backend specific object to raw pixels and then overlay \nEven where possible technically this is most likely very inefficient. \nattach the overlay data to the backend specific video frame buffers in a generic way and do the actual overlaying blitting later in backend specific code such as the video sink or an accelerated encoder transcoder \nIn this case the actual overlay rendering i.e. the actual text rendering or decoding DVD DVB data into pixels is done in the subtitle format specific GStreamer plugin. All knowledge about the subtitle format is contained in the overlay plugin then and all knowledge about the video backend in the video backend specific plugin. \nThe main question then is how to get the overlay pixels and we will only deal with pixels here from the overlay element to the video sink. \nThis could be done in multiple ways One could send custom events downstream with the overlay data or one could attach the overlay data directly to the video buffers in some way. \nSending inline events has the advantage that is is fairly transparent to any elements between the overlay element and the video sink if an effects plugin creates a new video buffer for the output nothing special needs to be done to maintain the subtitle overlay information since the overlay data is not attached to the buffer. However it slightly complicates things at the sink since it would also need to look for the new event in question instead of just processing everything in its buffer render function. \nIf one attaches the overlay data to the buffer directly any element between overlay and video sink that creates a new video buffer would need to be aware of the overlay data attached to it and copy it over to the newly created buffer. \nOne would have to do implement a special kind of new query e.g. FEATURE query that is not passed on automatically by gst_pad_query_default in order to make sure that all elements downstream will handle the attached overlay data. This is only a problem if we want to also attach overlay data to raw video pixel buffers for new non raw types we can just make it mandatory and assume support and be done with it for existing non raw types nothing changes anyway if subtitles don t work we need to maintain backwards compatibility for existing raw video pipelines like e.g. decoder suboverlay encoder.. \nEven though slightly more work attaching the overlay information to buffers seems more intuitive than sending it interleaved as events. And buffers stored or passed around e.g. via the last buffer property in the sink when doing screenshots via playbin always contain all the information needed. \ncreate a video x raw delta format and use a backend specific videomixer \nThis possibility was hinted at already in the digression in section \nOverall c appears to be the most promising solution. It is the least intrusive and should be fairly straight forward to implement with reasonable effort requiring only small changes to existing elements and requiring no new elements. \nDoing the final overlaying in the sink as opposed to a videomixer or overlay in the middle of the pipeline has other advantages \nif video frames need to be dropped e.g. for QoS reasons we could also skip the actual subtitle overlaying and possibly the decoding rendering as well if the implementation and API allows for that to be delayed. \nthe sink often knows the actual size of the window surface screen the output video is rendered to. This may make it possible to render the overlay image in a higher resolution than the input video solving a long standing issue with pixelated subtitles on top of low resolution videos that are then scaled up in the sink. This would require for the rendering to be delayed of course instead of just attaching an AYUV ARGB RGBA blog of pixels to the video buffer in the overlay but that could all be supported. \nif the video backend sink has support for high quality text rendering clutter we could just pass the text or pango markup to the sink and let it do the rest this is unlikely to be supported in the general case text and glyph rendering is hard also we don t really want to make up our own text markup system and pango markup is probably too limited for complex karaoke stuff \nRepresentation of subtitle overlays to be rendered \nWe need to pass the overlay pixels from the overlay element to the sink somehow. Whatever the exact mechanism let s assume we pass a refcounted GstVideoOverlayComposition struct or object. \nA composition is made up of one or more overlays rectangles. \nIn the simplest case an overlay rectangle is just a blob of RGBA ABGR FIXME or AYUV pixels with positioning info and other metadata and there is only one rectangle to render. \nWe re keeping the naming generic OverlayFoo rather than SubtitleFoo here since this might also be handy for other use cases such as e.g. logo overlays or so. It is not designed for full fledged video stream mixing though. \nFallback overlay rendering blitting on top of raw video \nEventually we want to use this overlay mechanism not only for hardware accelerated video but also for plain old raw video either at the sink or in the overlay element directly. \nApart from the advantages listed earlier in section this allows us to consolidate a lot of overlaying blitting code that is currently repeated in every single overlay element in one location. This makes it considerably easier to support a whole range of raw video formats out of the box add SIMD optimised rendering using ORC or handle corner cases correctly. \nNote side effect of overlaying raw video at the video sink is that if e.g. a screnshotter gets the last buffer via the last buffer property of basesink it would get an image without the subtitles on top. This could probably be fixed by re implementing the property in GstVideoSink though. Playbin2 could handle this internally as well \nFlatten all rectangles in a composition \nWe cannot assume that the video backend API can handle any number of rectangle overlays it s possible that it only supports one single overlay in which case we need to squash all rectangles into one. \nHowever we ll just declare this a corner case for now and implement it only if someone actually needs it. It s easy to add later API wise. Might be a bit tricky if we have rectangles with different PARs formats e.g. subs and a logo though we could probably always just use the code from b with a fully transparent video buffer to create a flattened overlay buffer. \nquery support for the new video composition mechanism \nThis is handled via GstMeta and an ALLOCATION query we can simply query whether downstream supports the GstVideoOverlayComposition meta. \nThere appears to be no issue with downstream possibly not being linked yet at the time when an overlay would want to do such a query but we would just have to default to something and update ourselves later on a reconfigure event then. \nOther considerations \nrenderers overlays or sinks may be able to handle only ARGB or only AYUV for most graphics hw API it s likely ARGB of some sort while our blending utility functions will likely want the same colour space as the underlying raw video format which is usually YUV of some sort We need to convert where required and should cache the conversion. \nrenderers may or may not be able to scale the overlay. We need to do the scaling internally if not simple case just horizontal scaling to adjust for PAR differences complex case both horizontal and vertical scaling e.g. if subs come from a different source than the video or the video has been rescaled or cropped between overlay element and sink \nrenderers may be able to generate possibly scaled pixels on demand from the original data e.g. a string or RLE encoded data We will ignore this for now since this functionality can still be added later via API additions. The most interesting case would be to pass a pango markup string since e.g. clutter can handle that natively. \nrenderers may be able to write data directly on top of the video pixels instead of creating an intermediary buffer with the overlay which is then blended on top of the actual video frame e.g. dvdspu dvbsuboverlay \nHowever in the interest of simplicity we should probably ignore the fact that some elements can blend their overlays directly on top of the video decoding uncompressing them on the fly even more so as it s not obvious that it s actually faster to decode the same overlay times say ie. ca. seconds of video frames and then blend it times instead of decoding it once into a temporary buffer and then blending it directly from there possibly SIMD accelerated. Also this is only relevant if the video is raw video and not some hardware acceleration backend object. \nAnd ultimately it is the overlay element that decides whether to do the overlay right there and then or have the sink do it if supported It could decide to keep doing the overlay itself for raw video and only use our new API for non raw video. \nrenderers may want to make sure they only upload the overlay pixels once per rectangle if that rectangle recurs in subsequent frames as part of the same composition or a different composition as is likely. This caching of e.g. surfaces needs to be done renderer side and can be accomplished based on the sequence numbers. The composition contains the lowest sequence number still in use upstream an overlay element may want to cache created compositions rectangles as well after all to re use them for multiple frames based on that the renderer can expire cached objects. The caching needs to be done renderer side because attaching renderer specific objects to the rectangles won t work well given the refcounted nature of rectangles and compositions making it unpredictable when a rectangle or composition will be freed or from which thread context it will be freed. The renderer specific objects are likely bound to other types of renderer specific contexts and need to be managed in connection with those. \ncomposition rectangles should internally provide a certain degree of thread safety. Multiple elements sinks overlay element might access or use the same objects from multiple threads at the same time and it is expected that elements will keep a ref to compositions and rectangles they push downstream for a while e.g. until the current subtitle composition expires. \nTEST should these look roughly alike note text distortion needs fixing in textoverlay \n"});