fragment_downloaded_cb({"url": "design/subtitle-overlays.html#background", "fragment": "Background \nSubtitles can be muxed in containers or come from an external source. \nSubtitles come in many shapes and colours. Usually they are either text based incl. pango markup or bitmap based e.g. DVD subtitles and the most common form of DVB subs Bitmap based subtitles are usually compressed in some way like some form of run length encoding. \nSubtitles are currently decoded and rendered in subtitle format specific overlay elements. These elements have two sink pads one for raw video and one for the subtitle format in question and one raw video source pad. \nThey will take care of synchronising the two input streams and of decoding and rendering the subtitles on top of the raw video stream. \nDigression one could theoretically have dedicated decoder render elements that output an AYUV or ARGB image and then let a videomixer element do the actual overlaying but this is not very efficient because it requires us to allocate and blend whole pictures x1080 AYUV MB x720 AYUV MB x576 AYUV MB even if the overlay region is only a small rectangle at the bottom. This wastes memory and CPU. We could do something better by introducing a new format that only encodes the region s of interest but we don t have such a format yet and are not necessarily keen to rewrite this part of the logic in playbin at this point and we can t change existing elements behaviour so would need to introduce new elements for this. \nPlaybin supports outputting compressed formats i.e. it does not force decoding to a raw format but is happy to output to a non raw format as long as the sink supports that as well. \nIn case of certain hardware accelerated decoding APIs we will make use of that functionality. However the decoder will not output a raw video format then but some kind of hardware API specific format in the caps and the buffers will reference hardware API specific objects that the hardware API specific sink will know how to handle. \n"});