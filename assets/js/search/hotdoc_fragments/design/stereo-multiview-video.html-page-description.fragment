fragment_downloaded_cb({"url": "design/stereo-multiview-video.html#page-description", "fragment": "<div id=\"page-description\" data-hotdoc-source=\"stereo-multiview-video.md\">\n<h1 id=\"stereoscopic-multiview-video-handling\">Stereoscopic &amp; Multiview Video Handling</h1>\n<p>There are two cases to handle:</p>\n<ul>\n<li>\n<p>Encoded video output from a demuxer to parser / decoder or from encoders\ninto a muxer.</p>\n</li>\n<li>\n<p>Raw video buffers</p>\n</li>\n</ul>\n<p>The design below is somewhat based on the proposals from\n<a href=\"https://bugzilla.gnome.org/show_bug.cgi?id=611157\">bug 611157</a></p>\n<p>Multiview is used as a generic term to refer to handling both\nstereo content (left and right eye only) as well as extensions for videos\ncontaining multiple independent viewpoints.</p>\n<h2 id=\"encoded-signalling\">Encoded Signalling</h2>\n<p>This is regarding the signalling in caps and buffers from demuxers to\nparsers (sometimes) or out from encoders.</p>\n<p>For backward compatibility with existing codecs many transports of\nstereoscopic 3D content use normal 2D video with 2 views packed spatially\nin some way, and put extra new descriptions in the container/mux.</p>\n<p>Info in the demuxer seems to apply to stereo encodings only. For all\nMVC methods I know, the multiview encoding is in the video bitstream itself\nand therefore already available to decoders. Only stereo systems have been retro-fitted\ninto the demuxer.</p>\n<p>Also, sometimes extension descriptions are in the codec (e.g. H.264 SEI FPA packets)\nand it would be useful to be able to put the info onto caps and buffers from the\nparser without decoding.</p>\n<p>To handle both cases, we need to be able to output the required details on\nencoded video for decoders to apply onto the raw video buffers they decode.</p>\n<p><em>If there ever is a need to transport multiview info for encoded data the\nsame system below for raw video or some variation should work</em></p>\n<h3 id=\"encoded-video-properties-that-need-to-be-encoded-into-caps\">Encoded Video: Properties that need to be encoded into caps</h3>\n<ol>\n<li>\n<p>multiview-mode (called \"Channel Layout\" in bug 611157)</p>\n<ul>\n<li>Whether a stream is mono, for a single eye, stereo, mixed-mono-stereo\n(switches between mono and stereo - mp4 can do this)</li>\n<li>Uses a buffer flag to mark individual buffers as mono or \"not mono\"\n(single|stereo|multiview) for mixed scenarios. The alternative (not\nproposed) is for the demuxer to switch caps for each mono to not-mono\nchange, and not used a 'mixed' caps variant at all.</li>\n<li><em>single</em> refers to a stream of buffers that only contain 1 view.\nIt is different from mono in that the stream is a marked left or right\neye stream for later combining in a mixer or when displaying.</li>\n<li><em>multiple</em> marks a stream with multiple independent views encoded.\nIt is included in this list for completeness. As noted above, there's\ncurrently no scenario that requires marking encoded buffers as MVC.</li>\n</ul>\n</li>\n<li>\n<p>Frame-packing arrangements / view sequence orderings</p>\n<ul>\n<li>Possible frame packings: side-by-side, side-by-side-quincunx,\ncolumn-interleaved, row-interleaved, top-bottom, checker-board</li>\n<li>bug 611157 - sreerenj added side-by-side-full and top-bottom-full but\nI think that's covered by suitably adjusting pixel-aspect-ratio. If\nnot, they can be added later.</li>\n<li><em>top-bottom</em>, <em>side-by-side</em>, <em>column-interleaved</em>, <em>row-interleaved</em> are as the names suggest.</li>\n<li><em>checker-board</em>, samples are left/right pixels in a chess grid +-+-+-/-+-+-+</li>\n<li><em>side-by-side-quincunx</em>. Side By Side packing, but quincunx sampling -\n1 pixel offset of each eye needs to be accounted when upscaling or displaying</li>\n<li>there may be other packings (future expansion)</li>\n<li>Possible view sequence orderings: frame-by-frame, frame-primary-secondary-tracks, sequential-row-interleaved</li>\n<li><em>frame-by-frame</em>, each buffer is left, then right view etc</li>\n<li><em>frame-primary-secondary-tracks</em> - the file has 2 video tracks (primary and secondary), one is left eye, one is right.\nDemuxer info indicates which one is which.\nHandling this means marking each stream as all-left and all-right views, decoding separately, and combining automatically (inserting a mixer/combiner in playbin)\n-&gt; <em>Leave this for future expansion</em></li>\n<li><em>sequential-row-interleaved</em> Mentioned by sreerenj in bug patches, I can't find a mention of such a thing. Maybe it's in MPEG-2\n-&gt; <em>Leave this for future expansion / deletion</em></li>\n</ul>\n</li>\n<li>\n<p>view encoding order</p>\n<ul>\n<li>Describes how to decide which piece of each frame corresponds to left or right eye</li>\n<li>Possible orderings left, right, left-then-right, right-then-left</li>\n</ul>\n<ul>\n<li>Need to figure out how we find the correct frame in the demuxer to start decoding when seeking in frame-sequential streams</li>\n<li>Need a buffer flag for marking the first buffer of a group.</li>\n</ul>\n</li>\n<li>\n<p>\"Frame layout flags\"</p>\n<ul>\n<li>flags for view specific interpretation</li>\n<li>horizontal-flip-left, horizontal-flip-right, vertical-flip-left, vertical-flip-right\nIndicates that one or more views has been encoded in a flipped orientation, usually due to camera with mirror or displays with mirrors.</li>\n<li>This should be an actual flags field. Registered GLib flags types aren't generally well supported in our caps - the type might not be loaded/registered yet when parsing a caps string, so they can't be used in caps templates in the registry.</li>\n<li>It might be better just to use a hex value / integer</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"buffer-representation-for-raw-video\">Buffer representation for raw video</h2>\n<ul>\n<li>Transported as normal video buffers with extra metadata</li>\n<li>The caps define the overall buffer width/height, with helper functions to\nextract the individual views for packed formats</li>\n<li>pixel-aspect-ratio adjusted if needed to double the overall width/height</li>\n<li>video sinks that don't know about multiview extensions yet will show the\npacked view as-is. For frame-sequence outputs, things might look weird, but\njust adding multiview-mode to the sink caps can disallow those transports.</li>\n<li><em>row-interleaved</em> packing is actually just side-by-side memory layout with\nhalf frame width, twice the height, so can be handled by adjusting the\noverall caps and strides</li>\n<li>Other exotic layouts need new pixel formats defined (checker-board,\ncolumn-interleaved, side-by-side-quincunx)</li>\n<li><em>Frame-by-frame</em> - one view per buffer, but with alternating metas marking\nwhich buffer is which left/right/other view and using a new buffer flag as\ndescribed above to mark the start of a group of corresponding frames.</li>\n<li>New video caps addition as for encoded buffers</li>\n</ul>\n<h3 id=\"proposed-caps-fields\">Proposed Caps fields</h3>\n<p>Combining the requirements above and collapsing the combinations into mnemonics:</p>\n<ul>\n<li>\n<p>multiview-mode =\nmono | left | right | sbs | sbs-quin | col | row | topbot | checkers |\nframe-by-frame | mixed-sbs | mixed-sbs-quin | mixed-col | mixed-row |\nmixed-topbot | mixed-checkers | mixed-frame-by-frame | multiview-frames mixed-multiview-frames</p>\n</li>\n<li>\n<p>multiview-flags =</p>\n<ul>\n<li>0x0000 none</li>\n<li>0x0001 right-view-first</li>\n<li>0x0002 left-h-flipped</li>\n<li>0x0004 left-v-flipped</li>\n<li>0x0008 right-h-flipped</li>\n<li>0x0010 right-v-flipped</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"proposed-new-buffer-flags\">Proposed new buffer flags</h3>\n<p>Add two new <code>GST_VIDEO_BUFFER_*</code> flags in video-frame.h and make it clear that\nthose flags can apply to encoded video buffers too. wtay says that's currently\nthe case anyway, but the documentation should say it.</p>\n<ul>\n<li>\n<p><strong><code>GST_VIDEO_BUFFER_FLAG_MULTIPLE_VIEW</code></strong> - Marks a buffer as representing\nnon-mono content, although it may be a single (left or right) eye view.</p>\n</li>\n<li>\n<p><strong><code>GST_VIDEO_BUFFER_FLAG_FIRST_IN_BUNDLE</code></strong> - for frame-sequential methods of\ntransport, mark the \"first\" of a left/right/other group of frames</p>\n</li>\n</ul>\n<h3 id=\"a-new-gstmultiviewmeta\">A new GstMultiviewMeta</h3>\n<p>This provides a place to describe all provided views in a buffer / stream,\nand through Meta negotiation to inform decoders about which views to decode if\nnot all are wanted.</p>\n<ul>\n<li>Logical labels/names and mapping to <code>GstVideoMeta</code> numbers</li>\n<li>Standard view labels LEFT/RIGHT, and non-standard ones (strings)</li>\n</ul>\n<pre><code class=\"language-c\">        GST_VIDEO_MULTIVIEW_VIEW_LEFT = 1\n        GST_VIDEO_MULTIVIEW_VIEW_RIGHT = 2\n\n        struct GstVideoMultiviewViewInfo {\n            guint view_label;\n            guint meta_id; // id of the GstVideoMeta for this view\n\n            padding;\n        }\n\n        struct GstVideoMultiviewMeta {\n            guint n_views;\n            GstVideoMultiviewViewInfo *view_info;\n        }\n</code></pre>\n<p>The meta is optional, and probably only useful later for MVC</p>\n<h2 id=\"outputting-stereo-content\">Outputting stereo content</h2>\n<p>The initial implementation for output will be stereo content in glimagesink</p>\n<h3 id=\"output-considerations-with-opengl\">Output Considerations with OpenGL</h3>\n<ul>\n<li>\n<p>If we have support for stereo GL buffer formats, we can output separate\nleft/right eye images and let the hardware take care of display.</p>\n</li>\n<li>\n<p>Otherwise, glimagesink needs to render one window with left/right in a\nsuitable frame packing and that will only show correctly in fullscreen on a\ndevice set for the right 3D packing -&gt; requires app intervention to set the\nvideo mode.</p>\n</li>\n<li>\n<p>Which could be done manually on the TV, or with HDMI 1.4 by setting the\nright video mode for the screen to inform the TV or third option, we support\nrendering to two separate overlay areas on the screen - one for left eye,\none for right which can be supported using the 'splitter' element and two\noutput sinks or, better, add a 2nd window overlay for split stereo output</p>\n</li>\n<li>\n<p>Intel hardware doesn't do stereo GL buffers - only nvidia and AMD, so\ninitial implementation won't include that</p>\n</li>\n</ul>\n<h2 id=\"other-elements-for-handling-multiview-content\">Other elements for handling multiview content</h2>\n<ul>\n<li>\n<p>videooverlay interface extensions</p>\n<ul>\n<li><strong>Q</strong>: Should this be a new interface?</li>\n<li>Element message to communicate the presence of stereoscopic information to the app</li>\n<li>App needs to be able to override the input interpretation - ie, set multiview-mode and multiview-flags\n<ul>\n<li>Most videos I've seen are side-by-side or top-bottom with no frame-packing metadata</li>\n</ul>\n</li>\n<li>New API for the app to set rendering options for stereo/multiview content</li>\n<li>This might be best implemented as a <strong>multiview GstContext</strong>, so that\nthe pipeline can share app preferences for content interpretation and downmixing\nto mono for output, or in the sink and have those down as far upstream/downstream as possible.</li>\n</ul>\n</li>\n<li>\n<p>Converter element</p>\n<ul>\n<li>convert different view layouts</li>\n<li>Render to anaglyphs of different types (magenta/green, red/blue, etc) and output as mono</li>\n</ul>\n</li>\n<li>\n<p>Mixer element</p>\n<ul>\n<li>take 2 video streams and output as stereo</li>\n<li>later take n video streams</li>\n<li>share code with the converter, it just takes input from n pads instead of one.</li>\n</ul>\n</li>\n<li>\n<p>Splitter element</p>\n</li>\n<li>\n<p>Output one pad per view</p>\n</li>\n</ul>\n<h3 id=\"implementing-mvc-handling-in-decoders-parsers-and-encoders\">Implementing MVC handling in decoders / parsers (and encoders)</h3>\n<p>Things to do to implement MVC handling</p>\n<ol>\n<li>Parsing SEI in h264parse and setting caps (patches available in\nbugzilla for parsing, see below)</li>\n<li>Integrate gstreamer-vaapi MVC support with this proposal</li>\n<li>Help with <a href=\"https://wiki.libav.org/Blueprint/MVC\">libav MVC implementation</a></li>\n<li>generating SEI in H.264 encoder</li>\n<li>Support for MPEG2 MVC extensions</li>\n</ol>\n<h2 id=\"relevant-bugs\">Relevant bugs</h2>\n<ul>\n<li><a href=\"https://bugzilla.gnome.org/show_bug.cgi?id=685215\">bug 685215</a> - codecparser h264: Add initial MVC parser</li>\n<li><a href=\"https://bugzilla.gnome.org/show_bug.cgi?id=696135\">bug 696135</a> - h264parse: Add mvc stream parsing support</li>\n<li><a href=\"https://bugzilla.gnome.org/show_bug.cgi?id=732267\">bug 732267</a> - h264parse: extract base stream from MVC or SVC encoded streams</li>\n</ul>\n<h2 id=\"other-information\">Other Information</h2>\n<p><a href=\"http://www.matroska.org/technical/specs/notes.html#3D\">Matroska 3D support notes</a></p>\n<h2 id=\"open-questions\">Open Questions</h2>\n<h3 id=\"background\">Background</h3>\n<h3 id=\"representation-for-gstgl\">Representation for GstGL</h3>\n<p>When uploading raw video frames to GL textures, the goal is to implement:</p>\n<p>Split packed frames into separate GL textures when uploading, and\nattach multiple <code>GstGLMemory</code> to the <code>GstBuffer</code>. The multiview-mode and\nmultiview-flags fields in the caps should change to reflect the conversion\nfrom one incoming <code>GstMemory</code> to multiple <code>GstGLMemory</code>, and change the\nwidth/height in the output info as needed.</p>\n<p>This is (currently) targetted as 2 render passes - upload as normal\nto a single stereo-packed RGBA texture, and then unpack into 2\nsmaller textures, output with <code>GST_VIDEO_MULTIVIEW_MODE_SEPARATED</code>, as\n2 <code>GstGLMemory</code> attached to one buffer. We can optimise the upload later\nto go directly to 2 textures for common input formats.</p>\n<p>Separat output textures have a few advantages:</p>\n<ul>\n<li>\n<p>Filter elements can more easily apply filters in several passes to each\ntexture without fundamental changes to our filters to avoid mixing pixels\nfrom separate views.</p>\n</li>\n<li>\n<p>Centralises the sampling of input video frame packings in the upload code,\nwhich makes adding new packings in the future easier.</p>\n</li>\n<li>\n<p>Sampling multiple textures to generate various output frame-packings\nfor display is conceptually simpler than converting from any input packing\nto any output packing.</p>\n</li>\n<li>\n<p>In implementations that support quad buffers, having separate textures\nmakes it trivial to do <code>GL_LEFT</code>/<code>GL_RIGHT</code> output</p>\n</li>\n</ul>\n<p>For either option, we'll need new glsink output API to pass more\ninformation to applications about multiple views for the draw signal/callback.</p>\n<p>I don't know if it's desirable to support <em>both</em> methods of representing\nviews. If so, that should be signalled in the caps too. That could be a\nnew multiview-mode for passing views in separate <code>GstMemory</code> objects\nattached to a <code>GstBuffer</code>, which would not be GL specific.</p>\n<h3 id=\"overriding-frame-packing-interpretation\">Overriding frame packing interpretation</h3>\n<p>Most sample videos available are frame packed, with no metadata\nto say so. How should we override that interpretation?</p>\n<ul>\n<li>Simple answer: Use capssetter + new properties on playbin to\noverride the multiview fields. <em>Basically implemented in playbin, using</em>\n<em>a pad probe. Needs more work for completeness</em></li>\n</ul>\n<h3 id=\"adding-extra-gstvideometa-to-buffers\">Adding extra GstVideoMeta to buffers</h3>\n<p>There should be one <code>GstVideoMeta</code> for the entire video frame in packed\nlayouts, and one <code>GstVideoMeta</code> per <code>GstGLMemory</code> when views are attached\nto a <code>GstBuffer</code> separately. This should be done by the buffer pool,\nwhich knows from the caps.</p>\n<h3 id=\"videooverlay-interface-extensions\">videooverlay interface extensions</h3>\n<p>GstVideoOverlay needs:</p>\n<ul>\n<li>A way to announce the presence of multiview content when it is\ndetected/signalled in a stream.</li>\n<li>A way to tell applications which output methods are supported/available</li>\n<li>A way to tell the sink which output method it should use</li>\n<li>Possibly a way to tell the sink to override the input frame\ninterpretation / caps - depends on the answer to the question\nabove about how to model overriding input interpretation.</li>\n</ul>\n<h3 id=\"whats-implemented\">What's implemented</h3>\n<ul>\n<li>Caps handling</li>\n<li>gst-plugins-base libsgstvideo pieces</li>\n<li>playbin caps overriding</li>\n<li>conversion elements - glstereomix, gl3dconvert (needs a rename),\nglstereosplit.</li>\n</ul>\n<h3 id=\"possible-future-enhancements\">Possible future enhancements</h3>\n<ul>\n<li>Make GLupload split to separate textures at upload time?\n<ul>\n<li>Needs new API to extract multiple textures from the upload. Currently only outputs 1 result RGBA texture.</li>\n</ul>\n</li>\n<li>Make GLdownload able to take 2 input textures, pack them and colorconvert / download as needed.\n<ul>\n<li>current done by packing then downloading which isn't OK overhead for RGBA download</li>\n</ul>\n</li>\n<li>Think about how we integrate GLstereo - do we need to do anything special,\nor can the app just render to stereo/quad buffers if they're available?</li>\n</ul>\n\n</div>\n\n\n        "});