fragment_downloaded_cb({"url": "design/stereo-multiview-video.html#page-description", "fragment": "There are two cases to handle \nEncoded video output from a demuxer to parser decoder or from encoders into a muxer. \nRaw video buffers \nThe design below is somewhat based on the proposals from bug \nMultiview is used as a generic term to refer to handling both stereo content left and right eye only as well as extensions for videos containing multiple independent viewpoints. \nThis is regarding the signalling in caps and buffers from demuxers to parsers sometimes or out from encoders. \nFor backward compatibility with existing codecs many transports of stereoscopic D content use normal D video with views packed spatially in some way and put extra new descriptions in the container mux. \nInfo in the demuxer seems to apply to stereo encodings only. For all MVC methods I know the multiview encoding is in the video bitstream itself and therefore already available to decoders. Only stereo systems have been retro fitted into the demuxer. \nAlso sometimes extension descriptions are in the codec e.g. H.264 SEI FPA packets and it would be useful to be able to put the info onto caps and buffers from the parser without decoding. \nTo handle both cases we need to be able to output the required details on encoded video for decoders to apply onto the raw video buffers they decode. \nIf there ever is a need to transport multiview info for encoded data the same system below for raw video or some variation should work \nmultiview mode called Channel Layout in bug \nFrame packing arrangements view sequence orderings \nview encoding order \nFrame layout flags \nCombining the requirements above and collapsing the combinations into mnemonics \nmultiview mode mono left right sbs sbs quin col row topbot checkers frame by frame mixed sbs mixed sbs quin mixed col mixed row mixed topbot mixed checkers mixed frame by frame multiview frames mixed multiview frames \nmultiview flags \nAdd two new GST_VIDEO_BUFFER_ flags in video frame.h and make it clear that those flags can apply to encoded video buffers too. wtay says that s currently the case anyway but the documentation should say it. \nGST_VIDEO_BUFFER_FLAG_MULTIPLE_VIEW Marks a buffer as representing non mono content although it may be a single left or right eye view. \nGST_VIDEO_BUFFER_FLAG_FIRST_IN_BUNDLE for frame sequential methods of transport mark the first of a left right other group of frames \nThis provides a place to describe all provided views in a buffer stream and through Meta negotiation to inform decoders about which views to decode if not all are wanted. \nThe meta is optional and probably only useful later for MVC \nThe initial implementation for output will be stereo content in glimagesink \nIf we have support for stereo GL buffer formats we can output separate left right eye images and let the hardware take care of display. \nOtherwise glimagesink needs to render one window with left right in a suitable frame packing and that will only show correctly in fullscreen on a device set for the right D packing requires app intervention to set the video mode. \nWhich could be done manually on the TV or with HDMI by setting the right video mode for the screen to inform the TV or third option we support rendering to two separate overlay areas on the screen one for left eye one for right which can be supported using the splitter element and two output sinks or better add a nd window overlay for split stereo output \nIntel hardware doesn t do stereo GL buffers only nvidia and AMD so initial implementation won t include that \nvideooverlay interface extensions \nConverter element \nMixer element \nSplitter element \nOutput one pad per view \nThings to do to implement MVC handling \nMatroska D support notes \nWhen uploading raw video frames to GL textures the goal is to implement \nSplit packed frames into separate GL textures when uploading and attach multiple GstGLMemory to the GstBuffer. The multiview mode and multiview flags fields in the caps should change to reflect the conversion from one incoming GstMemory to multiple GstGLMemory and change the width height in the output info as needed. \nThis is currently targetted as render passes upload as normal to a single stereo packed RGBA texture and then unpack into smaller textures output with GST_VIDEO_MULTIVIEW_MODE_SEPARATED as GstGLMemory attached to one buffer. We can optimise the upload later to go directly to textures for common input formats. \nSeparat output textures have a few advantages \nFilter elements can more easily apply filters in several passes to each texture without fundamental changes to our filters to avoid mixing pixels from separate views. \nCentralises the sampling of input video frame packings in the upload code which makes adding new packings in the future easier. \nSampling multiple textures to generate various output frame packings for display is conceptually simpler than converting from any input packing to any output packing. \nIn implementations that support quad buffers having separate textures makes it trivial to do GL_LEFT GL_RIGHT output \nFor either option we ll need new glsink output API to pass more information to applications about multiple views for the draw signal callback. \nI don t know if it s desirable to support both methods of representing views. If so that should be signalled in the caps too. That could be a new multiview mode for passing views in separate GstMemory objects attached to a GstBuffer which would not be GL specific. \nMost sample videos available are frame packed with no metadata to say so. How should we override that interpretation \nThere should be one GstVideoMeta for the entire video frame in packed layouts and one GstVideoMeta per GstGLMemory when views are attached to a GstBuffer separately. This should be done by the buffer pool which knows from the caps. \nGstVideoOverlay needs \n"});