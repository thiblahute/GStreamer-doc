fragment_downloaded_cb({"url": "application-development/advanced/clocks.html#page-description", "fragment": "Clocks and synchronization in GStreamer \nWhen playing complex media each sound and video sample must be played in a specific order at a specific time. For this purpose GStreamer provides a synchronization mechanism. \nGStreamer provides support for the following use cases \nNon live sources with access faster than playback rate. This is the case where one is reading media from a file and playing it back in a synchronized fashion. In this case multiple streams need to be synchronized like audio video and subtitles. \nCapture and synchronized muxing mixing of media from multiple live sources. This is a typical use case where you record audio and video from a microphone camera and mux it into a file for storage. \nStreaming from slow network streams with buffering. This is the typical web streaming case where you access content from a streaming server with http. \nCapture from live source and and playback to live source with configurable latency. This is used when for example capture from a camera apply an effect and display the result. It is also used when streaming low latency content over a network with UDP. \nSimultaneous live capture and playback from prerecorded content. This is used in audio recording cases where you play a previously recorded audio and record new samples the purpose is to have the new audio perfectly in sync with the previously recorded data. \nGStreamer uses a GstClock object buffer timestamps and a SEGMENT event to synchronize streams in a pipeline as we will see in the next sections. \nYou can see how the running time of a buffer always increments monotonically along with the clock time. Buffers are played when their running time is equal to the clock time base time. The stream time represents the position in the stream and jumps backwards when repeating. \n"});