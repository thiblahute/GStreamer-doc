fragment_downloaded_cb({"url": "application-development/advanced/clocks.html#page-description", "fragment": "<div id=\"page-description\" data-hotdoc-source=\"clocks.md\">\n<h1 id=\"clocks-and-synchronization-in-gstreamer\">Clocks and synchronization in GStreamer</h1>\n<p>When playing complex media, each sound and video sample must be played\nin a specific order at a specific time. For this purpose, GStreamer\nprovides a synchronization mechanism.</p>\n<p>GStreamer provides support for the following use cases:</p>\n<ul>\n<li>\n<p>Non-live sources with access faster than playback rate. This is the\ncase where one is reading media from a file and playing it back in a\nsynchronized fashion. In this case, multiple streams need to be\nsynchronized, like audio, video and subtitles.</p>\n</li>\n<li>\n<p>Capture and synchronized muxing/mixing of media from multiple live\nsources. This is a typical use case where you record audio and video\nfrom a microphone/camera and mux it into a file for storage.</p>\n</li>\n<li>\n<p>Streaming from (slow) network streams with buffering. This is the\ntypical web streaming case where you access content from a streaming\nserver with http.</p>\n</li>\n<li>\n<p>Capture from live source and and playback to live source with\nconfigurable latency. This is used when, for example, capture from a\ncamera, apply an effect and display the result. It is also used when\nstreaming low latency content over a network with UDP.</p>\n</li>\n<li>\n<p>Simultaneous live capture and playback from prerecorded content.\nThis is used in audio recording cases where you play a previously\nrecorded audio and record new samples, the purpose is to have the\nnew audio perfectly in sync with the previously recorded data.</p>\n</li>\n</ul>\n<p>GStreamer uses a <code>GstClock</code> object, buffer timestamps and a SEGMENT\nevent to synchronize streams in a pipeline as we will see in the next\nsections.</p>\n<h2 id=\"clock-runningtime\">Clock running-time</h2>\n<p>In a typical computer, there are many sources that can be used as a time\nsource, e.g., the system time, soundcards, CPU performance counters, ...\nFor this reason, there are many <code>GstClock</code> implementations available in\nGStreamer. The clock time doesn't always start from 0 or from some known\nvalue. Some clocks start counting from some known start date, other\nclocks start counting since last reboot, etc...</p>\n<p>A <code>GstClock</code> returns the <strong>absolute-time</strong> according to that clock with\n<code>gst_clock_get_time ()</code>. The absolute-time (or clock time) of a clock is\nmonotonically increasing. From the absolute-time is a <strong>running-time</strong>\ncalculated, which is simply the difference between a previous snapshot\nof the absolute-time called the <strong>base-time</strong>. So:</p>\n<p>running-time = absolute-time - base-time</p>\n<p>A GStreamer <code>GstPipeline</code> object maintains a <code>GstClock</code> object and a\nbase-time when it goes to the PLAYING state. The pipeline gives a handle\nto the selected <code>GstClock</code> to each element in the pipeline along with\nselected base-time. The pipeline will select a base-time in such a way\nthat the running-time reflects the total time spent in the PLAYING\nstate. As a result, when the pipeline is PAUSED, the running-time stands\nstill.</p>\n<p>Because all objects in the pipeline have the same clock and base-time,\nthey can thus all calculate the running-time according to the pipeline\nclock.</p>\n<h2 id=\"buffer-runningtime\">Buffer running-time</h2>\n<p>To calculate a buffer running-time, we need a buffer timestamp and the\nSEGMENT event that preceeded the buffer. First we can convert the\nSEGMENT event into a <code>GstSegment</code> object and then we can use the\n<code>gst_segment_to_running_time ()</code> function to perform the calculation of\nthe buffer running-time.</p>\n<p>Synchronization is now a matter of making sure that a buffer with a\ncertain running-time is played when the clock reaches the same\nrunning-time. Usually this task is done by sink elements. Sink also have\nto take into account the latency configured in the pipeline and add this\nto the buffer running-time before synchronizing to the pipeline clock.</p>\n<p>Non-live sources timestamp buffers with a running-time starting from 0.\nAfter a flushing seek, they will produce buffers again from a\nrunning-time of 0.</p>\n<p>Live sources need to timestamp buffers with a running-time matching the\npipeline running-time when the first byte of the buffer was captured.</p>\n<h2 id=\"buffer-streamtime\">Buffer stream-time</h2>\n<p>The buffer stream-time, also known as the position in the stream, is\ncalculated from the buffer timestamps and the preceding SEGMENT event.\nIt represents the time inside the media as a value between 0 and the\ntotal duration of the media.</p>\n<p>The stream-time is used in:</p>\n<ul>\n<li>\n<p>Report the current position in the stream with the POSITION query.</p>\n</li>\n<li>\n<p>The position used in the seek events and queries.</p>\n</li>\n<li>\n<p>The position used to synchronize controlled values.</p>\n</li>\n</ul>\n<p>The stream-time is never used to synchronize streams, this is only done\nwith the running-time.</p>\n<h2 id=\"time-overview\">Time overview</h2>\n<p>Here is an overview of the various timelines used in GStreamer.</p>\n<p>The image below represents the different times in the pipeline when\nplaying a 100ms sample and repeating the part between 50ms and 100ms.</p>\n<p><img src=\"images/clocks.png\" alt=\"GStreamer clock and various times\" title=\"fig:\" id=\"gstreamer-clock-and-various-times\"></p>\n<p>You can see how the running-time of a buffer always increments\nmonotonically along with the clock-time. Buffers are played when their\nrunning-time is equal to the clock-time - base-time. The stream-time\nrepresents the position in the stream and jumps backwards when\nrepeating.</p>\n<h2 id=\"clock-providers\">Clock providers</h2>\n<p>A clock provider is an element in the pipeline that can provide a\n<code>GstClock</code> object. The clock object needs to report an absolute-time\nthat is monotonically increasing when the element is in the PLAYING\nstate. It is allowed to pause the clock while the element is PAUSED.</p>\n<p>Clock providers exist because they play back media at some rate, and\nthis rate is not necessarily the same as the system clock rate. For\nexample, a soundcard may playback at 44,1 kHz, but that doesn't mean\nthat after <em>exactly</em> 1 second <em>according to the system clock</em>, the\nsoundcard has played back 44.100 samples. This is only true by\napproximation. In fact, the audio device has an internal clock based on\nthe number of samples played that we can expose.</p>\n<p>If an element with an internal clock needs to synchronize, it needs to\nestimate when a time according to the pipeline clock will take place\naccording to the internal clock. To estimate this, it needs to slave its\nclock to the pipeline clock.</p>\n<p>If the pipeline clock is exactly the internal clock of an element, the\nelement can skip the slaving step and directly use the pipeline clock to\nschedule playback. This can be both faster and more accurate. Therefore,\ngenerally, elements with an internal clock like audio input or output\ndevices will be a clock provider for the pipeline.</p>\n<p>When the pipeline goes to the PLAYING state, it will go over all\nelements in the pipeline from sink to source and ask each element if\nthey can provide a clock. The last element that can provide a clock will\nbe used as the clock provider in the pipeline. This algorithm prefers a\nclock from an audio sink in a typical playback pipeline and a clock from\nsource elements in a typical capture pipeline.</p>\n<p>There exist some bus messages to let you know about the clock and clock\nproviders in the pipeline. You can see what clock is selected in the\npipeline by looking at the NEW_CLOCK message on the bus. When a clock\nprovider is removed from the pipeline, a CLOCK_LOST message is posted\nand the application should go to PAUSED and back to PLAYING to select a\nnew clock.</p>\n<h2 id=\"latency\">Latency</h2>\n<p>The latency is the time it takes for a sample captured at timestamp X to\nreach the sink. This time is measured against the clock in the pipeline.\nFor pipelines where the only elements that synchronize against the clock\nare the sinks, the latency is always 0 since no other element is\ndelaying the buffer.</p>\n<p>For pipelines with live sources, a latency is introduced, mostly because\nof the way a live source works. Consider an audio source, it will start\ncapturing the first sample at time 0. If the source pushes buffers with\n44100 samples at a time at 44100Hz it will have collected the buffer at\nsecond 1. Since the timestamp of the buffer is 0 and the time of the\nclock is now &gt;= 1 second, the sink will drop this buffer because it is\ntoo late. Without any latency compensation in the sink, all buffers will\nbe dropped.</p>\n<h3 id=\"latency-compensation\">Latency compensation</h3>\n<p>Before the pipeline goes to the PLAYING state, it will, in addition to\nselecting a clock and calculating a base-time, calculate the latency in\nthe pipeline. It does this by doing a LATENCY query on all the sinks in\nthe pipeline. The pipeline then selects the maximum latency in the\npipeline and configures this with a LATENCY event.</p>\n<p>All sink elements will delay playback by the value in the LATENCY event.\nSince all sinks delay with the same amount of time, they will be\nrelative in sync.</p>\n<h3 id=\"dynamic-latency\">Dynamic Latency</h3>\n<p>Adding/removing elements to/from a pipeline or changing element\nproperties can change the latency in a pipeline. An element can request\na latency change in the pipeline by posting a LATENCY message on the\nbus. The application can then decide to query and redistribute a new\nlatency or not. Changing the latency in a pipeline might cause visual or\naudible glitches and should therefore only be done by the application\nwhen it is allowed.</p>\n\n</div>\n\n\n        "});