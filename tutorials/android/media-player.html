<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Android tutorial 4: A basic media player</title>

<link rel="stylesheet" href="assets/css/custom_bootstrap.css" type="text/css">
<link rel="stylesheet" href="assets/css/frontend.css" type="text/css">
<link rel="stylesheet" href="assets/css/jquery.mCustomScrollbar.min.css">

<link rel="stylesheet" href="assets/css/prism.css" type="text/css">

<script src="assets/js/mustache.min.js"></script>
<script src="assets/js/jquery.js"></script>
<script src="assets/js/bootstrap.js"></script>
<script src="assets/js/typeahead.jquery.min.js"></script>
<script src="assets/js/search.js"></script>
<script src="assets/js/isotope.pkgd.min.js"></script>
<script src="assets/js/compare-versions.js"></script>
<script src="assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
<script src="assets/js/bootstrap-toc.min.js"></script>
<script src="assets/js/utils.js"></script>
<script src="assets/js/tag_filtering.js"></script>
<script src="assets/js/language_switching.js"></script>
<script src="assets/js/navigation.js"></script>


<script src="assets/js/lines_around_headings.js"></script>

<script src="assets/js/prism-core.js"></script>
<script src="assets/js/prism-autoloader.js"></script>
<script src="assets/js/prism_autoloader_path_override.js"></script>
<script src="assets/js/trie.js"></script>


<link rel="icon" type="image/png" href="assets/images/favicon.png">
<link rel="shortcut icon" href="assets/images/favicon.png">

</head>

<body data-spy="scroll" data-target="#toc" data-offset="70">


<nav class="navbar navbar-fixed-top navbar-default">
	<div class="container-fluid">
		<div class="navbar-header">
			<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-wrapper" aria-expanded="false">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			<a href="https://gstreamer.freedesktop.org/" class="hotdoc-navbar-brand">
				<img src="assets/images/gstreamer-logo.svg" alt="Home" id="home">
			</a>
		</div>
		<div class="navbar-collapse collapse" id="navbar-wrapper">
			<ul class="nav navbar-nav" id="menu">
							</ul>
			<form action="" class="navbar-form navbar-right">
				<div class="form-group has-feedback">
					<input type="text" class="form-control" name="search" id="sidenav-lookup-field" placeholder="search" disabled>
					<span class="glyphicon glyphicon-search form-control-feedback"></span>
				</div>
			</form>
		</div>
	</div>
</nav>

<main class="page-row page-row-expanded">
<div data-extension="core" data-hotdoc-in-toplevel="True" data-hotdoc-project="GStreamer-1.0" data-hotdoc-ref="tutorials/android/media-player.html" class="page_container" id="page-wrapper">
	<div class="row">
		<div class="hidden-xs hidden-sm col-md-3 col-lg-3 col-xl-2" id="sidenav-column">
	<div class="panel panel-collapse" id="sidenav" data-hotdoc-role="navigation">
		<div id="sitenav-wrapper" class="mCustomScrollbar" data-mcs-theme="minimal">
			<div class="sidenav-main-panel-body">
				<div id="site-navigation">
				</div>
			</div>
		</div>
	</div>
</div>

<div class="col-sm-12 col-md-9 col-lg-7 col-xl-8">
	<div id="main">
				    <div id="page-description" data-hotdoc-source="media-player.md">
        <h1 id="android-tutorial-4-a-basic-media-player">Android tutorial 4: A basic media player</h1>
<h2 id="goal">Goal</h2>
<p><img src="images/tutorials/android-media-player-screenshot.png" alt="screenshot" id="screenshot"></p>
<p>Enough testing with synthetic images and audio tones! This tutorial
finally plays actual media, streamed directly from the Internet, in your
Android device. It shows:</p>
<ul>
<li>How to keep the User Interface regularly updated with the current
playback position and duration</li>
<li>How to implement a <a href="http://developer.android.com/reference/android/widget/SeekBar.html">Seek
Bar</a></li>
<li>How to report the media size to adapt the display surface</li>
</ul>
<p>It also uses the knowledge gathered in the <a href="../basic/index.html">Basic tutorials</a> regarding:</p>
<ul>
<li>How to use <code>playbin</code> to play any kind of media</li>
<li>How to handle network resilience problems</li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>From the previous tutorials, we already have almost all necessary pieces
to build a media player. The most complex part is assembling a pipeline
which retrieves, decodes and displays the media, but we already know
that the <code>playbin</code> element can take care of all that for us. We only
need to replace the manual pipeline we used in
<a href="video.html">Android tutorial 3: Video</a> with a single-element
<code>playbin</code> pipeline and we are good to go!</p>
<p>However, we can do better than. We will add a <a href="http://developer.android.com/reference/android/widget/SeekBar.html">Seek
Bar</a>,
with a moving thumb that will advance as our current position in the
media advances. We will also allow the user to drag the thumb, to jump
(or <em>seek</em>) to a different position.</p>
<p>And finally, we will make the video surface adapt to the media size, so
the video sink is not forced to draw black borders around the clip.
This also allows the Android layout to adapt more nicely to the actual
media content. You can still force the video surface to have a specific
size if you really want to.</p>
<h2 id="a-basic-media-player-java-code">A basic media player [Java code]</h2>
<p><strong>src/com/gst_sdk_tutorials/tutorial_4/Tutorial4.java</strong></p>
<pre><code class="language-java">package com.gst_sdk_tutorials.tutorial_4;

import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.TimeZone;

import android.app.Activity;
import android.os.Bundle;
import android.util.Log;
import android.view.SurfaceHolder;
import android.view.SurfaceView;
import android.view.View;
import android.view.View.OnClickListener;
import android.widget.ImageButton;
import android.widget.SeekBar;
import android.widget.SeekBar.OnSeekBarChangeListener;
import android.widget.TextView;
import android.widget.Toast;

import org.freedesktop.gstreamer.GStreamer;

public class Tutorial4 extends Activity implements SurfaceHolder.Callback, OnSeekBarChangeListener {
    private native void nativeInit();     // Initialize native code, build pipeline, etc
    private native void nativeFinalize(); // Destroy pipeline and shutdown native code
    private native void nativeSetUri(String uri); // Set the URI of the media to play
    private native void nativePlay();     // Set pipeline to PLAYING
    private native void nativeSetPosition(int milliseconds); // Seek to the indicated position, in milliseconds
    private native void nativePause();    // Set pipeline to PAUSED
    private static native boolean nativeClassInit(); // Initialize native class: cache Method IDs for callbacks
    private native void nativeSurfaceInit(Object surface); // A new surface is available
    private native void nativeSurfaceFinalize(); // Surface about to be destroyed
    private long native_custom_data;      // Native code will use this to keep private data

    private boolean is_playing_desired;   // Whether the user asked to go to PLAYING
    private int position;                 // Current position, reported by native code
    private int duration;                 // Current clip duration, reported by native code
    private boolean is_local_media;       // Whether this clip is stored locally or is being streamed
    private int desired_position;         // Position where the users wants to seek to
    private String mediaUri;              // URI of the clip being played

    private final String defaultMediaUri = "https://www.freedesktop.org/software/gstreamer-sdk/data/media/sintel_trailer-368p.ogv";

    // Called when the activity is first created.
    @Override
    public void onCreate(Bundle savedInstanceState)
    {
        super.onCreate(savedInstanceState);

        // Initialize GStreamer and warn if it fails
        try {
            GStreamer.init(this);
        } catch (Exception e) {
            Toast.makeText(this, e.getMessage(), Toast.LENGTH_LONG).show();
            finish();
            return;
        }

        setContentView(R.layout.main);

        ImageButton play = (ImageButton) this.findViewById(R.id.button_play);
        play.setOnClickListener(new OnClickListener() {
            public void onClick(View v) {
                is_playing_desired = true;
                nativePlay();
            }
        });

        ImageButton pause = (ImageButton) this.findViewById(R.id.button_stop);
        pause.setOnClickListener(new OnClickListener() {
            public void onClick(View v) {
                is_playing_desired = false;
                nativePause();
            }
        });

        SurfaceView sv = (SurfaceView) this.findViewById(R.id.surface_video);
        SurfaceHolder sh = sv.getHolder();
        sh.addCallback(this);

        SeekBar sb = (SeekBar) this.findViewById(R.id.seek_bar);
        sb.setOnSeekBarChangeListener(this);

        // Retrieve our previous state, or initialize it to default values
        if (savedInstanceState != null) {
            is_playing_desired = savedInstanceState.getBoolean("playing");
            position = savedInstanceState.getInt("position");
            duration = savedInstanceState.getInt("duration");
            mediaUri = savedInstanceState.getString("mediaUri");
            Log.i ("GStreamer", "Activity created with saved state:");
        } else {
            is_playing_desired = false;
            position = duration = 0;
            mediaUri = defaultMediaUri;
            Log.i ("GStreamer", "Activity created with no saved state:");
        }
        is_local_media = false;
        Log.i ("GStreamer", "  playing:" + is_playing_desired + " position:" + position +
                " duration: " + duration + " uri: " + mediaUri);

        // Start with disabled buttons, until native code is initialized
        this.findViewById(R.id.button_play).setEnabled(false);
        this.findViewById(R.id.button_stop).setEnabled(false);

        nativeInit();
    }

    protected void onSaveInstanceState (Bundle outState) {
        Log.d ("GStreamer", "Saving state, playing:" + is_playing_desired + " position:" + position +
                " duration: " + duration + " uri: " + mediaUri);
        outState.putBoolean("playing", is_playing_desired);
        outState.putInt("position", position);
        outState.putInt("duration", duration);
        outState.putString("mediaUri", mediaUri);
    }

    protected void onDestroy() {
        nativeFinalize();
        super.onDestroy();
    }

    // Called from native code. This sets the content of the TextView from the UI thread.
    private void setMessage(final String message) {
        final TextView tv = (TextView) this.findViewById(R.id.textview_message);
        runOnUiThread (new Runnable() {
          public void run() {
            tv.setText(message);
          }
        });
    }

    // Set the URI to play, and record whether it is a local or remote file
    private void setMediaUri() {
        nativeSetUri (mediaUri);
        is_local_media = mediaUri.startsWith("file://");
    }

    // Called from native code. Native code calls this once it has created its pipeline and
    // the main loop is running, so it is ready to accept commands.
    private void onGStreamerInitialized () {
        Log.i ("GStreamer", "GStreamer initialized:");
        Log.i ("GStreamer", "  playing:" + is_playing_desired + " position:" + position + " uri: " + mediaUri);

        // Restore previous playing state
        setMediaUri ();
        nativeSetPosition (position);
        if (is_playing_desired) {
            nativePlay();
        } else {
            nativePause();
        }

        // Re-enable buttons, now that GStreamer is initialized
        final Activity activity = this;
        runOnUiThread(new Runnable() {
            public void run() {
                activity.findViewById(R.id.button_play).setEnabled(true);
                activity.findViewById(R.id.button_stop).setEnabled(true);
            }
        });
    }

    // The text widget acts as an slave for the seek bar, so it reflects what the seek bar shows, whether
    // it is an actual pipeline position or the position the user is currently dragging to.
    private void updateTimeWidget () {
        final TextView tv = (TextView) this.findViewById(R.id.textview_time);
        final SeekBar sb = (SeekBar) this.findViewById(R.id.seek_bar);
        final int pos = sb.getProgress();

        SimpleDateFormat df = new SimpleDateFormat("HH:mm:ss");
        df.setTimeZone(TimeZone.getTimeZone("UTC"));
        final String message = df.format(new Date (pos)) + " / " + df.format(new Date (duration));
        tv.setText(message);
    }

    // Called from native code
    private void setCurrentPosition(final int position, final int duration) {
        final SeekBar sb = (SeekBar) this.findViewById(R.id.seek_bar);

        // Ignore position messages from the pipeline if the seek bar is being dragged
        if (sb.isPressed()) return;

        runOnUiThread (new Runnable() {
          public void run() {
            sb.setMax(duration);
            sb.setProgress(position);
            updateTimeWidget();
          }
        });
        this.position = position;
        this.duration = duration;
    }

    static {
        System.loadLibrary("gstreamer_android");
        System.loadLibrary("tutorial-4");
        nativeClassInit();
    }

    public void surfaceChanged(SurfaceHolder holder, int format, int width,
            int height) {
        Log.d("GStreamer", "Surface changed to format " + format + " width "
                + width + " height " + height);
        nativeSurfaceInit (holder.getSurface());
    }

    public void surfaceCreated(SurfaceHolder holder) {
        Log.d("GStreamer", "Surface created: " + holder.getSurface());
    }

    public void surfaceDestroyed(SurfaceHolder holder) {
        Log.d("GStreamer", "Surface destroyed");
        nativeSurfaceFinalize ();
    }

    // Called from native code when the size of the media changes or is first detected.
    // Inform the video surface about the new size and recalculate the layout.
    private void onMediaSizeChanged (int width, int height) {
        Log.i ("GStreamer", "Media size changed to " + width + "x" + height);
        final GStreamerSurfaceView gsv = (GStreamerSurfaceView) this.findViewById(R.id.surface_video);
        gsv.media_width = width;
        gsv.media_height = height;
        runOnUiThread(new Runnable() {
            public void run() {
                gsv.requestLayout();
            }
        });
    }

    // The Seek Bar thumb has moved, either because the user dragged it or we have called setProgress()
    public void onProgressChanged(SeekBar sb, int progress, boolean fromUser) {
        if (fromUser == false) return;
        desired_position = progress;
        // If this is a local file, allow scrub seeking, this is, seek as soon as the slider is moved.
        if (is_local_media) nativeSetPosition(desired_position);
        updateTimeWidget();
    }

    // The user started dragging the Seek Bar thumb
    public void onStartTrackingTouch(SeekBar sb) {
        nativePause();
    }

    // The user released the Seek Bar thumb
    public void onStopTrackingTouch(SeekBar sb) {
        // If this is a remote file, scrub seeking is probably not going to work smoothly enough.
        // Therefore, perform only the seek when the slider is released.
        if (!is_local_media) nativeSetPosition(desired_position);
        if (is_playing_desired) nativePlay();
    }
}
</code></pre>
<h3 id="supporting-arbitrary-media-uris">Supporting arbitrary media URIs</h3>
<p>The C code provides the <code>nativeSetUri()</code> method so we can indicate the
URI of the media to play. Since <code>playbin</code> will be taking care of
retrieving the media, we can use local or remote URIs indistinctly
(<code>file://</code> or <code>http://</code>, for example). From Java, though, we want to
keep track of whether the file is local or remote, because we will not
offer the same functionalities. We keep track of this in the
<code>is_local_media</code> variable, and update it every time we change the media
URI:</p>
<pre><code class="language-java">private void setMediaUri() {
    nativeSetUri (mediaUri);
    is_local_media = mediaUri.startsWith("file://");
}
</code></pre>
<p>We call <code>setMediaUri()</code> in the <code>onGStreamerInitialized()</code> callback, once
the pipeline is ready to accept commands.</p>
<h3 id="reporting-media-size">Reporting media size</h3>
<p>Every time the size of the media changes (which could happen mid-stream,
for some kind of streams), or when it is first detected, C code calls
our <code>onMediaSizeChanged()</code> callback:</p>
<pre><code class="language-java">private void onMediaSizeChanged (int width, int height) {
    Log.i ("GStreamer", "Media size changed to " + width + "x" + height);
    final GStreamerSurfaceView gsv = (GStreamerSurfaceView) this.findViewById(R.id.surface_video);
    gsv.media_width = width;
    gsv.media_height = height;
    runOnUiThread(new Runnable() {
        public void run() {
            gsv.requestLayout();
        }
    });
}
</code></pre>
<p>Here we simply pass the new size onto the <code>GStreamerSurfaceView</code> in
charge of displaying the media, and ask the Android layout to be
recalculated. Eventually, the <code>onMeasure()</code> method in
GStreamerSurfaceView will be called and the new size will be taken
into account. As we have already seen in
<a href="a-running-pipeline.html">Android tutorial 2: A running pipeline</a>, methods which change
the UI must be called from the main thread, and we are now in a
callback from some GStreamer internal thread. Hence, the usage of
<a href="http://developer.android.com/reference/android/app/Activity.html#runOnUiThread(java.lang.Runnable)">runOnUiThread()</a>.</p>
<h3 id="refreshing-the-seek-bar">Refreshing the Seek Bar</h3>
<p><a href="../basic/toolkit-integration.html">Basic tutorial 5: GUI toolkit integration</a>
has already shown how to implement a <a href="http://developer.android.com/reference/android/widget/SeekBar.html">Seek
Bar</a> using
the GTK+ toolkit. The implementation on Android is very similar.</p>
<p>The Seek Bar accomplishes to functions: First, it moves on its own to
reflect the current playback position in the media. Second, it can be
dragged by the user to seek to a different position.</p>
<p>To realize the first function, C code will periodically call our
<code>setCurrentPosition()</code> method so we can update the position of the thumb
in the Seek Bar. Again we do so from the UI thread, using
<code>RunOnUiThread()</code>.</p>
<pre><code class="language-java">private void setCurrentPosition(final int position, final int duration) {
    final SeekBar sb = (SeekBar) this.findViewById(R.id.seek_bar);

    // Ignore position messages from the pipeline if the seek bar is being dragged
    if (sb.isPressed()) return;

    runOnUiThread (new Runnable() {
      public void run() {
        sb.setMax(duration);
        sb.setProgress(position);
        updateTimeWidget();
      }
    });
    this.position = position;
    this.duration = duration;
}
</code></pre>
<p>To the left of the Seek Bar (refer to the screenshot at the top of this
page), there is a
<a href="http://developer.android.com/reference/android/widget/TextView.html">TextView</a>
widget which we will use to display the current position and duration in
<code>HH:mm:ss / HH:mm:ss</code> textual format. The <code>updateTimeWidget()</code> method
takes care of it, and must be called every time the Seek Bar is updated:</p>
<pre><code class="language-java">private void updateTimeWidget () {
    final TextView tv = (TextView) this.findViewById(R.id.textview_time);
    final SeekBar sb = (SeekBar) this.findViewById(R.id.seek_bar);
    final int pos = sb.getProgress();

    SimpleDateFormat df = new SimpleDateFormat("HH:mm:ss");
    df.setTimeZone(TimeZone.getTimeZone("UTC"));
    final String message = df.format(new Date (pos)) + " / " + df.format(new Date (duration));
    tv.setText(message);
}
</code></pre>
<h3 id="seeking-with-the-seek-bar">Seeking with the Seek Bar</h3>
<p>To perform the second function of the <a href="http://developer.android.com/reference/android/widget/SeekBar.html">Seek
Bar</a> (allowing
the user to seek by dragging the thumb), we implement the
<a href="http://developer.android.com/reference/android/widget/SeekBar.OnSeekBarChangeListener.html">OnSeekBarChangeListener</a>
interface in the
Activity:</p>
<pre><code class="language-java">public class Tutorial4 extends Activity implements SurfaceHolder.Callback, OnSeekBarChangeListener {
</code></pre>
<p>And we register the Activity as the listener for the <a href="http://developer.android.com/reference/android/widget/SeekBar.html">Seek
Bar</a>’s
events in the <code>onCreate()</code> method:</p>
<pre><code class="language-java">SeekBar sb = (SeekBar) this.findViewById(R.id.seek_bar);
sb.setOnSeekBarChangeListener(this);
</code></pre>
<p>We will now be notified of three events: When the user starts dragging
the thumb, every time the thumb moves and when the thumb is released by
the user:</p>
<pre><code class="language-java">public void onStartTrackingTouch(SeekBar sb) {
    nativePause();
}
</code></pre>
<p><a href="http://developer.android.com/reference/android/widget/SeekBar.OnSeekBarChangeListener.html#onStartTrackingTouch(android.widget.SeekBar)">onStartTrackingTouch()</a>
is called when the user starts dragging, and the only thing we do is
pause the pipeline. If the user is searching for a particular scene, we
do not want it to keep
moving.</p>
<pre><code class="language-java">public void onProgressChanged(SeekBar sb, int progress, boolean fromUser) {
    if (fromUser == false) return;
    desired_position = progress;
    // If this is a local file, allow scrub seeking, this is, seek soon as the slider is moved.
    if (is_local_media) nativeSetPosition(desired_position);
    updateTimeWidget();
}
</code></pre>
<p><a href="http://developer.android.com/reference/android/widget/SeekBar.OnSeekBarChangeListener.html#onProgressChanged(android.widget.SeekBar,%20int,%20boolean)">onProgressChanged()</a> is
called every time the thumb moves, be it because the user dragged it, or
because we called <code>setProgress()</code> on the Seek Bar. We discard the latter
case with the handy <code>fromUser</code> parameter.</p>
<p>As the comment says, if this is a local media, we allow scrub seeking,
this is, we jump to the indicated position as soon as the thumb moves.
Otherwise, the seek will be performed when the thumb is released, and
the only thing we do here is update the textual time widget.</p>
<pre><code class="language-java">public void onStopTrackingTouch(SeekBar sb) {
    // If this is a remote file, scrub seeking is probably not going to work smoothly enough.
    // Therefore, perform only the seek when the slider is released.
    if (!is_local_media) nativeSetPosition(desired_position);
    if (is_playing_desired) nativePlay();
}
</code></pre>
<p>Finally, <a href="http://developer.android.com/reference/android/widget/SeekBar.OnSeekBarChangeListener.html#onStopTrackingTouch(android.widget.SeekBar)">onStopTrackingTouch()</a>
is called when the thumb is released. We simply perform the seek
operation if the file was non-local, and restore the pipeline to the
desired playing state.</p>
<p>This concludes the User interface part of this tutorial. Let’s review
now the under-the-hood C code that allows this to work.</p>
<h2 id="a-basic-media-player-c-code">A basic media player [C code]</h2>
<p><strong>jni/tutorial-4.c</strong></p>
<pre><code class="language-c">#include &lt;string.h&gt;
#include &lt;jni.h&gt;
#include &lt;android/log.h&gt;
#include &lt;android/native_window.h&gt;
#include &lt;android/native_window_jni.h&gt;
#include &lt;gst/gst.h&gt;
#include &lt;gst/interfaces/xoverlay.h&gt;
#include &lt;gst/video/video.h&gt;
#include &lt;pthread.h&gt;

GST_DEBUG_CATEGORY_STATIC (debug_category);
#define GST_CAT_DEFAULT debug_category

/*
 * These macros provide a way to store the native pointer to CustomData, which might be 32 or 64 bits, into
 * a jlong, which is always 64 bits, without warnings.
 */
#if GLIB_SIZEOF_VOID_P == 8
## define GET_CUSTOM_DATA(env, thiz, fieldID) (CustomData *)(*env)-&gt;GetLongField (env, thiz, fieldID)
## define SET_CUSTOM_DATA(env, thiz, fieldID, data) (*env)-&gt;SetLongField (env, thiz, fieldID, (jlong)data)
#else
## define GET_CUSTOM_DATA(env, thiz, fieldID) (CustomData *)(jint)(*env)-&gt;GetLongField (env, thiz, fieldID)
## define SET_CUSTOM_DATA(env, thiz, fieldID, data) (*env)-&gt;SetLongField (env, thiz, fieldID, (jlong)(jint)data)
#endif

/* Do not allow seeks to be performed closer than this distance. It is visually useless, and will probably
 * confuse some demuxers. */
#define SEEK_MIN_DELAY (500 * GST_MSECOND)

/* Structure to contain all our information, so we can pass it to callbacks */
typedef struct _CustomData {
  jobject app;                  /* Application instance, used to call its methods. A global reference is kept. */
  GstElement *pipeline;         /* The running pipeline */
  GMainContext *context;        /* GLib context used to run the main loop */
  GMainLoop *main_loop;         /* GLib main loop */
  gboolean initialized;         /* To avoid informing the UI multiple times about the initialization */
  ANativeWindow *native_window; /* The Android native window where video will be rendered */
  GstState state;               /* Current pipeline state */
  GstState target_state;        /* Desired pipeline state, to be set once buffering is complete */
  gint64 duration;              /* Cached clip duration */
  gint64 desired_position;      /* Position to seek to, once the pipeline is running */
  GstClockTime last_seek_time;  /* For seeking overflow prevention (throttling) */
  gboolean is_live;             /* Live streams do not use buffering */
} CustomData;

/* playbin flags */
typedef enum {
  GST_PLAY_FLAG_TEXT = (1 &lt;&lt; 2)  /* We want subtitle output */
} GstPlayFlags;

/* These global variables cache values which are not changing during execution */
static pthread_t gst_app_thread;
static pthread_key_t current_jni_env;
static JavaVM *java_vm;
static jfieldID custom_data_field_id;
static jmethodID set_message_method_id;
static jmethodID set_current_position_method_id;
static jmethodID on_gstreamer_initialized_method_id;
static jmethodID on_media_size_changed_method_id;

/*
 * Private methods
 */

/* Register this thread with the VM */
static JNIEnv *attach_current_thread (void) {
  JNIEnv *env;
  JavaVMAttachArgs args;

  GST_DEBUG ("Attaching thread %p", g_thread_self ());
  args.version = JNI_VERSION_1_4;
  args.name = NULL;
  args.group = NULL;

  if ((*java_vm)-&gt;AttachCurrentThread (java_vm, &amp;env, &amp;args) &lt; 0) {
    GST_ERROR ("Failed to attach current thread");
    return NULL;
  }

  return env;
}

/* Unregister this thread from the VM */
static void detach_current_thread (void *env) {
  GST_DEBUG ("Detaching thread %p", g_thread_self ());
  (*java_vm)-&gt;DetachCurrentThread (java_vm);
}

/* Retrieve the JNI environment for this thread */
static JNIEnv *get_jni_env (void) {
  JNIEnv *env;

  if ((env = pthread_getspecific (current_jni_env)) == NULL) {
    env = attach_current_thread ();
    pthread_setspecific (current_jni_env, env);
  }

  return env;
}

/* Change the content of the UI's TextView */
static void set_ui_message (const gchar *message, CustomData *data) {
  JNIEnv *env = get_jni_env ();
  GST_DEBUG ("Setting message to: %s", message);
  jstring jmessage = (*env)-&gt;NewStringUTF(env, message);
  (*env)-&gt;CallVoidMethod (env, data-&gt;app, set_message_method_id, jmessage);
  if ((*env)-&gt;ExceptionCheck (env)) {
    GST_ERROR ("Failed to call Java method");
    (*env)-&gt;ExceptionClear (env);
  }
  (*env)-&gt;DeleteLocalRef (env, jmessage);
}

/* Tell the application what is the current position and clip duration */
static void set_current_ui_position (gint position, gint duration, CustomData *data) {
  JNIEnv *env = get_jni_env ();
  (*env)-&gt;CallVoidMethod (env, data-&gt;app, set_current_position_method_id, position, duration);
  if ((*env)-&gt;ExceptionCheck (env)) {
    GST_ERROR ("Failed to call Java method");
    (*env)-&gt;ExceptionClear (env);
  }
}

/* If we have pipeline and it is running, query the current position and clip duration and inform
 * the application */
static gboolean refresh_ui (CustomData *data) {
  GstFormat fmt = GST_FORMAT_TIME;
  gint64 current = -1;
  gint64 position;

  /* We do not want to update anything unless we have a working pipeline in the PAUSED or PLAYING state */
  if (!data || !data-&gt;pipeline || data-&gt;state &lt; GST_STATE_PAUSED)
    return TRUE;

  /* If we didn't know it yet, query the stream duration */
  if (!GST_CLOCK_TIME_IS_VALID (data-&gt;duration)) {
    if (!gst_element_query_duration (data-&gt;pipeline, &amp;fmt, &amp;data-&gt;duration)) {
      GST_WARNING ("Could not query current duration");
    }
  }

  if (gst_element_query_position (data-&gt;pipeline, &amp;fmt, &amp;position)) {
    /* Java expects these values in milliseconds, and GStreamer provides nanoseconds */
    set_current_ui_position (position / GST_MSECOND, data-&gt;duration / GST_MSECOND, data);
  }
  return TRUE;
}

/* Forward declaration for the delayed seek callback */
static gboolean delayed_seek_cb (CustomData *data);

/* Perform seek, if we are not too close to the previous seek. Otherwise, schedule the seek for
 * some time in the future. */
static void execute_seek (gint64 desired_position, CustomData *data) {
  gint64 diff;

  if (desired_position == GST_CLOCK_TIME_NONE)
    return;

  diff = gst_util_get_timestamp () - data-&gt;last_seek_time;

  if (GST_CLOCK_TIME_IS_VALID (data-&gt;last_seek_time) &amp;&amp; diff &lt; SEEK_MIN_DELAY) {
    /* The previous seek was too close, delay this one */
    GSource *timeout_source;

    if (data-&gt;desired_position == GST_CLOCK_TIME_NONE) {
      /* There was no previous seek scheduled. Setup a timer for some time in the future */
      timeout_source = g_timeout_source_new ((SEEK_MIN_DELAY - diff) / GST_MSECOND);
      g_source_set_callback (timeout_source, (GSourceFunc)delayed_seek_cb, data, NULL);
      g_source_attach (timeout_source, data-&gt;context);
      g_source_unref (timeout_source);
    }
    /* Update the desired seek position. If multiple requests are received before it is time
     * to perform a seek, only the last one is remembered. */
    data-&gt;desired_position = desired_position;
    GST_DEBUG ("Throttling seek to %" GST_TIME_FORMAT ", will be in %" GST_TIME_FORMAT,
        GST_TIME_ARGS (desired_position), GST_TIME_ARGS (SEEK_MIN_DELAY - diff));
  } else {
    /* Perform the seek now */
    GST_DEBUG ("Seeking to %" GST_TIME_FORMAT, GST_TIME_ARGS (desired_position));
    data-&gt;last_seek_time = gst_util_get_timestamp ();
    gst_element_seek_simple (data-&gt;pipeline, GST_FORMAT_TIME, GST_SEEK_FLAG_FLUSH | GST_SEEK_FLAG_KEY_UNIT, desired_position);
    data-&gt;desired_position = GST_CLOCK_TIME_NONE;
  }
}

/* Delayed seek callback. This gets called by the timer setup in the above function. */
static gboolean delayed_seek_cb (CustomData *data) {
  GST_DEBUG ("Doing delayed seek to %" GST_TIME_FORMAT, GST_TIME_ARGS (data-&gt;desired_position));
  execute_seek (data-&gt;desired_position, data);
  return FALSE;
}

/* Retrieve errors from the bus and show them on the UI */
static void error_cb (GstBus *bus, GstMessage *msg, CustomData *data) {
  GError *err;
  gchar *debug_info;
  gchar *message_string;

  gst_message_parse_error (msg, &amp;err, &amp;debug_info);
  message_string = g_strdup_printf ("Error received from element %s: %s", GST_OBJECT_NAME (msg-&gt;src), err-&gt;message);
  g_clear_error (&amp;err);
  g_free (debug_info);
  set_ui_message (message_string, data);
  g_free (message_string);
  data-&gt;target_state = GST_STATE_NULL;
  gst_element_set_state (data-&gt;pipeline, GST_STATE_NULL);
}

/* Called when the End Of the Stream is reached. Just move to the beginning of the media and pause. */
static void eos_cb (GstBus *bus, GstMessage *msg, CustomData *data) {
  data-&gt;target_state = GST_STATE_PAUSED;
  data-&gt;is_live = (gst_element_set_state (data-&gt;pipeline, GST_STATE_PAUSED) == GST_STATE_CHANGE_NO_PREROLL);
  execute_seek (0, data);
}

/* Called when the duration of the media changes. Just mark it as unknown, so we re-query it in the next UI refresh. */
static void duration_cb (GstBus *bus, GstMessage *msg, CustomData *data) {
  data-&gt;duration = GST_CLOCK_TIME_NONE;
}

/* Called when buffering messages are received. We inform the UI about the current buffering level and
 * keep the pipeline paused until 100% buffering is reached. At that point, set the desired state. */
static void buffering_cb (GstBus *bus, GstMessage *msg, CustomData *data) {
  gint percent;

  if (data-&gt;is_live)
    return;

  gst_message_parse_buffering (msg, &amp;percent);
  if (percent &lt; 100 &amp;&amp; data-&gt;target_state &gt;= GST_STATE_PAUSED) {
    gchar * message_string = g_strdup_printf ("Buffering %d%%", percent);
    gst_element_set_state (data-&gt;pipeline, GST_STATE_PAUSED);
    set_ui_message (message_string, data);
    g_free (message_string);
  } else if (data-&gt;target_state &gt;= GST_STATE_PLAYING) {
    gst_element_set_state (data-&gt;pipeline, GST_STATE_PLAYING);
  } else if (data-&gt;target_state &gt;= GST_STATE_PAUSED) {
    set_ui_message ("Buffering complete", data);
  }
}

/* Called when the clock is lost */
static void clock_lost_cb (GstBus *bus, GstMessage *msg, CustomData *data) {
  if (data-&gt;target_state &gt;= GST_STATE_PLAYING) {
    gst_element_set_state (data-&gt;pipeline, GST_STATE_PAUSED);
    gst_element_set_state (data-&gt;pipeline, GST_STATE_PLAYING);
  }
}

/* Retrieve the video sink's Caps and tell the application about the media size */
static void check_media_size (CustomData *data) {
  JNIEnv *env = get_jni_env ();
  GstElement *video_sink;
  GstPad *video_sink_pad;
  GstCaps *caps;
  GstVideoFormat fmt;
  int width;
  int height;

  /* Retrieve the Caps at the entrance of the video sink */
  g_object_get (data-&gt;pipeline, "video-sink", &amp;video_sink, NULL);
  video_sink_pad = gst_element_get_static_pad (video_sink, "sink");
  caps = gst_pad_get_negotiated_caps (video_sink_pad);

  if (gst_video_format_parse_caps(caps, &amp;fmt, &amp;width, &amp;height)) {
    int par_n, par_d;
    if (gst_video_parse_caps_pixel_aspect_ratio (caps, &amp;par_n, &amp;par_d)) {
      width = width * par_n / par_d;
    }
    GST_DEBUG ("Media size is %dx%d, notifying application", width, height);

    (*env)-&gt;CallVoidMethod (env, data-&gt;app, on_media_size_changed_method_id, (jint)width, (jint)height);
    if ((*env)-&gt;ExceptionCheck (env)) {
      GST_ERROR ("Failed to call Java method");
      (*env)-&gt;ExceptionClear (env);
    }
  }

  gst_caps_unref(caps);
  gst_object_unref (video_sink_pad);
  gst_object_unref(video_sink);
}

/* Notify UI about pipeline state changes */
static void state_changed_cb (GstBus *bus, GstMessage *msg, CustomData *data) {
  GstState old_state, new_state, pending_state;
  gst_message_parse_state_changed (msg, &amp;old_state, &amp;new_state, &amp;pending_state);
  /* Only pay attention to messages coming from the pipeline, not its children */
  if (GST_MESSAGE_SRC (msg) == GST_OBJECT (data-&gt;pipeline)) {
    data-&gt;state = new_state;
    gchar *message = g_strdup_printf("State changed to %s", gst_element_state_get_name(new_state));
    set_ui_message(message, data);
    g_free (message);

    /* The Ready to Paused state change is particularly interesting: */
    if (old_state == GST_STATE_READY &amp;&amp; new_state == GST_STATE_PAUSED) {
      /* By now the sink already knows the media size */
      check_media_size(data);

      /* If there was a scheduled seek, perform it now that we have moved to the Paused state */
      if (GST_CLOCK_TIME_IS_VALID (data-&gt;desired_position))
        execute_seek (data-&gt;desired_position, data);
    }
  }
}

/* Check if all conditions are met to report GStreamer as initialized.
 * These conditions will change depending on the application */
static void check_initialization_complete (CustomData *data) {
  JNIEnv *env = get_jni_env ();
  if (!data-&gt;initialized &amp;&amp; data-&gt;native_window &amp;&amp; data-&gt;main_loop) {
    GST_DEBUG ("Initialization complete, notifying application. native_window:%p main_loop:%p", data-&gt;native_window, data-&gt;main_loop);

    /* The main loop is running and we received a native window, inform the sink about it */
    gst_x_overlay_set_window_handle (GST_X_OVERLAY (data-&gt;pipeline), (guintptr)data-&gt;native_window);

    (*env)-&gt;CallVoidMethod (env, data-&gt;app, on_gstreamer_initialized_method_id);
    if ((*env)-&gt;ExceptionCheck (env)) {
      GST_ERROR ("Failed to call Java method");
      (*env)-&gt;ExceptionClear (env);
    }
    data-&gt;initialized = TRUE;
  }
}

/* Main method for the native code. This is executed on its own thread. */
static void *app_function (void *userdata) {
  JavaVMAttachArgs args;
  GstBus *bus;
  CustomData *data = (CustomData *)userdata;
  GSource *timeout_source;
  GSource *bus_source;
  GError *error = NULL;
  guint flags;

  GST_DEBUG ("Creating pipeline in CustomData at %p", data);

  /* Create our own GLib Main Context and make it the default one */
  data-&gt;context = g_main_context_new ();
  g_main_context_push_thread_default(data-&gt;context);

  /* Build pipeline */
  data-&gt;pipeline = gst_parse_launch("playbin", &amp;error);
  if (error) {
    gchar *message = g_strdup_printf("Unable to build pipeline: %s", error-&gt;message);
    g_clear_error (&amp;error);
    set_ui_message(message, data);
    g_free (message);
    return NULL;
  }

  /* Disable subtitles */
  g_object_get (data-&gt;pipeline, "flags", &amp;flags, NULL);
  flags &amp;= ~GST_PLAY_FLAG_TEXT;
  g_object_set (data-&gt;pipeline, "flags", flags, NULL);

  /* Set the pipeline to READY, so it can already accept a window handle, if we have one */
  data-&gt;target_state = GST_STATE_READY;
  gst_element_set_state(data-&gt;pipeline, GST_STATE_READY);

  /* Instruct the bus to emit signals for each received message, and connect to the interesting signals */
  bus = gst_element_get_bus (data-&gt;pipeline);
  bus_source = gst_bus_create_watch (bus);
  g_source_set_callback (bus_source, (GSourceFunc) gst_bus_async_signal_func, NULL, NULL);
  g_source_attach (bus_source, data-&gt;context);
  g_source_unref (bus_source);
  g_signal_connect (G_OBJECT (bus), "message::error", (GCallback)error_cb, data);
  g_signal_connect (G_OBJECT (bus), "message::eos", (GCallback)eos_cb, data);
  g_signal_connect (G_OBJECT (bus), "message::state-changed", (GCallback)state_changed_cb, data);
  g_signal_connect (G_OBJECT (bus), "message::duration", (GCallback)duration_cb, data);
  g_signal_connect (G_OBJECT (bus), "message::buffering", (GCallback)buffering_cb, data);
  g_signal_connect (G_OBJECT (bus), "message::clock-lost", (GCallback)clock_lost_cb, data);
  gst_object_unref (bus);

  /* Register a function that GLib will call 4 times per second */
  timeout_source = g_timeout_source_new (250);
  g_source_set_callback (timeout_source, (GSourceFunc)refresh_ui, data, NULL);
  g_source_attach (timeout_source, data-&gt;context);
  g_source_unref (timeout_source);

  /* Create a GLib Main Loop and set it to run */
  GST_DEBUG ("Entering main loop... (CustomData:%p)", data);
  data-&gt;main_loop = g_main_loop_new (data-&gt;context, FALSE);
  check_initialization_complete (data);
  g_main_loop_run (data-&gt;main_loop);
  GST_DEBUG ("Exited main loop");
  g_main_loop_unref (data-&gt;main_loop);
  data-&gt;main_loop = NULL;

  /* Free resources */
  g_main_context_pop_thread_default(data-&gt;context);
  g_main_context_unref (data-&gt;context);
  data-&gt;target_state = GST_STATE_NULL;
  gst_element_set_state (data-&gt;pipeline, GST_STATE_NULL);
  gst_object_unref (data-&gt;pipeline);

  return NULL;
}

/*
 * Java Bindings
 */

/* Instruct the native code to create its internal data structure, pipeline and thread */
static void gst_native_init (JNIEnv* env, jobject thiz) {
  CustomData *data = g_new0 (CustomData, 1);
  data-&gt;desired_position = GST_CLOCK_TIME_NONE;
  data-&gt;last_seek_time = GST_CLOCK_TIME_NONE;
  SET_CUSTOM_DATA (env, thiz, custom_data_field_id, data);
  GST_DEBUG_CATEGORY_INIT (debug_category, "tutorial-4", 0, "Android tutorial 4");
  gst_debug_set_threshold_for_name("tutorial-4", GST_LEVEL_DEBUG);
  GST_DEBUG ("Created CustomData at %p", data);
  data-&gt;app = (*env)-&gt;NewGlobalRef (env, thiz);
  GST_DEBUG ("Created GlobalRef for app object at %p", data-&gt;app);
  pthread_create (&amp;gst_app_thread, NULL, &amp;app_function, data);
}

/* Quit the main loop, remove the native thread and free resources */
static void gst_native_finalize (JNIEnv* env, jobject thiz) {
  CustomData *data = GET_CUSTOM_DATA (env, thiz, custom_data_field_id);
  if (!data) return;
  GST_DEBUG ("Quitting main loop...");
  g_main_loop_quit (data-&gt;main_loop);
  GST_DEBUG ("Waiting for thread to finish...");
  pthread_join (gst_app_thread, NULL);
  GST_DEBUG ("Deleting GlobalRef for app object at %p", data-&gt;app);
  (*env)-&gt;DeleteGlobalRef (env, data-&gt;app);
  GST_DEBUG ("Freeing CustomData at %p", data);
  g_free (data);
  SET_CUSTOM_DATA (env, thiz, custom_data_field_id, NULL);
  GST_DEBUG ("Done finalizing");
}

/* Set playbin's URI */
void gst_native_set_uri (JNIEnv* env, jobject thiz, jstring uri) {
  CustomData *data = GET_CUSTOM_DATA (env, thiz, custom_data_field_id);
  if (!data || !data-&gt;pipeline) return;
  const jbyte *char_uri = (*env)-&gt;GetStringUTFChars (env, uri, NULL);
  GST_DEBUG ("Setting URI to %s", char_uri);
  if (data-&gt;target_state &gt;= GST_STATE_READY)
    gst_element_set_state (data-&gt;pipeline, GST_STATE_READY);
  g_object_set(data-&gt;pipeline, "uri", char_uri, NULL);
  (*env)-&gt;ReleaseStringUTFChars (env, uri, char_uri);
  data-&gt;duration = GST_CLOCK_TIME_NONE;
  data-&gt;is_live = (gst_element_set_state (data-&gt;pipeline, data-&gt;target_state) == GST_STATE_CHANGE_NO_PREROLL);
}

/* Set pipeline to PLAYING state */
static void gst_native_play (JNIEnv* env, jobject thiz) {
  CustomData *data = GET_CUSTOM_DATA (env, thiz, custom_data_field_id);
  if (!data) return;
  GST_DEBUG ("Setting state to PLAYING");
  data-&gt;target_state = GST_STATE_PLAYING;
  data-&gt;is_live = (gst_element_set_state (data-&gt;pipeline, GST_STATE_PLAYING) == GST_STATE_CHANGE_NO_PREROLL);
}

/* Set pipeline to PAUSED state */
static void gst_native_pause (JNIEnv* env, jobject thiz) {
  CustomData *data = GET_CUSTOM_DATA (env, thiz, custom_data_field_id);
  if (!data) return;
  GST_DEBUG ("Setting state to PAUSED");
  data-&gt;target_state = GST_STATE_PAUSED;
  data-&gt;is_live = (gst_element_set_state (data-&gt;pipeline, GST_STATE_PAUSED) == GST_STATE_CHANGE_NO_PREROLL);
}

/* Instruct the pipeline to seek to a different position */
void gst_native_set_position (JNIEnv* env, jobject thiz, int milliseconds) {
  CustomData *data = GET_CUSTOM_DATA (env, thiz, custom_data_field_id);
  if (!data) return;
  gint64 desired_position = (gint64)(milliseconds * GST_MSECOND);
  if (data-&gt;state &gt;= GST_STATE_PAUSED) {
    execute_seek(desired_position, data);
  } else {
    GST_DEBUG ("Scheduling seek to %" GST_TIME_FORMAT " for later", GST_TIME_ARGS (desired_position));
    data-&gt;desired_position = desired_position;
  }
}

/* Static class initializer: retrieve method and field IDs */
static jboolean gst_native_class_init (JNIEnv* env, jclass klass) {
  custom_data_field_id = (*env)-&gt;GetFieldID (env, klass, "native_custom_data", "J");
  set_message_method_id = (*env)-&gt;GetMethodID (env, klass, "setMessage", "(Ljava/lang/String;)V");
  set_current_position_method_id = (*env)-&gt;GetMethodID (env, klass, "setCurrentPosition", "(II)V");
  on_gstreamer_initialized_method_id = (*env)-&gt;GetMethodID (env, klass, "onGStreamerInitialized", "()V");
  on_media_size_changed_method_id = (*env)-&gt;GetMethodID (env, klass, "onMediaSizeChanged", "(II)V");

  if (!custom_data_field_id || !set_message_method_id || !on_gstreamer_initialized_method_id ||
      !on_media_size_changed_method_id || !set_current_position_method_id) {
    /* We emit this message through the Android log instead of the GStreamer log because the later
     * has not been initialized yet.
     */
    __android_log_print (ANDROID_LOG_ERROR, "tutorial-4", "The calling class does not implement all necessary interface methods");
    return JNI_FALSE;
  }
  return JNI_TRUE;
}

static void gst_native_surface_init (JNIEnv *env, jobject thiz, jobject surface) {
  CustomData *data = GET_CUSTOM_DATA (env, thiz, custom_data_field_id);
  if (!data) return;
  ANativeWindow *new_native_window = ANativeWindow_fromSurface(env, surface);
  GST_DEBUG ("Received surface %p (native window %p)", surface, new_native_window);

  if (data-&gt;native_window) {
    ANativeWindow_release (data-&gt;native_window);
    if (data-&gt;native_window == new_native_window) {
      GST_DEBUG ("New native window is the same as the previous one", data-&gt;native_window);
      if (data-&gt;pipeline) {
        gst_x_overlay_expose(GST_X_OVERLAY (data-&gt;pipeline));
        gst_x_overlay_expose(GST_X_OVERLAY (data-&gt;pipeline));
      }
      return;
    } else {
      GST_DEBUG ("Released previous native window %p", data-&gt;native_window);
      data-&gt;initialized = FALSE;
    }
  }
  data-&gt;native_window = new_native_window;

  check_initialization_complete (data);
}

static void gst_native_surface_finalize (JNIEnv *env, jobject thiz) {
  CustomData *data = GET_CUSTOM_DATA (env, thiz, custom_data_field_id);
  if (!data) return;
  GST_DEBUG ("Releasing Native Window %p", data-&gt;native_window);

  if (data-&gt;pipeline) {
    gst_x_overlay_set_window_handle (GST_X_OVERLAY (data-&gt;pipeline), (guintptr)NULL);
    gst_element_set_state (data-&gt;pipeline, GST_STATE_READY);
  }

  ANativeWindow_release (data-&gt;native_window);
  data-&gt;native_window = NULL;
  data-&gt;initialized = FALSE;
}

/* List of implemented native methods */
static JNINativeMethod native_methods[] = {
  { "nativeInit", "()V", (void *) gst_native_init},
  { "nativeFinalize", "()V", (void *) gst_native_finalize},
  { "nativeSetUri", "(Ljava/lang/String;)V", (void *) gst_native_set_uri},
  { "nativePlay", "()V", (void *) gst_native_play},
  { "nativePause", "()V", (void *) gst_native_pause},
  { "nativeSetPosition", "(I)V", (void*) gst_native_set_position},
  { "nativeSurfaceInit", "(Ljava/lang/Object;)V", (void *) gst_native_surface_init},
  { "nativeSurfaceFinalize", "()V", (void *) gst_native_surface_finalize},
  { "nativeClassInit", "()Z", (void *) gst_native_class_init}
};

/* Library initializer */
jint JNI_OnLoad(JavaVM *vm, void *reserved) {
  JNIEnv *env = NULL;

  java_vm = vm;

  if ((*vm)-&gt;GetEnv(vm, (void**) &amp;env, JNI_VERSION_1_4) != JNI_OK) {
    __android_log_print (ANDROID_LOG_ERROR, "tutorial-4", "Could not retrieve JNIEnv");
    return 0;
  }
  jclass klass = (*env)-&gt;FindClass (env, "com/gst_sdk_tutorials/tutorial_4/Tutorial4");
  (*env)-&gt;RegisterNatives (env, klass, native_methods, G_N_ELEMENTS(native_methods));

  pthread_key_create (&amp;current_jni_env, detach_current_thread);

  return JNI_VERSION_1_4;
}
</code></pre>
<h3 id="supporting-arbitrary-media-uris1">Supporting arbitrary media URIs</h3>
<p>Java code will call <code>gst_native_set_uri()</code> whenever it wants to change
the playing URI (in this tutorial the URI never changes, but it could):</p>
<pre><code class="language-c">void gst_native_set_uri (JNIEnv* env, jobject thiz, jstring uri) {
  CustomData *data = GET_CUSTOM_DATA (env, thiz, custom_data_field_id);
  if (!data || !data-&gt;pipeline) return;
  const jbyte *char_uri = (*env)-&gt;GetStringUTFChars (env, uri, NULL);
  GST_DEBUG ("Setting URI to %s", char_uri);
  if (data-&gt;target_state &gt;= GST_STATE_READY)
    gst_element_set_state (data-&gt;pipeline, GST_STATE_READY);
  g_object_set(data-&gt;pipeline, "uri", char_uri, NULL);
  (*env)-&gt;ReleaseStringUTFChars (env, uri, char_uri);
  data-&gt;duration = GST_CLOCK_TIME_NONE;
  data-&gt;is_live = (gst_element_set_state (data-&gt;pipeline, data-&gt;target_state) == GST_STATE_CHANGE_NO_PREROLL);
}
</code></pre>
<p>We first need to convert between the
<a href="http://en.wikipedia.org/wiki/UTF-16">UTF16</a> encoding used by Java and
the <a href="http://en.wikipedia.org/wiki/UTF-8#Modified_UTF-8">Modified
UTF8</a> used by
GStreamer with
<a href="http://docs.oracle.com/javase/1.5.0/docs/guide/jni/spec/functions.html#wp17265">GetStringUTFChars()</a>
and
<a href="http://docs.oracle.com/javase/1.5.0/docs/guide/jni/spec/functions.html#wp17294">ReleaseStringUTFChars()</a>.</p>
<p><code>playbin</code> will only care about URI changes in the READY to PAUSED state
change, because the new URI might need a completely different playback
pipeline (think about switching from a local Matroska file to a remote
OGG file: this would require, at least, different source and demuxing
elements). Thus, before passing the new URI to <code>playbin</code> we set its
state to READY (if we were in PAUSED or PLAYING).</p>
<p><code>playbin</code>’s URI is exposed as a common GObject property, so we simply
set it with <code>g_object_set()</code>.</p>
<p>We then reset the clip duration, so it is re-queried later, and bring
the pipeline to the playing state it had before. In this last step, we
also take note of whether the new URI corresponds to a live source or
not. Live sources must not use buffering (otherwise latency is
introduced which is inacceptable for them), so we keep track of this
information in the <code>is_live</code> variable.</p>
<h3 id="reporting-media-size1">Reporting media size</h3>
<p>Some codecs allow the media size (width and height of the video) to
change during playback. For simplicity, this tutorial assumes that they
do not. Therefore, in the READY to PAUSED state change, once the Caps of
the decoded media are known, we inspect them in <code>check_media_size()</code>:</p>
<pre><code class="language-c">static void check_media_size (CustomData *data) {
  JNIEnv *env = get_jni_env ();
  GstElement *video_sink;
  GstPad *video_sink_pad;
  GstCaps *caps;
  GstVideoFormat fmt;
  int width;
  int height;

  /* Retrieve the Caps at the entrance of the video sink */
  g_object_get (data-&gt;pipeline, "video-sink", &amp;video_sink, NULL);
  video_sink_pad = gst_element_get_static_pad (video_sink, "sink");
  caps = gst_pad_get_negotiated_caps (video_sink_pad);

  if (gst_video_format_parse_caps(caps, &amp;fmt, &amp;width, &amp;height)) {
    int par_n, par_d;
    if (gst_video_parse_caps_pixel_aspect_ratio (caps, &amp;par_n, &amp;par_d)) {
      width = width * par_n / par_d;
    }
    GST_DEBUG ("Media size is %dx%d, notifying application", width, height);

    (*env)-&gt;CallVoidMethod (env, data-&gt;app, on_media_size_changed_method_id, (jint)width, (jint)height);
    if ((*env)-&gt;ExceptionCheck (env)) {
      GST_ERROR ("Failed to call Java method");
      (*env)-&gt;ExceptionClear (env);
    }
  }

  gst_caps_unref(caps);
  gst_object_unref (video_sink_pad);
  gst_object_unref(video_sink);
}
</code></pre>
<p>We first retrieve the video sink element from the pipeline, using the
<code>video-sink</code> property of <code>playbin</code>, and then its sink Pad. The
negotiated Caps of this Pad, which we recover using
<code>gst_pad_get_negotiated_caps()</code>,  are the Caps of the decoded media.</p>
<p>The helper functions <code>gst_video_format_parse_caps()</code> and
<code>gst_video_parse_caps_pixel_aspect_ratio()</code> turn the Caps into
manageable integers, which we pass to Java through
its <code>onMediaSizeChanged()</code> callback.</p>
<h3 id="refreshing-the-seek-bar1">Refreshing the Seek Bar</h3>
<p>To keep the UI updated, a GLib timer is installed in the
<code>app_function()</code> that fires 4 times per second (or every 250ms), right
before entering the main loop:</p>
<pre><code class="language-c">timeout_source = g_timeout_source_new (250);
g_source_set_callback (timeout_source, (GSourceFunc)refresh_ui, data, NULL);
g_source_attach (timeout_source, data-&gt;context);
g_source_unref (timeout_source);
</code></pre>
<p>Then, in the refresh_ui method:</p>
<pre><code class="language-c">static gboolean refresh_ui (CustomData *data) {
  GstFormat fmt = GST_FORMAT_TIME;
  gint64 current = -1;
  gint64 position;

  /* We do not want to update anything unless we have a working pipeline in the PAUSED or PLAYING state */
  if (!data || !data-&gt;pipeline || data-&gt;state &lt; GST_STATE_PAUSED)
    return TRUE;

  /* If we didn't know it yet, query the stream duration */
  if (!GST_CLOCK_TIME_IS_VALID (data-&gt;duration)) {
    if (!gst_element_query_duration (data-&gt;pipeline, &amp;fmt, &amp;data-&gt;duration)) {
      GST_WARNING ("Could not query current duration");
    }
  }

  if (gst_element_query_position (data-&gt;pipeline, &amp;fmt, &amp;position)) {
    /* Java expects these values in milliseconds, and GStreamer provides nanoseconds */
    set_current_ui_position (position / GST_MSECOND, data-&gt;duration / GST_MSECOND, data);
  }
  return TRUE;
}
</code></pre>
<p>If it is unknown, the clip duration is retrieved, as explained in
<a href="../basic/time-management.html">Basic tutorial 4: Time management</a>. The current position is
retrieved next, and the UI is informed of both through its
<code>setCurrentPosition()</code> callback.</p>
<p>Bear in mind that all time-related measures returned by GStreamer are in
nanoseconds, whereas, for simplicity, we decided to make the UI code
work in milliseconds.</p>
<h3 id="seeking-with-the-seek-bar1">Seeking with the Seek Bar</h3>
<p>The Java UI code already takes care of most of the complexity of seeking
by dragging the thumb of the Seek Bar. From C code, we just need to
honor the calls to <code>nativeSetPosition()</code> and instruct the pipeline to
jump to the indicated position.</p>
<p>There are, though, a couple of caveats. Firstly, seeks are only possible
when the pipeline is in the PAUSED or PLAYING state, and we might
receive seek requests before that happens. Secondly, dragging the Seek
Bar can generate a very high number of seek requests in a short period
of time, which is visually useless and will impair responsiveness. Let’s
see how to overcome these problems.</p>
<h4 id="delayed-seeks">Delayed seeks</h4>
<p>In
<code>gst_native_set_position()</code>:</p>
<pre><code class="language-c">void gst_native_set_position (JNIEnv* env, jobject thiz, int milliseconds) {
  CustomData *data = GET_CUSTOM_DATA (env, thiz, custom_data_field_id);
  if (!data) return;
  gint64 desired_position = (gint64)(milliseconds * GST_MSECOND);
  if (data-&gt;state &gt;= GST_STATE_PAUSED) {
    execute_seek(desired_position, data);
  } else {
    GST_DEBUG ("Scheduling seek to %" GST_TIME_FORMAT " for later", GST_TIME_ARGS (desired_position));
    data-&gt;desired_position = desired_position;
  }
}
</code></pre>
<p>If we are already in the correct state for seeking, execute it right
away; otherwise, store the desired position in the
<code>desired_position</code> variable. Then, in the
<code>state_changed_cb()</code> callback:</p>
<pre><code class="language-c">if (old_state == GST_STATE_READY &amp;&amp; new_state == GST_STATE_PAUSED) {
  /* By now the sink already knows the media size */
  check_media_size(data);

  /* If there was a scheduled seek, perform it now that we have moved to the Paused state */
  if (GST_CLOCK_TIME_IS_VALID (data-&gt;desired_position))
    execute_seek (data-&gt;desired_position, data);
 }
}
</code></pre>
<p>Once the pipeline moves from the READY to the PAUSED state, we check if
there is a pending seek operation and execute it. The
<code>desired_position</code> variable is reset inside <code>execute_seek()</code>.</p>
<h4 id="seek-throttling">Seek throttling</h4>
<p>A seek is potentially a lengthy operation. The demuxer (the element
typically in charge of seeking) needs to estimate the appropriate byte
offset inside the media file that corresponds to the time position to
jump to. Then, it needs to start decoding from that point until the
desired position is reached. If the initial estimate is accurate, this
will not take long, but, on some container formats, or when indexing
information is missing, it can take up to several seconds.</p>
<p>If a demuxer is in the process of performing a seek and receives a
second one, it is up to it to finish the first one, start the second one
or abort both, which is a bad thing. A simple method to avoid this issue
is <em>throttling</em>, which means that we will only allow one seek every half
a second (for example): after performing a seek, only the last seek
request received during the next 500ms is stored, and will be honored
once this period elapses.</p>
<p>To achieve this, all seek requests are routed through the
<code>execute_seek()</code> method:</p>
<pre><code class="language-c">static void execute_seek (gint64 desired_position, CustomData *data) {
  gint64 diff;

  if (desired_position == GST_CLOCK_TIME_NONE)
    return;

  diff = gst_util_get_timestamp () - data-&gt;last_seek_time;

  if (GST_CLOCK_TIME_IS_VALID (data-&gt;last_seek_time) &amp;&amp; diff &lt; SEEK_MIN_DELAY) {
    /* The previous seek was too close, delay this one */
    GSource *timeout_source;

    if (data-&gt;desired_position == GST_CLOCK_TIME_NONE) {
      /* There was no previous seek scheduled. Setup a timer for some time in the future */
      timeout_source = g_timeout_source_new ((SEEK_MIN_DELAY - diff) / GST_MSECOND);
      g_source_set_callback (timeout_source, (GSourceFunc)delayed_seek_cb, data, NULL);
      g_source_attach (timeout_source, data-&gt;context);
      g_source_unref (timeout_source);
    }
    /* Update the desired seek position. If multiple requests are received before it is time
     * to perform a seek, only the last one is remembered. */
    data-&gt;desired_position = desired_position;
    GST_DEBUG ("Throttling seek to %" GST_TIME_FORMAT ", will be in %" GST_TIME_FORMAT,
        GST_TIME_ARGS (desired_position), GST_TIME_ARGS (SEEK_MIN_DELAY - diff));
  } else {
    /* Perform the seek now */
    GST_DEBUG ("Seeking to %" GST_TIME_FORMAT, GST_TIME_ARGS (desired_position));
    data-&gt;last_seek_time = gst_util_get_timestamp ();
    gst_element_seek_simple (data-&gt;pipeline, GST_FORMAT_TIME, GST_SEEK_FLAG_FLUSH | GST_SEEK_FLAG_KEY_UNIT, desired_position);
    data-&gt;desired_position = GST_CLOCK_TIME_NONE;
  }
}
</code></pre>
<p>The time at which the last seek was performed is stored in the
<code>last_seek_time</code> variable. This is wall clock time, not to be confused
with the stream time carried in the media time stamps, and is obtained
with <code>gst_util_get_timestamp()</code>.</p>
<p>If enough time has passed since the last seek operation, the new one is
directly executed and <code>last_seek_time</code> is updated. Otherwise, the new
seek is scheduled for later. If there is no previously scheduled seek, a
one-shot timer is setup to trigger 500ms after the last seek operation.
If another seek was already scheduled, its desired position is simply
updated with the new one.</p>
<p>The one-shot timer calls <code>delayed_seek_cb()</code>, which simply calls
<code>execute_seek()</code> again.</p>
<blockquote>
<p><img src="images/icons/emoticons/information.png" alt="information" id="information">
Ideally, <code>execute_seek()</code> will now find that enough time has indeed passed since the last seek and the scheduled one will proceed. It might happen, though, that after 500ms of the previous seek, and before the timer wakes up, yet another seek comes through and is executed. <code>delayed_seek_cb()</code> needs to check for this condition to avoid performing two very close seeks, and therefore calls <code>execute_seek()</code> instead of performing it itself.</p>
<p>This is not a complete solution: the scheduled seek will still be executed, even though a more-recent seek has already been executed that should have cancelled it. However, it is a good tradeoff between functionality and simplicity.</p>
</blockquote>
<h3 id="network-resilience">Network resilience</h3>
<p><a href="../basic/streaming.html">Basic tutorial 12: Streaming</a> has already
shown how to adapt to the variable nature of the network bandwidth by
using buffering. The same procedure is used here, by listening to the
buffering
messages:</p>
<pre><code class="language-c">g_signal_connect (G_OBJECT (bus), "message::buffering", (GCallback)buffering_cb, data);
</code></pre>
<p>And pausing the pipeline until buffering is complete (unless this is a
live
source):</p>
<pre><code class="language-c">static void buffering_cb (GstBus *bus, GstMessage *msg, CustomData *data) {
  gint percent;

  if (data-&gt;is_live)
    return;

  gst_message_parse_buffering (msg, &amp;percent);
  if (percent &lt; 100 &amp;&amp; data-&gt;target_state &gt;= GST_STATE_PAUSED) {
    gchar * message_string = g_strdup_printf ("Buffering %d%%", percent);
    gst_element_set_state (data-&gt;pipeline, GST_STATE_PAUSED);
    set_ui_message (message_string, data);
    g_free (message_string);
  } else if (data-&gt;target_state &gt;= GST_STATE_PLAYING) {
    gst_element_set_state (data-&gt;pipeline, GST_STATE_PLAYING);
  } else if (data-&gt;target_state &gt;= GST_STATE_PAUSED) {
    set_ui_message ("Buffering complete", data);
  }
}
</code></pre>
<p><code>target_state</code> is the state in which we have been instructed to set the
pipeline, which might be different to the current state, because
buffering forces us to go to PAUSED. Once buffering is complete we set
the pipeline to the <code>target_state</code>.</p>
<h2 id="a-basic-media-player-androidmk">A basic media player [Android.mk]</h2>
<p>The only line worth mentioning in the makefile
is <code>GSTREAMER_PLUGINS</code>:</p>
<p><strong>jni/Android.mk</strong></p>
<pre><code>GSTREAMER_PLUGINS         := $(GSTREAMER_PLUGINS_CORE) $(GSTREAMER_PLUGINS_PLAYBACK) $(GSTREAMER_PLUGINS_CODECS) $(GSTREAMER_PLUGINS_NET) $(GSTREAMER_PLUGINS_SYS)
</code></pre>
<p>In which all plugins required for playback are loaded, because it is not
known at build time what would be needed for an unspecified URI (again,
in this tutorial the URI does not change, but it will in the next one).</p>
<h2 id="conclusion">Conclusion</h2>
<p>This tutorial has shown how to embed a <code>playbin</code> pipeline into an
Android application. This, effectively, turns such application into a
basic media player, capable of streaming and decoding all the formats
GStreamer understands. More particularly, it has shown:</p>
<ul>
<li>How to keep the User Interface regularly updated by using a timer,
querying the pipeline position and calling a UI code method.</li>
<li>How to implement a Seek Bar which follows the current position and
transforms thumb motion into reliable seek events.</li>
<li>How to report the media size to adapt the display surface, by
reading the sink Caps at the appropriate moment and telling the UI
about it.</li>
</ul>
<p>The next tutorial adds the missing bits to turn the application built
here into an acceptable Android media player.</p>
<p>As usual, it has been a pleasure having you here, and see you soon!</p>

        

    </div>


        <div id="subpages"></div>
	</div>
	<div id="search_results">
		<p>The results of the search are</p>
	</div>
</div>
<div class="hidden-xs hidden-sm hidden-md col-lg-2 col-xl-2">
	<div id="toc-column">
		<div id="toc-wrapper" class="mCustomScrollbar" data-mcs-theme="dark">
			<nav id="toc"></nav>
		</div>
	</div>
</div>
	</div>
</div>
</main>


<footer class="page-row">
	<div class="container-fluid">
	<div class="row">
		<div class="hidden-xs hidden-sm col-md-3 col-xl-2"></div>
		<div class="col-sm-12 col-md-9 col-xl-8">
						
		</div>
		<div class="hidden-xs col-xl-2"></div>
	</div>
</div>
</footer>

</body>

<script src="assets/js/navbar_offset_scroller.js"></script>
</html>